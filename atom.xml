<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Mark&#39;s Tech Blog</title>
  
  <subtitle>技术理想国</subtitle>
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="http://mmhhss1991.github.io/"/>
  <updated>2018-09-01T08:39:45.913Z</updated>
  <id>http://mmhhss1991.github.io/</id>
  
  <author>
    <name>Mark Meng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Analysing Use of High Privileges in Android Applications</title>
    <link href="http://mmhhss1991.github.io/2018/08/21/analysing-high-privileges/"/>
    <id>http://mmhhss1991.github.io/2018/08/21/analysing-high-privileges/</id>
    <published>2018-08-21T00:00:00.000Z</published>
    <updated>2018-09-01T08:39:45.913Z</updated>
    
    <content type="html"><![CDATA[<div align="center" style="padding-bottom:15px;"><img src="/blog/2018/08/21/analysing-high-privileges/android.jpg"></div><p>This project is an extension to my thesis for the degree of master in computing at National University of Singapore, School of Computing. This work talks about the most popular un-rooted approach to escalate privilege on Android OS.<br><a id="more"></a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>The number of Android smartphone and tablet users has experienced a rapid growth in the past few years and it raises users’ awareness on the privacy and security of their mobile devices. The features of openness and extensibility make Android unique, attractive and competitive but meanwhile vulnerable to malicious attack.<br>There are lots of users rooting their Android devices for some useful functions, which are not originally provided to developers and users, such as backup and taking screenshot. However, after observing the danger of rooting devices, the developers begin to look for other non-root alternatives to implement those functions. ADB workaround is one of the best known non-root alternatives to help app gain higher privilege on Android. It used to be considered as a secure practice until some cases of ADB privilege leakage have been found.In this project, we design an approach and implement a couple of tools to detect the privilege leakage in Android apps. We apply them to analyse three real-world apps with millions of users, and successfully identify three ADB privilege leaks from them. Moreover, we also conduct an exploitation of the ADB privilege in one app, and therefore we prove the existence of vulnerabilities in ADB workaround. Based on out study, we propose some suggestion to help developers create their apps that could not only satisfy users’ needs but also protect users’ privacy from similar attacks in future.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The rise of mobile devices has greatly enriched people’s lives in this digital era. As the dominator of current mobile device market, Android reserves over 82.8% of the entire smartphone market share [13]. Andy Rubin, the VP of Google stated on his twitter that the number of Android devices activated per day has reach 1.3 millions in past few years [21]. The global gross shipments of Android devices is expected to grow from 1.2 billion in 2015 to 1.54 billions in 2019 [12]. The over-reliance on mobile devices makes people save all the data regardless of personal or business purpose onto their smartphone or tablet, which may lead their privacy under exposure if no proper protection has been enforced.</p><p>Android is well-known by its rich functionality and customisation, but there are still some requirements which cannot be implemented by using the official APIs. Google creates a collection of permission labels to define the privilege of apps running on Android OS. Some actions like reading the content displaying on the screen, in another word taking screenshot, was marked as <em>signature level</em> permission, hence are not allowed to be realised by the third party application. As long as the requirement of users exists, the developers would never stop to push the boundary. For that reason, developers are all motivated and successfully come up with two approaches to solve the permission dilemma, namely “rooting the phone” and “ADB workaround”.</p><p>Rooting the devices could enable users to gain the administration privileges to do anything they want such as removing pre-installed apps, unlocking more functionalities, or sometimes just simply changing the theme of UI. According to a statistic done by Kristijan Lucic in 2014 [15], there are over 27.44% users indicating that they have rooted their smartphones to remove redundant and useless pre-installed applications. However, there are several security issues behind the “rooting” because it disables the permission mechanism on Android system. </p><p>The good news says there is an increasing number of people who have realised the risk and danger of rooting their devices, and has started seeking non-root approaches. Gaining system privilege through <em>Android Debug Bridge (ADB)</em> is one of the best known and widely used workarounds. Users can connect their devices to a PC via either USB or wireless network, launch the ADB and then invoke a service with system level privilege running in the background. After that, an application could communicate with that service, send command to it, and thereby trigger it to work for the application with system privilege. In this manner, that app can do the job even without APIs provided by Android, any time and anywhere unless being powered off.</p><p>There are plenty of apps on Google Play Store adopting this ADB workaround to satisfy users’ specific needs which are not provided with APIs by Android OS, including, but not limited to, performing backup and restoration, taking screenshot, recording screen, etc. Those apps using ADB workaround to achieve high privilege are very popular and some of them has millions users.</p><p>The security of ADB workaround has been raised up after some exploitation being successfully conducted. It is critical because it refers to the privacy of millions of Android users. In this project, we design an approach to discover the vulnerabilities of ADB workaround and implement a solution to detect the privilege leakage. We apply this approach to three real-world apps downloaded from Google Play Store, analyse them and eventually discover the privilege leakage on each of three apps by using the tool we implemented. In addition, we conduct an exploitation on one of these three apps named “No Root Screenshot It” and successfully prove the existence of vulnerabilities that we have recently found. Last but not least, we provide some advice to the developers to help them achieve users’ requirement and meanwhile protect users’ data and privacy. </p><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Privilege-amp-Permission-on-Android"><a href="#Privilege-amp-Permission-on-Android" class="headerlink" title="Privilege &amp; Permission on Android"></a>Privilege &amp; Permission on Android</h2><p>Privilege is a security attribute required for certain operations. In Unix-like operation system, the process privileges are organised in shape of a flat tree, where the users’ privilege is presented as the leaves and the superuser is described as root [17]. Android, as a mobile operation system built based on Unix/Linux, takes advantage of the user-based protection to identify and isolate the resources used by applications. All applications on Android run within the <em>application sandbox</em> and only have limited permissions to access resources which have declared in advance [8].</p><p>Similar to other Unix-like operating systems, Android also has the <em>superuser-level</em> privilege called <em>root</em> that has full access to all apps’ data. By default, only a small subset of the core applications run with system or signature permissions, and most of the apps downloaded from the Google Play Store only request the user-level permissions such as access to the Internet, hardware sensor and camera. Android OS does not prevent users or applications with the root permission from accessing and even modifying the other applications even it is not encouraged [8].</p><p>Besides employing underlying Unix-like privilege system, Android alsohas its own privilege management mechanism, which is also known aspermission levels. On Android platform, permissions are classified intoseveral protection levels. Most of the Android developers are madeavailable to either the “<em>normal</em>” level permissions or the“<em>dangerous</em>” level permissions in their development. The <em>normal</em> levelpermissions, such as Internet, vibration, NFC or setting alarm, areconsidered as having no great risk to the user’s privacy or security.Those permissions will not prompt the user to grant permission ifproperly declared in manifest during the development. The <em>dangerous</em>level permissions indicate that the application needs access to privatedata or control over the device that may potentially have a negativeimpact to user. Unlike the <em>normal</em> level, all the operations classifiedin <em>dangerous</em> level will not be executed until obtaining user consent.In addition to <em>normal</em> level and <em>dangerous</em> level, there are two moreprotection levels namely <em>signature</em> level and <em>signature or system</em>level defining risky permissions. The former is only granted to theapplications signed by a trusted party like Android development group.The latter could only be granted to the apps that are embedded inAndroid system image or signed by vendors of the system image [6]. The grant of these two permission levels is not to beapproved by users, instead, it is conducted by signature validationmechanism of Andorid system during installation [22]. Manyfunctions that users require but not provided as public APIs by AndroidOS, like backing up, taking screenshot and screen recording, belong tothe <em>signature</em> level permission.</p><table><thead><tr><th><strong>Permission</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td><code>CALL_PRIVILEGED</code></td><td>Initiate a call without user confirmation</td></tr><tr><td><code>CAPTURE_VIDEO_OUTPUT</code></td><td>Capture video output stream</td></tr><tr><td><code>DELETE_CACHE_FILES</code></td><td>Delete cache files</td></tr><tr><td><code>DELETE_PACKAGE</code></td><td>Uninstall package</td></tr><tr><td><code>INSTALL_PACKAGES</code></td><td>Install packages</td></tr><tr><td><code>READ_FRAME_BUFFER</code></td><td>Access to the frame buffer data (e.g. screenshot)</td></tr><tr><td><code>READ_LOGS</code></td><td>Read system log files</td></tr><tr><td><code>REBOOT</code></td><td>Reboot the system</td></tr><tr><td><code>SET_TIME</code></td><td>Set system time</td></tr><tr><td><code>WRITE_APN_SETTINGS</code></td><td>Overwrite APN setting</td></tr></tbody></table><p>Examples of <em>signature</em>-level Permissions Not Granted to the Third|</p><h2 id="Privilege-Escalation"><a href="#Privilege-Escalation" class="headerlink" title="Privilege Escalation "></a>Privilege Escalation </h2><p>In order to implement the function like backup, taking screenshot or screen recording, developers have to find a way to escalate the privilege of their apps to the signature level or higher. There are two privilege escalation approaches on Android, namely rooting and non-root workarounds.  </p><h3 id="Rooting"><a href="#Rooting" class="headerlink" title="Rooting"></a>Rooting</h3><p>The word “root” may not be a strange term to an Android player. Rooting is the process of allowing users of Android devices to attain privileged control, which is also known as the root access [25]. The rooting process grants users with <em>superuser</em> level permissions to use their devices, which could provide users a ton of customizations and opportunities to exhaust the functionality of Android OS. However, besides the risks of voiding warranty and bricking the devices, rooting an Android device could also expose all the data and program to the adversary and bring severe security vulnerabilities [18]. The rooting process varies with hardware manufacture and system version, but generally speaking, there are three steps to root an Android device, including unlocking Bootloader, flashing custom Recovery ROM, and lastly, installing <em>SuperSU</em> [2]. Once an Android device is rooted, users can access and modify the system resource. Furthermore, user can customize privilege and assign to any application installed on the rooted device. </p><h3 id="Non-root-Alternative"><a href="#Non-root-Alternative" class="headerlink" title="Non-root Alternative"></a>Non-root Alternative</h3><p>Rooting Android device is a risky practice because it may void the warranty, brick the device and bring with numerous security vulnerabilities. Therefore, developers start to seek non-root alternatives to escalate privilege. There is an alternative approach called <em>ADB workaround</em> to attain high level privilege without rooting the device, and it becomes popular whilst the growth of users’ concern to their device security.</p><p>Take the programmatic screenshot as an example. An app needs to have a signature-level permission from the system to take screenshot, which is impossible for normal developers to obtain through normal level permission request in interface. However, there are still two workarounds even without the concession from Android development team: (1) taking screenshot on rooted devices; or (2) making use of a process with higher privilege to escalate the privilege of the app. The latter approach does not require the holistic change to the Android devices like “rooting”, and has better security and reliability [14].</p><p>ADB is a development tool provided by Google to allow developers to debug apps <em>shell</em> in the Android devices from their PCs. A process requiring signature-level permissions, such as taking screenshot or backup, is not allowed to be implemented in app by third party developers, but could be started from a ADB shell window [14]. That is the reason why ADB workaround could achieve higher privilege. Using ADB workaround, developers could implement all methods requiring signature-level permissions, pack all of them into a executive that could be started on ADB and run them in the background of Android OS as a service until being killed (e.g. power-off, restart). Thus the unprivileged App could communicate with the privileged proxy to indirectly achieve the functionality that may not able to be done on unprivileged App only.</p><h2 id="Reverse-Engineering-on-Android"><a href="#Reverse-Engineering-on-Android" class="headerlink" title="Reverse Engineering on Android"></a>Reverse Engineering on Android</h2><p>Reverse engineering, also known as back engineering, is defined by Wikipedia as “<em>the term describing processes of extracting knowledge or design information from an existing product and reproducing based on the extracted knowledge or design information with personal modification</em>” [24].</p><p>The Apps on Android are mainly written in Java and HTML5/CSS, packaged in format of APK and executed on the virtual machine called <em>Dalvik (DVM)</em>. Unlike Java Virtual Machine (JVM), the DVM uses a single Dalvik executive (DEX) file with different data structures and opcodes rather than a jar file compressing multiple Java classes together. Besides that, the JVM is a stack-based machine but DVM is designed as a register-based machine. For those reasons, the decompilation and reverse engineering on Android are more complicated and more challenging than doing the same things on Java code [16].</p><p>There are some tools designed for easy decompilation of Android Apps being made available to the public for free. For example, <em>Apktool</em> [a] is a very useful tool providing smali assembler and disassembler for APK files, thereby facilitate to modify an app and repackage it [5]. Another tool called <em>dex2jar</em> [b] allows users to convert APK back into a jar file, which could be easily further into Java codes by using Java decompiler such as <em>JD-GUI</em> [c] [16].</p><p>Most developers do not want the reverse engineering happened onto their products. Techniques like obfuscation could help developers to make the code unreadable before it being released to the market to prevent reverse engineering. A tool named <em>ProGuard</em> has been integrated into the Android build system by Google to help users shrinks, optimizes, and obfuscates the code [7]. Another tool <em>DexGuard</em>, which is a premium version of ProGuard, are widely used by commercial organisations and enterprises because it offers more advanced features to protect Android Apps from both static and dynamic analysis [9]. </p><h2 id="Instrumentation"><a href="#Instrumentation" class="headerlink" title="Instrumentation"></a>Instrumentation</h2><p>There are two famous alternatives of frameworks that achieve hooking on Andorid. One of them is an open-source frameworks called <em>Xposed</em> [d]. Another well-known framework is <em>Substrate</em> [e] which is not open-sourced but has better documented API. Both of them have similar functions and work based on a similar mechanism as well [3]. Xposed is more prevalent in the research field and developers community. There are over 800 modules implemented based on Xposed framework available to the public for free on Xposed Module Repository [19]. </p><p>The core of Android runtime is the process called <em>Zygote</em>. Each application is executed in form of a copy of Zygote, which is also known as “fork”. When an Android device is booted, a script named “<code>init.rc</code>” is started, followed by loading all necessary classes and methods by “<code>/system/bin/app_process</code>”. When Xposed comes into play, an extended startup app_process is copied to the “<code>/system/bin</code>” and then be started during the booting. In this way, Xposed can inject that extended startup process into the DVM instance, before each <code>main</code> method being called and invoked. Xposed provides a private and native method named <code>hookMethodNative</code> which is implemented within extended app_process. It could change the method type to “native”, and then onload its own implementation to that “native” method. Each method being hooked by Xposed will be converted into a “native” one, and the actual method being called could be treated as a brand new method with original implementation and overrided hook handling code written by Xposed framework developer [20]. </p><p>Hooking usually happened before or after the execution of target methods. A hook module could not only record and log all the data sending in and passing out the target methods, but also alter the behaviour of target methods by modifying its parameters or return value [11]. Therefore a dynamic analysis by Xposed hooking could help us to sketch out the data flow of a program. Figure below shows an example of hooking process onto an imaginary screenshot app. Hooking by using Xposed module could not only obtained the data flows within a running app, but also alter its behaviour by modifying return value. </p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/introXposedHooking.png" title="Example of Hooking Process: An Imaginary Screenshot App"><br></div></div><p> [a]: Apktool is available on<br>    <span><a href="http://ibotpeaches.github.io/apktool/" target="_blank" rel="noopener">http://ibotpeaches.github.io/apktool/</a></span></p><p> [b]: dex2jar is available on<br>    <span><a href="https://sourceforge.net/projects/dex2jar/" target="_blank" rel="noopener">https://sourceforge.net/projects/dex2jar/</a></span></p><p> [c]: JD-GUI is available on <span><a href="http://jd.benow.ca/" target="_blank" rel="noopener">http://jd.benow.ca/</a></span></p><p> [d]: Xposed is available on <span><a href="http://repo.xposed.info/" target="_blank" rel="noopener">http://repo.xposed.info/</a></span></p><p> [e]: Cydia Substrate is available on<br>    <span><a href="http://www.cydiasubstrate.com/" target="_blank" rel="noopener">http://www.cydiasubstrate.com/</a></span></p><h1 id="Approach-amp-Case-Studies"><a href="#Approach-amp-Case-Studies" class="headerlink" title="Approach &amp; Case Studies"></a>Approach &amp; Case Studies</h1><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>The apps using ADB workaround is usually a combination of a normal app with all functions being restricted at normal or dangerous level permission, and a proxy started by ADB and therefore has signature level permission on Android. In Android, most of apps communicates with proxies through socket channel, which has no strong protection and generic access control. A malicious app could easily obtain the control of proxy if it knows the protocol of communication between app and it. Whether the interface of the proxy is protected to prevent from third party access is the first potential issue.</p><p>Some apps implement password authentication into the protocol to strengthen protection to the proxy. However, because of the inconsistency of app and proxy’s life cycles, there must be a mechanism to temporarily save the password. By this means, a malicious app could still have chance to know the password if proper analysis has been done. Therefore, whether the protection is effective and secure enough is the second potential issue.</p><p>The approach to analyse the app and find the vulnerabilities of ADB workaround is initiated based on two potential issues we have mentioned recently, and targeted to exploit if any vulnerability has been found. We summarised this approach into four step:</p><div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/approach-of-analysis.png" title="Approach of App Analysis"><br></div></div><p>(1) <strong>an analysis on the proxy activation;</strong> This analysis could be done on reading proxy activation script if exists. The script could be either in batch file or bash script depends on which OS, Windows or Linux, to be run. Some apps do not provide script file to the user for Windows OS, for instead, a desktop application with UI is provided to achieve better user experience. In this circumstance, the Linux version of activation package is recommended to be download because script file is more widely used and possibly given on Linux OS. A simple and clear script file could disclose some details of the protocol of communication between app and proxy, such as the name of service proxy, the native executive file of proxy if any, and how the service being activated. Besides the analysis onto the script file, the name, process ID and permission group of service proxy running in the background could also be found by typing command “<code>adb shell ps</code>” in ADB through USB to the device. Moreover, the port opened for the communication between service proxy and app could be found in similar way by typing ADB command “<code>adb shell netstat</code>” to retrieve all active network usage on the device. However, in this step, the pairing of process and specific port listening may not be able to be observed if multiple proxies had been activated, like the scenario during the case study of app III.</p><p>(2) <strong>an analysis on the APK file;</strong> Concept of reverse engineering will be involved in this step like APK decompilation. Once the service proxy and port number have been identified, the next step is to discover the implementation of the communication between proxy and app in APK file. The APK file could be unpackaged and decompiled into smali/Java source code or assembly code, by using tools like <em>Apktool</em> or <em>dex2jar</em>. The smali/Java code has supreme readability which may help us to look through different classes to locate the code of protocol’s implementation. In fact, the decompilation analysis may not always to be proved as a smooth and easy process because most of developers obfuscated their code before releasing the APK file to the app Store. In this situation, the disassembly will be helpful and a supplement to the samli/Java code reading. Reading assembly code could help us recognise the constant strings and numbers defined within same class. For example, the magic number used for authentication in app II , which is <code>89234820</code>, was found in this manner.</p><p>(3) <strong>an dynamic analysis;</strong> Only reading the script and source code may not be sufficient to sketch out the entire protocol between proxy and app. The objective of dynamic analysis is to find both control flow and data flow occurred when the app interacts with service proxy. Reading logs through <code>logcat</code> is a simple but effective way to gain a brief understanding to the protocol. However, hooking by <em>Xposed</em> framework will be one of the best solution to complete the analysis when the source code has been enforced with strict obfuscation or an authentication has been applied onto the socket channel between app and proxy server. Hooking method could be forced onto the key methods in class(es) in charge of communication between app and proxy which has been discovered in last step, then sniff and extract the arguments passed in and return value through the system logs. According to the case studies in this project, the methods to be hooked are mostly used to handle the action trigger (e.g. <code>takeScreenshot</code>) and socket channel I/O (e.g. <code>write</code>). Hooking on the prior method(s) by printing logs could show us the control flow of the protocol, and hooking on the latter method(s) by extracting arguments’ value could help us understand the data flow between service proxy and app itself. By now with both control flow and data flow confirmed, the protocol between service proxy and app, which used to escalate privilege on Android OS, has been unveiled.</p><p>(4) <strong>authentication analysis if any;</strong> There is very possibly an authentication process if any series of numbers or a random string was found in the data flow of the protocol. In that case, it is encouraged to clarify if the password a constant (like app II) or dynamic (like app III); then followed by determining relationship between the password and life cycle of app or/and proxy(s) if the password is found to be dynamic. For the dynamic password, if the password change is triggered by proxy and independent of app itself, the password is generally stored at somewhere that the app has permission to read; otherwise the password itself or the mechanism of its changing should be able to be found in app’s source code.<br>Once these four steps listed above have been fully understood, attackers are theoretically able to exploit an app that uses ADB workaround in a programmatic manner. In the later part of this chapter, case studies on three apps using ADB workaround will be analysed and one of them will be exploited by apply this approach.</p><h2 id="Case-Studies"><a href="#Case-Studies" class="headerlink" title="Case Studies"></a>Case Studies</h2><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/classification-apps.png" title="Classification of Mainstream Screenshot Apps on Google Play Store"><br></div></div><p>A group of screenshot apps with the highest popularity and the most recent updates were chosen from the Google Play Store. There are altogether 13 screenshot apps being downloaded from Google Play Store and installed on testing Android devices. We have run all of them by following their instruction, observed the outcomes, and recorded all key information. The statistics of those 13 screenshot apps could be found in Table below in appendix of this report. After reviewing all 13 screenshot apps, as shown in Figure above, a simple classification were been made according to their functionality. These apps were firstly be separated according to their compatibility with un-rooted devices. 5 apps has instruction saying that they only support rooted devices. Among the 8 apps which works on un-rooted devices, 6 apps could only take screenshot by pressing hard-key combination (usually power key + volume down key), which may vary with configuration of manufactures, This method almost works on Android OS version 4.0 and later versions only [10]. Two apps left, <em>Screenshot Ultimate</em> developed by “icecoldapps” (noted as app I) and <em>No Root Screenshot It</em> developed by “edwardkim” (noted as app II) , has been found using ADB workaround to take screenshot and selected in case studies.</p><p>Unlike taking screenshot, screen recording is the function that users could not obtain by either third party apps without signature level permission, or any solution provided by Android OS. Keyword “<em>screen recording</em>” has been searched on Google Play Store. The number of apps found is much less than the searching result of “screenshot”. However, most of screen recording apps are using ADB workaround to achieve its functionality. In this project, one recording app, <em>FREE screen recorder NO ROOT</em> developed by “Invisibility Ltd” (Noted as app III), has also been chosen and analysed in case studies.</p><table><thead><tr><th><strong>Name</strong></th><th><strong>Description</strong></th><th><strong>Size</strong></th><th><strong>Identity Package Name</strong></th></tr></thead><tbody><tr><td>Screenshot Ultimate</td><td>Screenshot</td><td>3.2M</td><td>com.icecoldapps, screenshotultimate</td></tr><tr><td>No Root Screenshot It</td><td>Screenshot</td><td>838k</td><td>com.edwardkim.android, screenshotitfullnoroot</td></tr><tr><td>FREE screen recorder NO ROOT</td><td>Recording</td><td>7.4M</td><td>uk.org.invisibility, recordablefree</td></tr></tbody></table><h3 id="App-I-–-“Screenshot-Ultimate”"><a href="#App-I-–-“Screenshot-Ultimate”" class="headerlink" title="App I – “Screenshot Ultimate”"></a>App I – “Screenshot Ultimate”</h3><p>“Screenshot Ultimate” is one typical screenshot app that does not require a rooted device. It supports screenshot taking through ADB workaround. However, it is not like the deal above board since too obvious instruction may bring Google’s restriction. The ADB workaround is mentioned in a paragraph of “Help” instruction, and the URL to download the script and other necessary files are given in another place and could only be found in “Setting” &gt;“Capture Methods” &gt;“Manual”. Overall, it is still kind of good experience because of detailed step-by-step instruction and troubleshooting notes.</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/tutorial-icecoldapps.png" title="Steps to Find Unrooted Screenshot Method in App I"><br></div></div><h4 id="Analysis-of-ADB-Channel"><a href="#Analysis-of-ADB-Channel" class="headerlink" title="Analysis of ADB Channel"></a>Analysis of ADB Channel</h4><p>The native executable file, named “<code>screenshotultimatenative1</code>”, and scripts for both Linux and Windows OS have been downloaded as a zipped file from the URL given in the help instruction. After reading through the script file, the flow of service activation has been summarized and shown in Figure Service Activation of App I. With the process name of service running in the background, what we are going to do is to analyse the APK file and unveil the protocol of screenshot taking process between app and that service.</p><div align="center"><br><div style="width:40%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/activation-screenshot-ultimate.png" title="Proxy Activation of App I"><br></div></div><h4 id="Decompilation-of-APK"><a href="#Decompilation-of-APK" class="headerlink" title="Decompilation of APK"></a>Decompilation of APK</h4><p>The APK file is generated from Java code through a systematic routine which might be easy to be analysed when compared with the native executable. APK file could be downloaded by searching the mirror site of Google Play Store [a] or extracted by using specific apps like “<em>APK Extractor</em>” [b]. Once the APK file has been obtained, the decompilation could be done for analysis purpose.</p><p>The reverse engineering tool “dex2jar” has been used to decompile the APK file to the jar format. Then further Java decompilation has been done by “JD-GUI”. So far, the original source code should be perfectly reversed if there is no exception happened due to code obfuscation or other protection mechanism. Unfortunately, the class organisation of the code obtained from the decompilation of “Screenshot Ultimate” is not quite readable because the obfuscation is believed to be applied. Some core methods which control the logic flow of screenshot taking are missing. Clues could only be found by analysing package structure, libraries imported and source code from the remaining classes.</p><h4 id="ASL-Library-amp-Static-Analysis-of-Socket-Channel"><a href="#ASL-Library-amp-Static-Analysis-of-Socket-Channel" class="headerlink" title="ASL Library &amp; Static Analysis of Socket Channel"></a>ASL Library &amp; Static Analysis of Socket Channel</h4><p>Obfuscation cannot perfectly hide everything in the decompiled source code. After carefully reading through the source code of “Screenshot Ultimate”, we found some hints to shape the mechanism of screenshot taking. For example, there was an address in Android OS partition “<code>/system/bin/fbread</code>” appearing more than once in the obfuscated classes, just like the code snippet (<code>com.icecoldapps.screenshotultimate.an</code>) shown below: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (;;)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="keyword">this</span>.k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>.l) &#123;</span><br><span class="line">            <span class="keyword">break</span> label470;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="keyword">this</span>.m);</span><br><span class="line">        t = <span class="string">"/system/bin/fbread"</span>;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        Thread.sleep(<span class="number">200L</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Exception paramContext) &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The occurrence of “<code>/system/bin/fbread</code>” suggests that this app was very likely taking screenshot by reading image data from <em>framebuffer</em>, which is commonly expressed as short writing “fb” in Android development. </p><p>Many apps take screenshots by using a library called <em>Android screenshot library</em> (known as “ASL”) [c] A comparison has been done between the native executive provided by both ASL and “Screenshot Ultimate” to validate that presumption. The exact equivalence in file checksum value among two native executives as shown in Figure below indicates that the “Screenshot Ultimate” is one of the third party apps that take screenshot by invoking ASL APIs on unrooted devices.</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/checksum-comp-icecoldapps.png" title="Checksum of Native Executive Files from App I & ASL"><br></div></div><p>The ASL enables Android developer to write screenshot app without root requirement. Once the user followeds the instruction and executed the native executive file by running the given scripts, proxy with shell permission could help user take screenshot which the application has no privilege to do so. Take “Screenshot Ultimate” as example, user could just click the “Screenshot” button when user want to take screenshot of his/her device, then the app send the screenshot command to the proxy running in background via socket channel, following by the proxy as a process named “<code>screenshotultimatenative1</code>” reading the current hardware framebuffer, converting to the image format and saving to the specific location.</p><p>Hence the protocol of communication between app and proxy won’t be difficult to be sketched out. After writing a demo app using ASL API and running on the test devices, the result of screenshot taking was proved to be same as “Screenshot Ultimate”. The communication protocol to take a screenshot by invoking ASL library was shown in Figure below. </p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/process-diagram-screenshot-ultimate.png" title="Process of Application Taking Screenshot of App I"><br></div></div><h3 id="App-II-–-“No-Root-Screenshot-It”"><a href="#App-II-–-“No-Root-Screenshot-It”" class="headerlink" title="App II – “No Root Screenshot It”"></a>App II – “No Root Screenshot It”</h3><p>Similar with the “<em>Screenshot Ultimate</em>”, “<em>No Root Screenshot It</em>” is another screenshot app does not require rooting the devices. However, what makes “<em>No Root Screenshot It</em>” unique is the strong security and protection enforcement have been done during the development. Obfuscation has been conducted onto both service activator and APK. Meanwhile, the communication channel between app and proxy has also been protected by using some identification trick like a password, which will be pointed out later.</p><h4 id="Analysis-of-ADB-Channel-1"><a href="#Analysis-of-ADB-Channel-1" class="headerlink" title="Analysis of ADB Channel"></a>Analysis of ADB Channel</h4><p>The service activation has been done by executing a .Net application named “Screenshot It Enabler” rather than simply running a batch script. Therefore the analysis of the service activation will be more complicated because it is involved with the decompilation of .Net application. Moreover, the script file was not found in the enabler’s package, which means it has been packaged into the APK and the purpose of the enabler is just to run the “shell” command to execute it.</p><div align="center"><br><div style="width:40%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/activation-no-root-screenshotit.png" title="Proxy Activation of App II"><br></div></div><p>In this project, a .Net decompilation tool named “JetBrains dotPeek” [d] has been used to do the reverse engineering of the activation tool. Even though the enabler application has been obfuscated, some variables and C# code logics could still be recovered after the decompilation. The scripts to enable the proxy has been unveiled by observing the C# code from the decompilation result. In addition, the file name of the script could be confirmed as “screenshot” in this way. Then the script file’s location could be easily found by browsing the file manager on a rooted phone, or by decompiling the APK file to search the file name. The code snippet providing key clue is indicated below.</p><pre><code>startInfo2.FileName = &quot;adb.exe&quot;;startInfo2.Arguments = &quot;shell /data/data/&quot; + this.b + &quot;/screenshot daemon&quot;;</code></pre><h4 id="Static-Analysis-of-Socket-Channel"><a href="#Static-Analysis-of-Socket-Channel" class="headerlink" title="Static Analysis of Socket Channel"></a>Static Analysis of Socket Channel</h4><p>After clarifying the ADB communication to activate the proxy, the following steps will focus on discovering the communication between app and the proxy, thereby obtain the command(s) to control proxy to take screenshot at any occasion. On the APK side, obfuscation has been down very strongly onto both class names and variable names, which makes it impossible to observe the entire protocol by just reading the decompiled Java code. It is cleared that the class named <code>ScreenshotService</code> is in charge of the communication with the proxy but the code is not as readable as the previous one <em>Screenshot Ultimate</em>. What makes things worse is that one magic number, <code>89234820</code>, being found and being referenced multiple times by reading through the assembly code of <code>ScreenshotService</code> class. For this reason, it is proved that there is a great possibility that the app having (1) multiple communication session with proxy to take a screenshot; and/or (2) an authentication trick to indicate the app’s identity, which might be the reason of the existence of the magic number <code>89234820</code>.</p><pre><code>0xfe04: 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 |................|0xfe14: 38 39 32 33 34 38 32 30  00 00 00 00 00 00 00 00 |89234820........|0xfe24: 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00 |................|</code></pre><h4 id="Dynamic-Analysis-of-Socket-Channel"><a href="#Dynamic-Analysis-of-Socket-Channel" class="headerlink" title="Dynamic Analysis of Socket Channel"></a>Dynamic Analysis of Socket Channel</h4><p>Unlike what we have done in case study of app I, only static analysis is not adequate to find the protocol that used for communication between “<em>No Root Screenshot It</em>” and proxy. In order to unveil what kind of command that the app has sent to the proxy to take screenshot and how they interact with each other, a dynamic analysis technique called “hooking” was brought in this project.</p><p>Hooking system APIs on Android could be enabled by using a framework known as <em>Xposed</em> on a rooted device. By reading the decompiled code during the static analysis, the communication between proxy and app has been found conducted through socket channel of Android. Therefore, the monitoring of the socket channel during the communication between app and proxy could be done by writing a module based on Xposed framework which hook all the socket channel related packet data IO functions.</p><table><thead><tr><th><strong>Class Name</strong></th><th><strong>Method Name</strong></th></tr></thead><tbody><tr><td><code>screenShotServiceClass</code></td><td><code>takeScreenshot</code></td></tr><tr><td><code>screenShotServiceClass</code></td><td><code>read</code></td></tr><tr><td><code>screenShotServiceClass</code></td><td><code>write</code></td></tr></tbody></table><p> Methods Hooked in App II Dynamic Analysis</p><p>The <code>takeScreenshot</code> was hooked to indicate the beginning of communication.</p><div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/process-diagram-noroot-screenshot-it.png" title="Process of Application Taking Screenshot of App II"><br></div></div><p>After deploying the Xposed module named “hookNoRootScreenshotIt”, all the necessary data have been logged and printed out during the IO operation of the socket channel. The control flow of the app and proxy communication has been finally unveiled and shown in Figure above.</p><h3 id="App-III-–-“FREE-screen-recorder-NO-ROOT”"><a href="#App-III-–-“FREE-screen-recorder-NO-ROOT”" class="headerlink" title="App III – “FREE screen recorder NO ROOT”"></a>App III – “FREE screen recorder NO ROOT”</h3><p>In addition to screenshot apps, a screen recording app named “<em>FREE screen recorder NO ROOT</em>” has also been analysed in this project. According to the description on the Google Play Store, this app could enable users to record their screen regardless of which app or activity is on the top of stack, and then export the video recorded as MP4 format. The entire process doesn’t request users to root their devices.</p><h4 id="Proxy-Service-Identification"><a href="#Proxy-Service-Identification" class="headerlink" title="Proxy Service Identification"></a>Proxy Service Identification</h4><p>Similar with those two screenshot apps we have analysed previously, this app III also requires users to complete the proxy activation before the app unlocking the record function. However, this activation process on Windows OS is executed by running an <em>exe</em> file, and the same process on Linux OS is also started by a well compiled and packaged <em>jar</em> file, which makes the analysis on the proxy activation very difficult. </p><pre><code>&gt; adb shell psUSER     PID   PPID  VSIZE  RSS     WCHAN    PC         NAME......shell     26757 1     1084   240   c06f020c b6ef8500 S        /data/data/uk.org.invisibility.recordablefree/files/inputservshell     26760 1     1076   188   c06f020c b6f01500 S        /data/data/uk.org.invisibility.recordablefree/files/videoserv</code></pre><p> Missing of activation script doesn’t mean the identification of proxy is impossible. Actually, with the help of ADB, we could still find the details proxy(s) activated, including the process name, PID and the port(s) listening. Here, two ADB commends, “<code>ps</code>” and “<code>netstat</code>”, have been used to retrieve the list of running processes and active ports on the Android device. By this means the proxy and the ports number could be found. There are two services namely “<code>videoserv</code>” and “<code>inputserv</code>” running on the background to enable users to record their screen. One of them uses port 7938, and the other one uses port 7940 to communicate with app.</p><pre><code>&gt; adb shell netstatProto   Recv-Q  Send-Q  Local Address       Foreign     Address State......tcp 0   0   0.0.0.0:7938        0.0.0.0:*   LISTENtcp 0   0   0.0.0.0:7940        0.0.0.0:*   LISTEN</code></pre><h4 id="Decompilation"><a href="#Decompilation" class="headerlink" title="Decompilation"></a>Decompilation</h4><p>By now we have still yet to connect all the clues found in that last step because the port(s) listening did not tell us which services they are belonging to. For that reason, the decompilation is needed for the further analysis.</p><p>Firstly, the APK file has been extracted out of the device, and then been decompiled into samli code. The clues that recently found, two port numbers 7938 and 7940, could be searched within the source code to locate the key classes we will analyse on. As the searching result of keyword 7938 shown below, we found the variable name which bearing that port number, namely “<code>videoport</code>”. Similarly, the port 7940 has been found in variable name “<code>inputport</code>” recorded in same <em>xml</em> file.</p><pre><code>user{at}ubuntu:$ grep -rnw &apos;/home/user/Documents/Recordable Free/&apos; -e &quot;7938&quot;/home/user/Documents/Recordable Free/res/values/integers.xml:6:        &lt;integer name=&quot;video\textunderscore port&quot;&gt;7938&lt;/integer&gt;</code></pre><p>Next, we continued searching the occurrence of two variables “<code>videoport</code>” and “<code>audioport</code>”. After filtering from the search result, we preliminarily confirmed that the code reflecting the control flow and data flow was located in class “<code>RecordService</code>” and “<code>Projection</code>” separately.</p><p>Take the communication between video server and app as example, the core function in charge of the communication flow is supposed to be “<code>videoWrite</code>”, which located in line 1038 in the smali code of <code>RecordService</code>. This <code>videoWrite</code> method has been called many time once after the occurrence of the constant string with all letter being capitalised, which is suspected to be the command sending to the server. Moreover, by browsering through the smali code, a method named “<code>openSocket</code>” has been called within the class <code>RecordService</code>, which helps us to come up with a presumption that the protocol we are going to discover was achieved through socket channel.</p><pre><code>.line 1038const-string v1, &quot;AUTH &quot;invoke-virtual {p0, v1},Luk/org/invisibility/recorder/service/RecordService;             -&gt;videoWrite(Ljava/lang/String;)V.line 1036const-string v1, &quot;127.0.0.1&quot;invoke-virtual {p0}, Luk/org/invisibility/recorder/service/RecordService;             -&gt;getResources()Landroid/content/res/Resources;move-result-object v5sget v6, Luk/org/invisibility/recorder/core/R$integer;-&gt;video_port:Iinvoke-virtual {v5, v6}, Landroid/content/res/Resources;-&gt;getInteger(I)Imove-result v5invoke-static {v1, v5}, Luk/org/invisibility/recorder/service/RecordService;             -&gt;openSocket(Ljava/lang/String;I)Imove-result v1iput v1, p0, Luk/org/invisibility/recorder/service/RecordService;             -&gt;mVideoReadFd:I</code></pre><h4 id="Dynamic-Analysis-on-Socket-Channel"><a href="#Dynamic-Analysis-on-Socket-Channel" class="headerlink" title="Dynamic Analysis on Socket Channel"></a>Dynamic Analysis on Socket Channel</h4><p>Like what we have done onto the app II, hooking by exposed is always considered the most convenient and effective way to sketch out the complete control flow and data flow of the protocol. The target function to be hooked has been confirmed during the previous analysis, which is “<code>videoWrite</code>” located in class named “<code>RecordService</code>”. In order to find as many details about the protocol as possible, some other methods located in the same class of “<code>videoWrite</code>” have also been hooked and shown in Table below. With the information obtained from the output logs of methods’ hooking, the protocol of the communication between video server and app to start screen recording has been found and displayed below</p><table><thead><tr><th><strong>Class Name</strong></th><th><strong>Method Name</strong></th></tr></thead><tbody><tr><td><code>RecordService</code></td><td><code>videoWrite</code></td></tr><tr><td><code>RecordService</code></td><td><code>writeCaptureOption</code></td></tr><tr><td><code>RecordService</code></td><td><code>startListen</code></td></tr><tr><td><code>RecordService</code></td><td><code>stopListen</code></td></tr><tr><td><code>RecordService</code></td><td><code>startCountdown</code></td></tr><tr><td><code>RecordService</code></td><td><code>startRecord</code></td></tr><tr><td><code>RecordService</code></td><td><code>stopRecord</code></td></tr></tbody></table><p>Methods Hooked in App III Dynamic Analysis</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/analysis-recordable-free.png" title="Process of Screen Recording on App III"><br></div></div><h4 id="Poaching-the-Passcode"><a href="#Poaching-the-Passcode" class="headerlink" title="Poaching the Passcode"></a>Poaching the Passcode</h4><p>A sixteen-digit-long string <code>01c5cdf96b5a2a63</code> as showed in Figure above grabbed our attentions because it was presumed to be the authentication code, or password for short, according to the location of its occurrence. Nevertheless, it has not yet been confirmed to be a string constant or a dynamic changing string so far. In order to clarify the nature of that password, a series of experiments has been conducted.</p><p>Firstly, we close the app after taking a screen recording video clips, then re-opened it and took another screen recording. Hooking logs shown in logcat console showed that the password did’t change. In that means, the password is independent of app’s life-cycle. We have repeated the above steps for many times and all the results proved the conclusion is correct. Next, we killed all proxies related to this app and did the service activation again. The purpose of this experiment is to determine if the password is independent of the life cycle of proxies. If answer is yes, that means this password is generated by one of the proxies, and will keep updating each time re-activating the proxies, such as rebooting the device. On the contrary, the password would be confirmed to be a constant, which is same as the magic number in the case study of app II, if there is no evidence indicating that the password itself varies with different instance of proxy. After many times repeating the above steps, the password was found changed each time we re-activate the proxies.</p><p>Since the password is proved to be generated by proxy, there must be a place that the proxy stored the code at somewhere that the app with user privilege could access and read. This idea was generated by recalling the analysis on a backup app called “<em>Helium</em>” in paper of Bai et al. [1]. After a series of searching initiated by this idea, we finally located the password in a <em>log</em> file named <code>videoserv.log</code> under the directory <code>data/local/tmp</code>, and luckily found the first occurrence of the current password was always after the word “<code>AUTH</code>” in the log file. For that reason, the attackers could all along hold the current password by writing a simple code snippet on Android to read the log file and then extract the string at given location.<br>    …<br>    12618: ready (1408)<br>    12618: client_read_fd: 4<br>    12618: client_write_fd: 4<br>    12618: AUTH 01c5cdf96b5a2a63 (1422)<br>    12618: cmd_auth (1072)<br>    12618: key accepted (1076)<br>    12618: SET VideoScale SCALE_FULL (1422)<br>    …</p><p> [a]: <span><a href="http://www.apkmirror.com/" target="_blank" rel="noopener">http://www.apkmirror.com/</a></span></p><p> [b]: Apk Extractor could be downloaded at<br>    <span><a href="https://play.google.com/store/apps/details?id=com.ext.ui&amp;hl=en" target="_blank" rel="noopener">https://play.google.com/store/apps/details?id=com.ext.ui&amp;hl=en</a></span></p><p> [c]: Android screenshot library is available at<br>    <span><a href="https://code.google.com/archive/p/android-screenshot-library/downloads" target="_blank" rel="noopener">https://code.google.com/archive/p/android-screenshot-library/downloads</a></span></p><p> [d]: dotPeek is available on<br>    <span><a href="https://www.jetbrains.com/decompiler/" target="_blank" rel="noopener">https://www.jetbrains.com/decompiler/</a></span></p><h1 id="Exploitation"><a href="#Exploitation" class="headerlink" title="Exploitation"></a>Exploitation</h1><p>The processes of privilege escalation in three apps have been completely unveiled during the analysis and described in last chapter. Theoretically speaking, if there is a malicious app being installed on your device while the proxy running in background of OS, behaving exactly same as the original screenshot app, it can unperceivably steal the screenshot of your device at any time on any occasion. In this project, the app II is chosen to be conducted an exploitation to prove the privacy vulnerabilities of ADB workaround.</p><p>Unlike the back-up app named <em>Helium</em> mentioned in the paper of Bai, et al. [1], app II doesn’t implement a dynamic identity schemes. That means the exploitation of app II in this project, could be conducted in a very straight forward manner. Besides that, the malicious app could also co-exist with the genuine App. In this chapter, the exploitation processes of app II will be introduced and the outcome of exploitation will be shown at the end of this chapter.</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/exploit-sequence-diagram-edward.png" title="Sequence Diagram of Exploitation of App II"><br></div></div><p>An app named “<em>exploitNoRootScreenshotIt</em>” simulating the malicious exploitation of the app II had been implemented for the demonstration purpose. In that exploitation app, there are in total four messages being organized into two lots and sent out to the <code>localhost</code> on port <code>6003</code> through Android’s socket channel. The first two messages are used for the configuration purpose, and the last two messages are sent out as screenshot taking command once the acknowledgement of first batch messages have been received from the proxy, which is “<code>screenshotService</code>” running in the background. The entire process was displayed in figure above.</p><p> The screenshot obtained is converted to a <em>bmp</em> file under the sub-directory named “<code>temp</code>” [a]. The access permission of that folder was set as read-only to the user group. Therefore, once the screenshot has been taken by the proxy, the exploitation app could access to the newly captured screenshot located in the “<code>temp</code>” folder and make a copy to the target location such as folder under external storage “<code>/sdcard/hack_screenshots/</code>”.</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2018/08/21/analysing-high-privileges/tutorial-exploit.png" title="Steps to Take a Screenshot in Exploitation App"><br></div></div><p>The screenshot image is renamed according to the capture time to avoid being overwritten and convenient maintenance at the same time. The exploitation has been tested on two devices (one <em>Nexus 7</em> and one <em>Xiaomi Rednote 3G</em> ) and worked well just like the genuine app “<em>No Root Screenshot It</em>”. This exploitation could even been further designed and programmed to take screenshot automatically with specific frequency without any notice of user, the user’s privacy could be consequently exposed to attacker. </p><p> [a]: The full directory path is <code>/data/data/com.edwardkim.android.screenshotitfullnoroot/temp</code></p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>There are some previous studies unveiling the security risk of ADB workaround despite it is considered much safer than device rooting. Security concern of ADB workaround mainly comes from the difference between roles of proxy and application on Android OS. In this project, these risks could be summarised into two types –</p><p>(1) whether other apps could send commend to the opening proxy; and –</p><p>(2) whether the communication between app and proxy is properly protected if the scenario of (1) is possible to happen.</p><p>The description of the first kind of security concern could be found in the paper written in 2014 by Lin, et al. The communication channel between the application and its ADB proxy relies on network sockets without any protection enforced. For that reason, once an ADB proxy has been activated, any application has the privilege to communicate with it and even request service from it at any time without restrictions. This vulnerability gives attackers a chance to analyse the protocol of such communication and build a malicious application to request service from ADB proxy exactly as same as what genuine application does [14]. </p><p>Some developers have realised the fact that the communication channel between the application and ADB proxy may be risky, and therefore implemented some authentication routines to strengthen security. However in the paper of Bai, et al, it was proved that such authentication was ineffective as long as the reverse engineering and analysis being feasible on given application. What developer can do to secure the communication is only applying some basic authentication since there is no way to enforce strong protection onto the socket network. That authentication is usually very weak in front of analysis [1]. Some application like “<em>Helium</em>”, a backup/restore application mentioned in the paper written by Bai, et al., has been found using protection during the communication between application and ADB proxy. ADB proxy requests a password that sent out from a specific process to provide service. Unfortunately, vulnerability was found in the protocol of password distribution. The password generated each time when ADB proxy being activated, and it is independent of app’s life cycle. In this way, the proxy has to find a place that readable by apps executed with user group privilege, save the password into a file and waiting for app to read from it. This life cycle inconsistency makes adversary possible to find the current using password and thereby exploit the genuine app by writing another app following exactly same protocol in communication with the ADB proxy.</p><h1 id="Conclusion-amp-Future-Work"><a href="#Conclusion-amp-Future-Work" class="headerlink" title="Conclusion &amp; Future Work"></a>Conclusion &amp; Future Work</h1><p>In this project, we have come up with an approach to find security vulnerabilities of ADB workaround. By conducting case studies on three different apps, we found most of apps using ADB workaround have risk of being exploited. Followed by case studies, an exploitation has been done successfully, which proves and verifies the approach we have summarised in the analysis.</p><p>This project could be treated as not only a support to those previous studies on the similar field, but also a reference to the Android developers. The successful exploitation in this project is sufficient to show that the security and protection enforcement of the Android apps during design and development periods is crucial to users’ privacy and doomed to be an endless and challenging journey.</p><p>Some suggestions on Android development have been summarised throughout the study and research in this project. The Android developers are advised to raise security awareness and take some security practices into account when implementing the functionality based on ADB workaround, including:</p><ol><li><p><strong>Access control for socket channel communication</strong>. An access control protocol is strongly advised to be implemented on both app and proxy sides. It could be like exchanging of passcode, to enable the proxy to validate the identity of app. A good access authentication should not been exploited by analysing on solely one side among app and proxy. In this way, the proxy could reject to provide service when the command received through socket channel failed to validate the identity of app. The objective of this advice is to solve the problem (1) mentioned in Chapter 5.</p></li><li><p><strong>Identification for application</strong>. Only the access control is not enough to ensure the security when facing to the second problem described in Chapter 5. One possible solution for this issue may be writing a <em>handshake</em> process in the proxy implementation, to make both app and proxy exchange their authentication. And the ADB proxy will execute the commend only after a successful validation. Thus the proxy service will only accept the command sent from the exactly same app. Once the app is removed and re-installed, regardless of genuine app or malicious app, another handshake validation will be required thereby to ensure the ADB proxy will not be misused.</p></li></ol><h1 id="Acknowledge"><a href="#Acknowledge" class="headerlink" title="Acknowledge"></a>Acknowledge</h1><p>As a postgraduate project in infocomm security area, this project has been a challenging but wonderful experience throughout my entire mater programme. The numberless difficulties encountered and ceaseless confusion experienced did not defeat me, but make me stronger and more optimistic in study and research. My knowledge has been greatly broadened and my hands-on skills have also been notably enhanced during the past few months.</p><p>I wish to take this chance to express my deepest gratitude to my supervisor and advisor, <em>Associate Profess Dong Jin Song</em> and <em>Doctor Bai Guangdong</em>, for their guidance in this project. My most sincere thanks to Doctor Bai for his constant encouragement and inspiration. I will definitely not able to learn so much and come up with this report without his generous help and support. Last but not least, a special thanks to my friends and my family for their support and advice in both academic and psychological aspects.</p><h1 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h1><p>[1] G. Bai, J. Sun, J. Wu, Q. Ye, L. Li, J. S. Dong, and S. Guo. All Your Sessions are Belong to us: Investigating Authenticator Leakage through Backup Channels on Android. In Proceedings of the 20th International Conference on Engineering of Complex Computer Systems (ICECCS), 2015.<br>[2] M. Casserly. How to root Android Marshmallow, Lollipop and older versions of Android: The beginner’s guide to rooting, risks and benefits. How to install the latest version of Android, and how to install custom ROMs including CyanogenMod. <a href="http://www.pcadvisor.co.uk/how-to/google-android/" target="_blank" rel="noopener">http://www.pcadvisor.co.uk/how-to/google-android/</a> how-root-android-phone-tablet-unroot-summary-3342120/, January 2016. Online; Accessed: 2016-03-20.<br>[3] A. S. Constantinescu. Ensuring privacy in the android os by hooking methods in its api. Journal of Mobile, Embedded and Distributed Systems, 7(3):107-112, 2015.<br>[4] Council of European Union. Directive 2009/24/ec of the european parliament and of the council of 23 april 2009 on the legal protection of computer programs (codified version), 2009.<a href="http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32009L0024" target="_blank" rel="noopener">http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32009L0024</a>.<br>[5] J. Freke. smali/baksmali - GitHub. <a href="https://github.com/JesusFreke/smali" target="_blank" rel="noopener">https://github.com/JesusFreke/smali</a>. Online; Accessed: 2016-03-23.<br>[6] Google. <permission>, Android Developers. http: //developer.android.com/guide/topics/manifest/permission-element.html, . Online; Accessed: 2016-03-18. 33<br>[7] Google. Proguard. <a href="http://developer.android.com/tools/help/proguard.html" target="_blank" rel="noopener">http://developer.android.com/tools/help/proguard.html</a>, . Online; Accessed: 2016-03-23.<br>[8] Google. System and kernel security. <a href="https://source.android.com/security/overview/kernel-security.html" target="_blank" rel="noopener">https://source.android.com/security/overview/kernel-security.html</a>, . Online; Accessed: 2016-03-18.<br>[9] GuardSquare. Dexguard. <a href="https://www.guardsquare.com/dexguard" target="_blank" rel="noopener">https://www.guardsquare.com/dexguard</a>. Online; Accessed: 2016-03-23.<br>[10] C. Hoffman. How-To Geek: How to Take Screenshots on Android Devices Since 4.0. <a href="http://www.howtogeek.com/121133/" target="_blank" rel="noopener">http://www.howtogeek.com/121133/</a> how-to-take-screenshots-on-android-devices-since-4.0, June 2012. Online; Accessed: 2016-03-20.<br>[11] M. Hofmarcher, M. Straut, and W. Narzt. Cross-platform end-to-end encryption of contact data for mobile platforms using the example of android. 2014.<br>[12] IDC. Worldwide smartphone growth expected to slow to 10.4% in 2015, down from 27.5% growth in 2014, according to IDC. <a href="http://www.idc.com/getdoc.jsp?containerId=prUS25860315" target="_blank" rel="noopener">http://www.idc.com/getdoc.jsp?containerId=prUS25860315</a>, 2015. Online; Accessed: 2016-03-14.<br>[13] IDC. Smartphone OS Market Share, 2015 Q2. <a href="http://www.idc.com/prodserv/smartphone-os-market-share.jsp" target="_blank" rel="noopener">http://www.idc.com/prodserv/smartphone-os-market-share.jsp</a>, 2015. Online; Accessed: 2016-03-14.<br>[14] C.-C. Lin, H. Li, X. -Y. Zhou, and X. Wang. Screenmilker: How to Milk Your Android Screen for Secrets. In NDSS, 2014.<br>[15] K. Lucic. Over 27.44% Users Root Their Phone(s) In Order To Remove Built-In Apps, Are You One Of Them? <a href="http://www.androidheadlines.com/2014/11/50-users-root-phones-order-remove-built-apps-one.html" target="_blank" rel="noopener">http://www.androidheadlines.com/2014/11/50-users-root-phones-order-remove-built-apps-one.html</a>, November 2014. Online; Accessed: 2016-03-14. 34<br>[16] G. Nolan. Decompiling android. Apress, 2012.<br>[17] N. Provos, M. Friedl, and P. Honeyman. Preventing Privilege Escalation. In USENIX Security, volume 3, 2003.<br>[18] E. Ravenscraft. Rooted vs. Unrooted Android: Your Best Arguments. <a href="http://lifehacker.com/" target="_blank" rel="noopener">http://lifehacker.com/</a> rooted-vs-unrooted-android-your-best-arguments-1599101019, July 2014. Online; Accessed: 2016-03-18.<br>[19] rovo89. Xposed Module Repository. <a href="http://repo.xposed.info/" target="_blank" rel="noopener">http://repo.xposed.info/</a>, . Online; Accessed: 2016-03-23.<br>[20] rovo89. XposedBridge Development tutorial. <a href="https://github.com/rovo89/XposedBridge/wiki/Development-tutorial" target="_blank" rel="noopener">https://github.com/rovo89/XposedBridge/wiki/Development-tutorial</a>, . Online; Accessed: 2016-04-02.<br>[21] A. Rubin. There have been half a billion android activations to date, with over 1.3m added every day. <a href="https://twitter.com/Arubin/status/245663570812100608" target="_blank" rel="noopener">https://twitter.com/Arubin/status/245663570812100608</a>, September 2012. Online; Accessed: 2016-03-14.<br>[22] A. Shabtai, Y. Fledel, U. Kanonov, Y. Elovici, S. Dolev, and C. Glezer. Google android: A comprehensive security assessment. IEEE Security &amp; Privacy, (2):35{44, 2010.<br>[23] U.S.C. U.s. code: Title 17, chapter 12, par. 1201. circumvention of copyright protection systems, 1999. Available at: <a href="https://www.law.cornell.edu/uscode/text/17/1201" target="_blank" rel="noopener">https://www.law.cornell.edu/uscode/text/17/1201</a>.<br>[24] Wikipedia. Reverse Engineering - Wikipedia, the free encyclopedia. <a href="https://en.wikipedia.org/wiki/Reverse_engineering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Reverse_engineering</a>, February 2016. Online; Accessed: 2016-03-23.<br>[25] Wikipedia. Rooting (Android OS) - Wikipedia, the free encyclopedia. <a href="https://en.wikipedia.org/wiki/Rooting_(Android_OS)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Rooting_(Android_OS)</a>, February 2016. Online; Accessed: 2016-03-18.</permission></p><h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="Abbreviation"><a href="#Abbreviation" class="headerlink" title="Abbreviation"></a>Abbreviation</h2><table><thead><tr><th><strong>Abbreviation</strong></th><th><strong>Full Word/Phrase</strong></th></tr></thead><tbody><tr><td>ADB</td><td>Android Debug Bridge</td></tr><tr><td>API</td><td>Application Programming Interface</td></tr><tr><td>APK</td><td>Android Application Package</td></tr><tr><td>App</td><td>Application Software</td></tr><tr><td>ASL</td><td>Android Screenshot Library</td></tr><tr><td>DEX</td><td>Dalvik Executable</td></tr><tr><td>DVM</td><td>Dalvik Virtual Machine</td></tr><tr><td>IO</td><td>Input/Output</td></tr><tr><td>IPC</td><td>Inter process communication</td></tr><tr><td>JVM</td><td>Java Virtual Machine</td></tr><tr><td>NFC</td><td>Near Field Communication</td></tr><tr><td>OS</td><td>Operating System</td></tr><tr><td>PC</td><td>Personal Computer</td></tr><tr><td>ROM</td><td>Read-only Memory</td></tr><tr><td>UI</td><td>User Interface</td></tr><tr><td>USB</td><td>Universal Serial Bus</td></tr></tbody></table><h2 id="Statistics-of-Screenshot-Application"><a href="#Statistics-of-Screenshot-Application" class="headerlink" title="Statistics of Screenshot Application"></a>Statistics of Screenshot Application</h2><table><thead><tr><th><strong>Name</strong></th><th><strong>Unrooted Method</strong></th><th><strong>Size</strong></th><th><strong>Identity Package Name</strong></th></tr></thead><tbody><tr><td>Screen Capture - Sigourney</td><td>Hardkey</td><td>5.2M</td><td>com.mobilescreen, capture</td></tr><tr><td>Screenshot Easy</td><td>Hardkey</td><td>5.2M</td><td>com.icecoldapps, screenshoteasy</td></tr><tr><td>Screenshot Ultimate</td><td>ADB</td><td>3.2M</td><td>com.icecoldapps, screenshotultimate</td></tr><tr><td>Screenshot Capture</td><td>Hardkey</td><td>3.1M</td><td>com.tools, screenshot</td></tr><tr><td>NoRoot Screenshot Lite</td><td>N.A [a]</td><td>545k</td><td>com.mobikasa, screenshot.lite</td></tr><tr><td>Screenshot and Draw</td><td>N.A.</td><td>1.1M</td><td>com.conditiondelta, screenshotanddraw.trial</td></tr><tr><td>Screenshot</td><td>Hardkey</td><td>2.4M</td><td>com.enlightment, screenshot</td></tr><tr><td>Screenshot</td><td>Hardkey</td><td>1.2M</td><td>com.geekslab, screenshot</td></tr><tr><td>Screenshot</td><td>Hardkey [b]</td><td>4.86M</td><td>com.icondice, screenshot</td></tr><tr><td>Screenshot</td><td>N.A.</td><td>2.3M</td><td>com.geeksoft, screenshot</td></tr><tr><td>Screenshot ER Demo</td><td>N.A.</td><td>3.2M</td><td>fahrbot.apps, screen.demo</td></tr><tr><td>No Root Screenshot It</td><td>ADB</td><td>838k</td><td>com.edwardkim, android.screenshotitfullnoroot</td></tr><tr><td>Screenshot It</td><td>N.A.</td><td>840k</td><td>com.edwardkim, android.screenshotitfull</td></tr></tbody></table><p>[a] N.A. indicates that application only work on rooted devices.<br>[b] only compatible with devices made by some fixed manufactures.</p><h2 id="Code-Snippets-of-Hooking-App-II"><a href="#Code-Snippets-of-Hooking-App-II" class="headerlink" title="Code Snippets of Hooking App II"></a>Code Snippets of Hooking App II</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">XposedBridge.log(<span class="string">"Loaded app: "</span> + lpparam.packageName);</span><br><span class="line">Log.d(<span class="string">"hookEdward"</span>, <span class="string">"Loaded app: "</span> + lpparam.packageName);</span><br><span class="line"><span class="keyword">if</span> (lpparam.packageName.equals(<span class="string">"com.edwardkim.android.screenshotitfullnoroot"</span>)) &#123;</span><br><span class="line">    XposedBridge.log(<span class="string">"Screenshot App Found : "</span> + lpparam.packageName);</span><br><span class="line">    Class&lt;?&gt; screenShotServiceClass = XposedHelpers</span><br><span class="line">    .findClass(<span class="string">"com.edwardkim.android.screenshotit.services.ScreenShotService"</span>, </span><br><span class="line">    lpparam.classLoader);</span><br><span class="line"></span><br><span class="line">    XposedBridge.hookAllMethods(screenShotServiceClass, <span class="string">"read"</span>, <span class="keyword">new</span> XC_MethodHook() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">beforeHookedMethod</span><span class="params">(MethodHookParam param)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">            XposedBridge.log(<span class="string">"Before Hooking (r) -- param.args[0] "</span> + param.args[<span class="number">0</span>]);</span><br><span class="line">            XposedBridge.log(<span class="string">"Before Hooking (r) -- param.args[1] "</span> + </span><br><span class="line">            <span class="keyword">new</span> String((<span class="keyword">byte</span>[]) param.args[<span class="number">1</span>]));</span><br><span class="line">            XposedBridge</span><br><span class="line">            .log(<span class="string">"Before Hooking (r) -- param.args[1] "</span> + </span><br><span class="line">            <span class="keyword">new</span> String((<span class="keyword">byte</span>[]) param.args[<span class="number">1</span>]).length());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">afterHookedMethod</span><span class="params">(MethodHookParam param)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">            XposedBridge.log(<span class="string">"After Hooking (r) -- param.args[0] "</span> + param.args[<span class="number">0</span>]);</span><br><span class="line">            XposedBridge.log(<span class="string">"After Hooking (r) -- param.args[1] "</span> + </span><br><span class="line">            <span class="keyword">new</span> String((<span class="keyword">byte</span>[]) param.args[<span class="number">1</span>]));</span><br><span class="line">            XposedBridge</span><br><span class="line">            .log(<span class="string">"After Hooking (r) -- param.args[1] "</span> + </span><br><span class="line">            <span class="keyword">new</span> String((<span class="keyword">byte</span>[]) param.args[<span class="number">1</span>]).length());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Relevant-Materials-For-This-Project"><a href="#Relevant-Materials-For-This-Project" class="headerlink" title="Relevant Materials For This Project"></a>Relevant Materials For This Project</h2><p>You are free to download relecant materials used/mentioned in this report (for non-commercial purpose only).<br>(1) <a href="../../../../attachments/analysing-use-of-high-privileges/apk_folder.zip">Download</a> APKs used in project.<br>(2) <a href="../../../../attachments/analysing-use-of-high-privileges/tools_folder.zip">Download</a> tools mentioned in this rpeort.<br>(3) <a href="../../../../attachments/analysing-use-of-high-privileges/source_code_folder.zip">Download</a> source code of hooking &amp; exploitation.</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot; style=&quot;padding-bottom:15px;&quot;&gt;&lt;img src=&quot;/blog/2018/08/21/analysing-high-privileges/android.jpg&quot;&gt;&lt;/div&gt;

&lt;p&gt;This project is an extension to my thesis for the degree of master in computing at National University of Singapore, School of Computing. This work talks about the most popular un-rooted approach to escalate privilege on Android OS.&lt;br&gt;
    
    </summary>
    
    
      <category term="academic, thesis, mobile, security, exploit" scheme="http://mmhhss1991.github.io/tags/academic-thesis-mobile-security-exploit/"/>
    
  </entry>
  
  <entry>
    <title>The Application of Hologram into Museum Exhibition</title>
    <link href="http://mmhhss1991.github.io/2016/06/17/hologram/"/>
    <id>http://mmhhss1991.github.io/2016/06/17/hologram/</id>
    <published>2016-06-17T03:35:46.000Z</published>
    <updated>2018-09-01T06:39:37.745Z</updated>
    
    <content type="html"><![CDATA[<img src="/blog/2016/06/17/hologram/cover.png"><!--div align="center">    <img src="images/2016/06/3d-holographic-cvr.png" alt="Hologram is amazing..." width="100%"></div--><p>This is my part-time contract at the museum of Nanyang Technological University (Singapore) during my vocation of post-graduate study. It may be the first ever pyramid hologram display in exhibition usage in Singapore. This article will explian what I have prepared and experienced throughout the project.<br><a id="more"></a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1>]]></content>
    
    <summary type="html">
    
      &lt;img src=&quot;/blog/2016/06/17/hologram/cover.png&quot;&gt;
&lt;!--div align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/2016/06/3d-holographic-cvr.png&quot; alt=&quot;Hologram is amazing...&quot; width=&quot;100%&quot;&gt;
&lt;/div--&gt;
&lt;p&gt;This is my part-time contract at the museum of Nanyang Technological University (Singapore) during my vocation of post-graduate study. It may be the first ever pyramid hologram display in exhibition usage in Singapore. This article will explian what I have prepared and experienced throughout the project.&lt;br&gt;
    
    </summary>
    
    
      <category term="technical" scheme="http://mmhhss1991.github.io/tags/technical/"/>
    
  </entry>
  
  <entry>
    <title>Business Analytics - What Motivates You to Open a Restaurant There?</title>
    <link href="http://mmhhss1991.github.io/2016/03/24/biz-analytics/"/>
    <id>http://mmhhss1991.github.io/2016/03/24/biz-analytics/</id>
    <published>2016-03-24T11:05:27.000Z</published>
    <updated>2018-09-01T08:53:00.653Z</updated>
    
    <content type="html"><![CDATA[<div align="center" style="padding-bottom:15px;"><img src="/blog/2016/03/24/biz-analytics/cvr-analytics.jpg"></div><p>This project is carried out in class of IS5126 Hands-on with Business Analytics at National University of Singapore, School of Computing. This project uses R and database techniques to conduct a simple analytics.<br><a id="more"></a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>The total number of restaurants in the USA is now over 616 thousands and keeps an increase of 7 percent per year. Restaurants, as a typical and important business tightly connected with people’s daily life, are often treated as an indicator of local economic scale and resident income, reflecting people’s quality of life in that region. However, there are no previous studies unveiling the real factors that influence the number of restaurants in a region during the last few decades when the total number of restaurants in the USA ever experienced the greatest growth. In this project, we collect data from the largest restaurant review website in the USA named “Yelp” and a demographic information website “City-Data”, make use of statistical models and thereby summarize the result obtained from statistical analysis tools. Eight factors have been proved to be the significant variables that have a linear relationship with the number of restaurants in an area defined by ZIP code. We interpret these eight factors under the social circumstance of the USA and analyse the reason behind. Last but not least, some suggestions have been provided to those people who plan to open a restaurant in USA to help them make a decision on location choosing.</p><h1 id="Background-amp-Objective"><a href="#Background-amp-Objective" class="headerlink" title="Background &amp; Objective"></a>Background &amp; Objective</h1><p>Restaurant is a typical and important business which is tightly connected with people’s daily life and accurately reflects local economic vitality and residents’ income. According to a statistics done by NDP, there are over 616 thousands restaurants opening in the United States and 7 percent increase in total number every year. People always prefer to live in a region with sufficient dining business, therefore it provides great opportunity and motivation to investors to open a restaurant. However, the location choosing is one of the most critical things for an investor to consider. The population, economy, security and other demographic factors of an area may affect the customers’ behavior and consumption, and thereby impact the return of restaurant business. In this project, we are going to make use of the data from the largest dining information website named “Yelp” and one biggest demographic data website in US called “city-data.com”, build an analytic model and then discover the potential demographic factors that may influence the number of restaurant business in a specific region in US. Our finding in this project could not only unveil the connection between demographic data and the economic scale of dining industry in USA, but also provide suggestions to those potential restaurant owners to choose a good location to open their business.</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><h2 id="Data-Selection"><a href="#Data-Selection" class="headerlink" title="Data Selection"></a>Data Selection</h2><p>Our project goal is to find out the correlation between restaurants and attributes of a location. We selected yelp data set as our source of information of restaurants in USA. The yelp restaurant data set is provided by the USA based restaurant review site YELP. It includes six main object types, business, review, user and others. It has 2.2M reviews and 591K tips provided by 552K users for 77K businesses, along with 566K business attributes, e.g. Opening hours, ambiance, parking availability. The data was collected from mainly Nevada and Arizona.<br>The data set is available for download and already in processed format. We have chosen the business object as the main reference data for our analysis. The data is provided in JSON format, with detail template as given below.</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">business</span><br><span class="line">&#123;</span><br><span class="line">    'type': 'business',</span><br><span class="line">    'business_id': (encrypted business id),</span><br><span class="line">    'name': (business name),</span><br><span class="line">    'neighborhoods': [(hood names)],</span><br><span class="line">    'full_address': (localized address),</span><br><span class="line">    'city': (city),</span><br><span class="line">    'state': (state),</span><br><span class="line">    'latitude': latitude,</span><br><span class="line">    'longitude': longitude,</span><br><span class="line">    'stars': (star rating, rounded to half-stars),</span><br><span class="line">    'review_count': review count,</span><br><span class="line">    'categories': [(localized category names)]</span><br><span class="line">    'open': True / False (corresponds to closed, not business hours),</span><br><span class="line">    'hours': &#123;</span><br><span class="line">        (day_of_week): &#123;</span><br><span class="line">            'open': (HH:MM),</span><br><span class="line">            'close': (HH:MM)</span><br><span class="line">        &#125;,</span><br><span class="line">        ...</span><br><span class="line">    &#125;,</span><br><span class="line">    'attributes': &#123;</span><br><span class="line">        (attribute_name): (attribute_value),</span><br><span class="line">        ...</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As we can see from above, each data entry contains the name, type, address, overall rating and other attributes. The zip code of the restaurant is available as part of the address field, which is later used as the joining attribute to join the restaurant data set with the city data set.<br>We selected <a href="http://www.city-data.com" target="_blank" rel="noopener">www.city-data.com</a> as the source of city related data. This website allows searching via zip code, and detail information about the city such as average household income, percentage of renters or population per sex offender.<br>The data is displayed on web page per zip code. Since our restaurant data set contains mainly data from Nevada and Arizona, we have collected all the current zip codes located in the above mentioned states. Then we have written a Python program to crawl the web according to the zip code list and grab the useful data. We selected 31 attributes in each zip code, as listed below.</p><ul><li>Zip Code</li><li>Avg. household income</li><li>Renters number</li><li>Living cost</li><li>Population density</li><li>Males number</li><li>Females number</li><li>High school educated %</li><li>Bachelor educated %</li><li>Master and up educated %</li><li>Unemployment %</li><li>Married %</li><li>Separated %</li><li>Widowed %</li><li>Divorced %</li><li>Avg. house value</li><li>Houses number</li><li>Population per sex offender</li><li>Lesbian %</li><li>Gay %</li><li>Married Household %</li><li>Unmarried Household %</li><li>Poverty resident %</li><li>Median age</li><li>Household Number</li><li>Married household number</li><li>Unmarried household number</li><li>Number of Married couples with child</li><li>Number of Single parent households</li><li>Housing unit without plumbing %</li><li>Housing unit without kitchen %        </li></ul><h2 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h2><p>The yelp data set is provided in JSON format, with one object type per file and one record per line. We imported the yelp data into SQLite database for easier data cleaning. From there, we performed several commands to calculate some useful statistics of the restaurants in each zip code region, for example the average rating, rating variance, highest rating and so on.<br>The city data is crawled from web sites and stored in .csv file separated by commas. The city data was also imported into the same database. We used zip code as identifier and joined city data together with the yelp data set. The final data was exported to csv file for further analysis. </p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2016/03/24/biz-analytics/figure-1.png" title="Sample of the final data set"><br></div></div><h1 id="Analytics"><a href="#Analytics" class="headerlink" title="Analytics"></a>Analytics</h1><p>We use the number of restaurants as the dependent variable, and independent variables are houses (the number of houses and condos), renters (the number of renter-occupied apartments), cost (Mar. 2013 cost of living index in zip code), density (population density), males (males population), females (females population), sexOffenders (the number of residents per sex offenders), medianAge (median resident age), householdNum, inNonFamilyHouseholdNum, numMarriedCouplesWithChild, numSingleParentHouseholds, highschool (the percentage of high school or higher for population 25 years), bachelor (the percentage of bachelor’s degree or higher for population 25 years), professional (the percentage of graduate or professional degree for population 25 years), unemployed (the percentage of unemployed for population 25 years), married (the percentage of married), separated (the percentage of separated), widowed (the percentage of widowed), divorced (the percentage of divorced), lesbian (the percentage of lesbian couples), gay (the percentage of gay men), familyHousehold, unmarriedHousehold (the percentage of households with unmarried partners), povertyResident (residents with income below the poverty level in 2013), housingUnitW/oPlumbing (the percentage of housing units lacking complete plumbing facilities), housingUnitW/oKitchen (the percentage of housing units lacking complete kitchen facilities).</p><p>The adjusted R2 tells the percentage of variation explained by only the independent variables that actually affect the dependent variable. It can be used to compare regression results across various regression models with different predictors. The best model is the one with the largest adjusted R2.</p><p>There are many possible predictors to predict the number of restaurants which were collected from city-data.com :  </p><blockquote><p>elp_data_with_sd_final_data.csv</p></blockquote><h2 id="The-method-of-selecting-models"><a href="#The-method-of-selecting-models" class="headerlink" title="The method of selecting models"></a>The method of selecting models</h2><p>1)    Conduct a multiple linear regression on all the predictors and compute the adjusted R2.<br>2)    Each time we remove one predictor from the model of previous round, and then compute the respective adjusted R2. The new model with greatest adjusted R2 is selected as candidate and compared with the one in previous round.<br>3)    If the new adjusted R2 is greater than adjusted R2 obtained from previous round, we repeat the step 2 with the new model. Otherwise the previous model is considered as the optimistic one.</p><h2 id="The-process-of-selecting-models"><a href="#The-process-of-selecting-models" class="headerlink" title="The process of selecting models"></a>The process of selecting models</h2><blockquote><p>linear_regression.R</p></blockquote><p>1)    Clean the data.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;setwd(<span class="string">"C:/Users/linyanting/Desktop/IS5126/final_project/final data"</span>)</span><br><span class="line">&gt;yelp&lt;-read.csv(<span class="string">"yelp_data_with_sd_final_data.csv"</span>,header=<span class="literal">TRUE</span>,sep=<span class="string">","</span>)</span><br><span class="line">&gt;g &lt;- complete.cases(yelp)</span><br><span class="line">&gt;cleandatasu &lt;- yelp[g,]</span><br></pre></td></tr></table></figure><p>2)    Choose all the predictors and computed the adjusted R2 is 0.6204.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&gt;reg1 &lt;-lm(Number ~ houses+renters+cost+density+males+females+sexoffenders+medianAge+householdNum+inNonFamilyHouseholdNum+numMarriedCouplesWithChild+numSingleParentHouseholds+highschool+bachelor+professional+unemployed+married+separated+widowed+divorced+lesbian+gay+familyHousehold+unmarriedHousehold+povertyResident+housingUnitWoPlumbing+housingUnitWoKitchen,data =yelp )</span><br><span class="line">&gt;summary(reg1)</span><br><span class="line">Call:</span><br><span class="line">lm(formula = Number ~ houses + renters + cost + density + males + </span><br><span class="line">females + sexoffenders + medianAge + householdNum + inNonFamilyHouseholdNum + numMarriedCouplesWithChild + numSingleParentHouseholds + </span><br><span class="line">    highschool + bachelor + professional + unemployed + married + </span><br><span class="line">    separated + widowed + divorced + lesbian + gay + familyHousehold + </span><br><span class="line">    unmarriedHousehold + povertyResident + housingUnitWoPlumbing + </span><br><span class="line">    housingUnitWoKitchen, data = yelp)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max </span><br><span class="line">-<span class="number">341.28</span>  -<span class="number">87.72</span>   -<span class="number">9.60</span>   <span class="number">68.73</span>  <span class="number">432.06</span> </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">                             Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept)                -<span class="number">2.989e+02</span>  <span class="number">3.640e+02</span>  -<span class="number">0.821</span> <span class="number">0.412282</span>    </span><br><span class="line">houses                     -<span class="number">8.891e-04</span>  <span class="number">7.638e-03</span>  -<span class="number">0.116</span> <span class="number">0.907416</span>    </span><br><span class="line">renters                     <span class="number">7.869e-02</span>  <span class="number">1.192e-02</span>   <span class="number">6.600</span> <span class="number">2.17e-10</span> ***</span><br><span class="line">cost                        <span class="number">5.110e+00</span>  <span class="number">3.076e+00</span>   <span class="number">1.661</span> <span class="number">0.097855</span> .  </span><br><span class="line">density                    -<span class="number">4.681e-03</span>  <span class="number">4.825e-03</span>  -<span class="number">0.970</span> <span class="number">0.332872</span>    </span><br><span class="line">males                       <span class="number">9.023e-03</span>  <span class="number">6.722e-03</span>   <span class="number">1.342</span> <span class="number">0.180632</span>    </span><br><span class="line">females                    -<span class="number">5.887e-03</span>  <span class="number">9.748e-03</span>  -<span class="number">0.604</span> <span class="number">0.546446</span>    </span><br><span class="line">sexoffenders                <span class="number">7.811e-03</span>  <span class="number">2.189e-03</span>   <span class="number">3.568</span> <span class="number">0.000425</span> ***</span><br><span class="line">medianAge                   <span class="number">1.779e+00</span>  <span class="number">3.032e+00</span>   <span class="number">0.587</span> <span class="number">0.557795</span>    </span><br><span class="line">householdNum                <span class="number">1.999e-03</span>  <span class="number">6.399e-03</span>   <span class="number">0.312</span> <span class="number">0.755046</span>    </span><br><span class="line">inNonFamilyHouseholdNum    -<span class="number">1.217e-02</span>  <span class="number">1.043e-02</span>  -<span class="number">1.167</span> <span class="number">0.244356</span>    </span><br><span class="line">numMarriedCouplesWithChild -<span class="number">1.227e-03</span>  <span class="number">1.008e-02</span>  -<span class="number">0.122</span> <span class="number">0.903234</span>    </span><br><span class="line">numSingleParentHouseholds  -<span class="number">8.318e-02</span>  <span class="number">1.625e-02</span>  -<span class="number">5.118</span> <span class="number">5.86e-07</span> ***</span><br><span class="line">highschool                 -<span class="number">3.032e+02</span>  <span class="number">1.771e+02</span>  -<span class="number">1.712</span> <span class="number">0.088134</span> .  </span><br><span class="line">bachelor                    <span class="number">4.023e+02</span>  <span class="number">2.033e+02</span>   <span class="number">1.979</span> <span class="number">0.048780</span> *  </span><br><span class="line">professional               -<span class="number">5.760e+02</span>  <span class="number">3.855e+02</span>  -<span class="number">1.494</span> <span class="number">0.136258</span>    </span><br><span class="line">unemployed                  <span class="number">1.565e+02</span>  <span class="number">2.759e+02</span>   <span class="number">0.567</span> <span class="number">0.571020</span>    </span><br><span class="line">married                    -<span class="number">2.837e+02</span>  <span class="number">2.123e+02</span>  -<span class="number">1.336</span> <span class="number">0.182550</span>    </span><br><span class="line">separated                  -<span class="number">2.012e+03</span>  <span class="number">1.000e+03</span>  -<span class="number">2.011</span> <span class="number">0.045290</span> *  </span><br><span class="line">widowed                    -<span class="number">3.622e+02</span>  <span class="number">3.886e+02</span>  -<span class="number">0.932</span> <span class="number">0.352076</span>    </span><br><span class="line">divorced                    <span class="number">8.815e+02</span>  <span class="number">3.279e+02</span>   <span class="number">2.689</span> <span class="number">0.007621</span> ** </span><br><span class="line">lesbian                     <span class="number">1.956e+03</span>  <span class="number">3.017e+03</span>   <span class="number">0.648</span> <span class="number">0.517296</span>    </span><br><span class="line">gay                         <span class="number">5.108e+03</span>  <span class="number">2.418e+03</span>   <span class="number">2.113</span> <span class="number">0.035527</span> *  </span><br><span class="line">familyHousehold             <span class="number">1.295e+02</span>  <span class="number">1.840e+02</span>   <span class="number">0.704</span> <span class="number">0.482251</span>    </span><br><span class="line">unmarriedHousehold         -<span class="number">1.496e+02</span>  <span class="number">4.665e+02</span>  -<span class="number">0.321</span> <span class="number">0.748638</span>    </span><br><span class="line">povertyResident            -<span class="number">3.289e+01</span>  <span class="number">1.925e+02</span>  -<span class="number">0.171</span> <span class="number">0.864452</span>    </span><br><span class="line">housingUnitWoPlumbing      -<span class="number">5.851e+02</span>  <span class="number">4.377e+02</span>  -<span class="number">1.337</span> <span class="number">0.182423</span>    </span><br><span class="line">housingUnitWoKitchen        <span class="number">7.906e+01</span>  <span class="number">2.421e+02</span>   <span class="number">0.327</span> <span class="number">0.744279</span>    </span><br><span class="line">---</span><br><span class="line">Signif. codes:  <span class="number">0</span> ‘***’ <span class="number">0.001</span> ‘**’ <span class="number">0.01</span> ‘*’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br><span class="line"></span><br><span class="line">Residual standard error: <span class="number">135.8</span> on <span class="number">270</span> degrees of freedom</span><br><span class="line">Multiple R-squared:  <span class="number">0.6549</span>,Adjusted R-squared:  <span class="number">0.6204</span> </span><br><span class="line"><span class="literal">F</span>-statistic: <span class="number">18.97</span> on <span class="number">27</span> and <span class="number">270</span> DF,  p-value: &lt; <span class="number">2.2e-16</span></span><br></pre></td></tr></table></figure><p>3)    Remove one predictor and compute the respective adjusted R2. From the following table, we can see the largest adjusted R2 is 0.6217. So we choose the model which removes the predictors houses, numMarriedCouplesWithChild and povertyResident.</p><table><thead><tr><th><strong>predictor which was removed</strong></th><th><strong>the corresponding adjusted R2</strong></th></tr></thead><tbody><tr><td>houses</td><td>0.6217</td></tr><tr><td>renters</td><td>0.5607</td></tr><tr><td>cost</td><td>0.6179</td></tr><tr><td>density</td><td>0.6204</td></tr><tr><td>males</td><td>0.6192</td></tr><tr><td>females</td><td>0.6212</td></tr><tr><td>sexoffenders</td><td>0.6039</td></tr><tr><td>medianAge</td><td>0.6213</td></tr><tr><td>householdNum</td><td>0.6216</td></tr><tr><td>inNonFamilyHouseholdNum</td><td>0.6198</td></tr><tr><td>numMarriedCouplesWithChild</td><td>0.6217</td></tr><tr><td>numSingleParentHouseholds</td><td>0.5851</td></tr><tr><td>highschool</td><td>0.6177</td></tr><tr><td>bachelor</td><td>0.6163</td></tr><tr><td>professional</td><td>0.6186</td></tr><tr><td>unemployed</td><td>0.6213</td></tr><tr><td>married</td><td>0.6193</td></tr><tr><td>separated</td><td>0.6161</td></tr><tr><td>widowed</td><td>0.6205</td></tr><tr><td>divorced</td><td>0.6116</td></tr><tr><td>lesbian</td><td>0.6212</td></tr><tr><td>gay</td><td>0.6155</td></tr><tr><td>familyHousehold</td><td>0.6211</td></tr><tr><td>unmarriedHousehold</td><td>0.6216</td></tr><tr><td>povertyResident</td><td>0.6217</td></tr><tr><td>housingUnitWoPlumbing</td><td>0.6193</td></tr><tr><td>housingUnitWoKitchen</td><td>0.6216</td></tr></tbody></table><p>4)    Repeat the step 3). </p><p>5)    Finally, we can get the best model which is the one with the largest adjusted R2 (0.5173):</p><blockquote><p>Number= -366.9 - 0.00681<em>houses + 0.07062</em>renters - 5.022<em>cost + 0.008071</em> males + 0.008324<em>sexoffenders – 0.07256</em>numSingleParentHouseholds - 0.02553<em>highschool + 0.04206</em>bachelor - 0.07721<em>professional - 0.001453</em>separated + 0.09188<em>divorced + 0.005921</em>gay - 0.04219*housingUnitWoPlumbing</p></blockquote><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;reg_final&lt;-lm(formula = Number ~ houses+renters+cost+males+sexoffenders+numSingleParentHouseholds+highschool+bachelor+professional+separated+divorced+gay+housingUnitWoPlumbing, data = yelp)</span><br><span class="line">&gt;summary(reg_final) </span><br></pre></td></tr></table></figure><p>Call:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">lm(formula = Number ~ houses + renters + cost + males + sexoffenders + </span><br><span class="line">    numSingleParentHouseholds + highschool + bachelor + professional + </span><br><span class="line">    separated + divorced + gay + housingUnitWoPlumbing, data = yelp)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max </span><br><span class="line">-<span class="number">328.17</span>  -<span class="number">83.37</span>   -<span class="number">7.53</span>   <span class="number">69.00</span>  <span class="number">448.90</span> </span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">                            Estimate Std. Error t value Pr(&gt;|t|)    </span><br><span class="line">(Intercept)               -<span class="number">3.669e+02</span>  <span class="number">2.532e+02</span>  -<span class="number">1.449</span> <span class="number">0.148375</span>    </span><br><span class="line">houses                    -<span class="number">6.810e-03</span>  <span class="number">4.119e-03</span>  -<span class="number">1.653</span> <span class="number">0.099383</span> .  </span><br><span class="line">renters                    <span class="number">7.062e-02</span>  <span class="number">5.994e-03</span>  <span class="number">11.782</span>  &lt; <span class="number">2e-16</span> ***</span><br><span class="line">cost                       <span class="number">5.022e+00</span>  <span class="number">2.696e+00</span>   <span class="number">1.863</span> <span class="number">0.063514</span> .  </span><br><span class="line">males                      <span class="number">8.071e-03</span>  <span class="number">3.348e-03</span>   <span class="number">2.411</span> <span class="number">0.016565</span> *  </span><br><span class="line">sexoffenders               <span class="number">8.324e-03</span>  <span class="number">2.124e-03</span>   <span class="number">3.918</span> <span class="number">0.000112</span> ***</span><br><span class="line">numSingleParentHouseholds -<span class="number">7.256e-02</span>  <span class="number">1.175e-02</span>  -<span class="number">6.174</span> <span class="number">2.29e-09</span> ***</span><br><span class="line">highschool                -<span class="number">2.553e+02</span>  <span class="number">1.371e+02</span>  -<span class="number">1.862</span> <span class="number">0.063596</span> .  </span><br><span class="line">bachelor                   <span class="number">4.206e+02</span>  <span class="number">1.744e+02</span>   <span class="number">2.412</span> <span class="number">0.016506</span> *  </span><br><span class="line">professional              -<span class="number">7.721e+02</span>  <span class="number">3.244e+02</span>  -<span class="number">2.380</span> <span class="number">0.017963</span> *  </span><br><span class="line">separated                 -<span class="number">1.453e+03</span>  <span class="number">8.596e+02</span>  -<span class="number">1.691</span> <span class="number">0.092013</span> .  </span><br><span class="line">divorced                   <span class="number">9.188e+02</span>  <span class="number">2.650e+02</span>   <span class="number">3.467</span> <span class="number">0.000607</span> ***</span><br><span class="line">gay                        <span class="number">5.921e+03</span>  <span class="number">2.196e+03</span>   <span class="number">2.696</span> <span class="number">0.007437</span> ** </span><br><span class="line">housingUnitWoPlumbing     -<span class="number">4.219e+02</span>  <span class="number">3.660e+02</span>  -<span class="number">1.153</span> <span class="number">0.249969</span>    </span><br><span class="line">---</span><br><span class="line">Signif. codes:  <span class="number">0</span> ‘***’ <span class="number">0.001</span> ‘**’ <span class="number">0.01</span> ‘*’ <span class="number">0.05</span> ‘.’ <span class="number">0.1</span> ‘ ’ <span class="number">1</span></span><br><span class="line"></span><br><span class="line">Residual standard error: <span class="number">134.6</span> on <span class="number">284</span> degrees of freedom</span><br><span class="line">Multiple R-squared:  <span class="number">0.6435</span>,Adjusted R-squared:  <span class="number">0.6272</span> </span><br><span class="line"><span class="literal">F</span>-statistic: <span class="number">39.44</span> on <span class="number">13</span> and <span class="number">284</span> DF,  p-value: &lt; <span class="number">2.2e-16</span></span><br></pre></td></tr></table></figure><h2 id="Diagnostics"><a href="#Diagnostics" class="headerlink" title="Diagnostics"></a>Diagnostics</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; plot(reg_final)</span><br></pre></td></tr></table></figure><div align="center"><br><div style="width:60%;margin-top:-30px;"><br><img src="/blog/2016/03/24/biz-analytics/figure-2.png"><br></div></div><p>We can see reasonably equally spread residuals around a horizontal line without distinct patterns, which indicates that non-linear pattern does not exist.</p><div align="center"><br><div style="width:60%;margin-top:-30px;"><br><img src="/blog/2016/03/24/biz-analytics/figure-3.png"><br></div></div><p>The residuals are lined well on the straight dashed line, so it’s a good model.</p><div align="center"><br><div style="width:60%;margin-top:-30px;"><br><img src="/blog/2016/03/24/biz-analytics/figure-4.png"><br></div></div><p>In this diagram, the residuals spread randomly along the horizontal line, which indicates that the assumption of equal variance is reasonable.</p><div align="center"><br><div style="width:60%;margin-top:-30px;"><br><img src="/blog/2016/03/24/biz-analytics/figure-5.png"><br></div></div><p>In this diagram, there is no spot outside the dashed line. There are no influential cases which may alter the results.</p><h2 id="Significant-variables"><a href="#Significant-variables" class="headerlink" title="Significant variables"></a>Significant variables</h2><p>The variables highlighted in red color(renters, males, sexoffenders, numSingleParentHouseholds,  bachelor, professional, divorced, gay) have significant coefficient relationship with restaurant number. Because their <em>corresponding |t value|&gt;= 1.96(5% significance level)</em> and <em>p-value &lt; 0.05</em>.</p><h1 id="Interpretation"><a href="#Interpretation" class="headerlink" title="Interpretation"></a>Interpretation</h1><h2 id="Factor-1-Renters"><a href="#Factor-1-Renters" class="headerlink" title="Factor 1 - Renters"></a>Factor 1 - Renters</h2><p>In USA, 51% renters are under 30, mostly renting a room in big cities, living with high housing cost. One of the most direct interpretations for this factor is that the renters may not have time to cook, or they have no sufficient cooking facilities in their rented house. They are the group of people who provide the demand for restaurants.<br>On the other hand, cities with a large number of renters are generally considered as big cities, which bring with a high living standard and large population. Hence the number of restaurants is supposed to be large. </p><h2 id="Factor-2-Males-Number"><a href="#Factor-2-Males-Number" class="headerlink" title="Factor 2 - Males Number"></a>Factor 2 - Males Number</h2><p>One of the interpretations for this factor is that more males implies larger population and hence higher consuming demand for food and dining business. </p><h2 id="Factor-3-Number-of-Single-Parents-Households"><a href="#Factor-3-Number-of-Single-Parents-Households" class="headerlink" title="Factor 3 - Number of Single Parents Households"></a>Factor 3 - Number of Single Parents Households</h2><p>Generally speaking, comparing with other types of families, single-parent families are more likely to have limited financial resources to cover their life expenses, especially the single-mother households. According to a research in US, 7 in 10 children living with mother are living poorly; this percentage is much higher than other types of families. Hence single-parent families are more likely to choose home cooking, rather than going to restaurant, to save the expenses. </p><h2 id="Factor-4-amp-5-Percentage-of-Bachelor-and-Graduate-Professional-Degree-Holder"><a href="#Factor-4-amp-5-Percentage-of-Bachelor-and-Graduate-Professional-Degree-Holder" class="headerlink" title="Factor 4 &amp; 5 - Percentage of Bachelor and Graduate/Professional Degree Holder"></a>Factor 4 &amp; 5 - Percentage of Bachelor and Graduate/Professional Degree Holder</h2><p>Bachelor degree describes the education level of a man and could explicitly reflect the work nature and consumption capacity. Well-educated people usually spend more time onto work or social activity and desire higher quality of life, which make them the potential customers of restaurants. However, people with even higher degree of Graduate / Professional tend to follow their daily routine and may not be willing to invest the time and energy to try new restaurants.</p><h2 id="Factor-6-amp-7-Divorced-and-Gay-Percentage"><a href="#Factor-6-amp-7-Divorced-and-Gay-Percentage" class="headerlink" title="Factor 6 &amp; 7 - Divorced and Gay Percentage"></a>Factor 6 &amp; 7 - Divorced and Gay Percentage</h2><p>Higher divorce percentage may indicate a higher percentage of people who like to make changes. They do not adhere to the old habits and likely not to stay at home to do home cooking. Instead they might dine out more and try new restaurants.<br>Higher gay percentage may indicate the higher percentage of People who are more open to break the tradition and accept new ideas. It also indicates the acceptance level of the public to new things in the region.<br>It might be a great idea to open a new restaurant in places whereby people are more likely to accept and make change and try something new.  </p><h2 id="Factor-8-Population-per-Sex-Offender"><a href="#Factor-8-Population-per-Sex-Offender" class="headerlink" title="Factor 8 Population per Sex Offender"></a>Factor 8 Population per Sex Offender</h2><p>Higher population per sex offender means lower number of sex offenders, which indicates better security and social morality.<br>Customers prefer to live and consume at a safe and secure place. Meanwhile, investors also prefer to do business at a safe area to keep their money secure. That could explain why the number of restaurants has negative correlation coefficient with the cases number of sex offender.</p><h1 id="Limitation-amp-Conclusion"><a href="#Limitation-amp-Conclusion" class="headerlink" title="Limitation &amp; Conclusion"></a>Limitation &amp; Conclusion</h1><ol><li>Inadequate amount of data<br>The restaurant data from Yelp is based on 3 states only, which has some impact on the result because it is not a full reflection of the whole USA restaurant business. </li><li>Lack of time-series data<br>The data is only a cross-sectional data (both Yelp and city-data), but not time-series. This brings difficulty in doing causal analysis and other further analysis.<br>Moreover, the restaurant data and the data from city-data.com may not be perfectly matching as there could be some differences in the date when the data was recorded. </li><li>Limitation of linear regression model<br>The adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model.</li><li>Other unquantifiable factors<br>The data used does not include several other unquantifiable factors, such as style of food, scale of a restaurant, etc. These factors would bring a big difference to the result analysis. For instance, a multi-vendor canteen may classified as only one restaurant, but it provides a large variety of food selection and options. It could be sufficient enough to have only one such multi-vendor canteen in a region.  </li><li>Inter-relationship with other restaurants<br>In this project, inter-relationship with other restaurants is too complicated and difficult to be considered in doing the statistical analysis. For example, restaurant with competitive relationship or complementary relationship. </li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>If you are going to open a new restaurant in USA. This project will provide some suggestions in selecting the location of your business. You are recommended to consider these eight factors provided in this project when choosing the location, instead of simply referring to local GPD per Capita or population. </p><h1 id="Demo-amp-Visualisation"><a href="#Demo-amp-Visualisation" class="headerlink" title="Demo &amp; Visualisation"></a>Demo &amp; Visualisation</h1><h2 id="Data-of-Las-Vegas-NV"><a href="#Data-of-Las-Vegas-NV" class="headerlink" title="Data of Las Vegas, NV"></a>Data of Las Vegas, NV</h2><div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/blog/2016/03/24/biz-analytics/visual-1.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-2.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-3.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-4.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-5.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-6.png"><br></div></div><h2 id="Data-of-Phoenix-AZ"><a href="#Data-of-Phoenix-AZ" class="headerlink" title="Data of Phoenix, AZ"></a>Data of Phoenix, AZ</h2><div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/blog/2016/03/24/biz-analytics/visual-7.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-8.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-9.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-10.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-11.png"><br><img src="/blog/2016/03/24/biz-analytics/visual-12.png"><br></div></div><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Kusisto, L., &amp; Hudson, K. (2015). Renters Are Majority in Big U.S. Cities.   Retrieved from <a href="http://www.wsj.com/articles/renters-are-majority-in-big-u-s-cities-1423432009" target="_blank" rel="noopener">http://www.wsj.com/articles/renters-are-majority-in-big-u-s-cities-1423432009</a><br>[2] NPD. (2013). U.S. Total Restaurant Count Increases by 4,442 Units over Last Year.   Retrieved from <a href="https://www.npd.com/wps/portal/npd/us/news/press-releases/us-total-restaurant-count-increases-by-4442-units-over-last-year-reports-npd/" target="_blank" rel="noopener">https://www.npd.com/wps/portal/npd/us/news/press-releases/us-total-restaurant-count-increases-by-4442-units-over-last-year-reports-npd/</a><br>[3] Steen, K. V. (2010). Using R for Linear Regression. Retrieved from<br>Wikipedia. (2016). Gender pay gap in the United States.   Retrieved from<br><a href="https://en.wikipedia.org/wiki/Gender_pay_gap_in_the_United_States#cite_note-jec_p80-3" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gender_pay_gap_in_the_United_States#cite_note-jec_p80-3</a></p><h1 id="Appendix-Poster"><a href="#Appendix-Poster" class="headerlink" title="Appendix: Poster"></a>Appendix: Poster</h1><div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/blog/2016/03/24/biz-analytics/poster.jpg"><br></div></div>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot; style=&quot;padding-bottom:15px;&quot;&gt;&lt;img src=&quot;/blog/2016/03/24/biz-analytics/cvr-analytics.jpg&quot;&gt;&lt;/div&gt;

&lt;p&gt;This project is carried out in class of IS5126 Hands-on with Business Analytics at National University of Singapore, School of Computing. This project uses R and database techniques to conduct a simple analytics.&lt;br&gt;
    
    </summary>
    
    
      <category term="academic, project" scheme="http://mmhhss1991.github.io/tags/academic-project/"/>
    
  </entry>
  
  <entry>
    <title>Chinese Segmentation in User-Generated Content</title>
    <link href="http://mmhhss1991.github.io/2014/05/20/chinese-segementation/"/>
    <id>http://mmhhss1991.github.io/2014/05/20/chinese-segementation/</id>
    <published>2014-05-20T13:13:14.000Z</published>
    <updated>2018-09-01T10:18:42.724Z</updated>
    
    <content type="html"><![CDATA[<div align="center" style="padding-bottom:15px;"><img src="/blog/2014/05/20/chinese-segementation/cvr-chinese.png"></div><p>This project is my final year project to fulfil the requirement of the degree of bachelor of engineering (hon.) at Nanyang Technological University (Singapore), School of Computer Engineering. This project talks about the Natural Language Processing in Chinese Language.<br><a id="more"></a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>With the rapid development of social media, website like weibo has become a very important channel of getting information in Chinese speaking regions. In past few years, more and more people chose to use weibo to record what happened around them and express their experience and emotion in daily life due to its highly socialized and real-time features. Human society was greatly speeded up by Internet. And many new vocabularies or new informal writing were invented and widely used in the Internet.<br>Thanks to the natural language processing technical, the new vocabulary discovery could be done by computer programs rather than manually search. Some statistical concepts, such as Hidden Markov Model, provided a guideline to design the algorithm that could segment one sentence into a sequence of words with the maximum probability and best rationality. However, the realization of sentence segmentation would be varied for different languages. For instance, the words are naturally separately by spaces in Western languages but for most of East Asian languages such as Chinese, Japanese, Korean or Thai, on the contrary, the words are presented into characters and connected with each other without any separator. And that makes new words discovery in East Asian languages more difficult, challenging and flexible.<br>This project aimed to find an efficient way to discover new Chinese vocabularies invented from the Internet and used them into Chinese article segmentation, and consequently provided suggestion and methods to improve the current Chinese natural language processing technology. In this project, several concepts and techniques related to natural language processing and information extraction were adopted and several times of experiments were carried out to obtain the most desirable result.<br>The experimental results showed that the algorithms implemented for this project could find most of new vocabularies successfully and make some improvement onto the existing Chinese segmentation tools. The detailed result analysis showing how much improvement could be made and further discussion about the limitation and future development work were conducted and reflected in this report.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background "></a>Background </h2><h3 id="Internet-Slang"><a href="#Internet-Slang" class="headerlink" title="Internet Slang"></a>Internet Slang</h3><p>21st Century is the century of Internet. With the explosion in population of Internet users, a new kind of language, the Internet Slang, was introduced into people’s lives and became extremely popular during last few years.</p><p>As the language with most people used around the world, Chinese language also been exposed to the language revolution brought by Internet. More and more new vocabularies were invented on the Internet and adopted by people into real daily lives. Within these new vocabularies, some of them were simpler or shorter writing of the formal words (e.g. 女主 - heroine); some of them were invented to express new things or new phenomena in the society (e.g. 微博 - weibo, a Chinese social network like twitter), and what the most interesting is, some of the new words were informal or mistake writing to the existing words intentionally or unintentionally, but more popular than the original ones (e.g. 程序猿 – a homophony of programmer in Chinese).</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/1-new-words.png" title="New words"><br></div></div><h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h3><p>The natural language processing (shorted as NLP) became a hot topic in computer science field in last few years. With statistical knowledge and many cutting-edge information technologies like text extraction, machine learning and artificial intelligence involved in, the NLP enables machine to understand human’s natural languages and response with intelligence. These natural languages could not only be English but also Arabic, Germany or even Chinese.</p><h3 id="When-Natural-Language-Processing-Meets-Internet-Slang"><a href="#When-Natural-Language-Processing-Meets-Internet-Slang" class="headerlink" title="When Natural Language Processing Meets Internet Slang"></a>When Natural Language Processing Meets Internet Slang</h3><p>The mechanism that enables machine to understand human’s natural languages is completely realized by being given a dictionary with all vocabularies listed in it. And it works well in formal language such as documentations, reports or journals. But in past few years, the Internet slang started being used by people frequently and widely, the traditional natural language processing could not satisfy people’s need just by simply reading language and checking in the dictionary. Therefore, the latest natural language processing technics were expected to discovery new words in natural languages and add them into the dictionary automatically. In fact, the efficiency of the new words discovery nowadays has already became one of the most significant factors for determining the quality of a natural language processing tool.</p><h2 id="Project-Objectives"><a href="#Project-Objectives" class="headerlink" title="Project Objectives"></a>Project Objectives</h2><p>The natural language processing in English and other Western languages were well developed in last several decades. Today there are more than one mature products or toolkits for the NLP in English, for example, the well-known personal voice assistant application called “Siri”, or a famous free NLP development toolkit named “Natural Language Toolkit (NLTK)”. However, unlike those language formed by Latin letters, some East Asian languages, such as Chinese, it is presented as consecutive characters rather than writing in letters and segmented by white spaces. In another word, before understanding and analysis the detailed meaning of Chinese language, the machine must be able to segment an entire coherent sentence constructed by a series of Chinese characters in right ways.</p><p>There are two main objectives of this project:</p><p>Firstly, this project was to learn the existing Chinese sentence segmentation tools and research on the working mechanism behind.</p><p>Then, the student was expected to come up with a solution than could make improvement onto one of them for usage onto the texts used in social networks like weibo, and then implement it. The final result will be measured by the precision/recall rate in both the new words discovery and the segmentation result on the given testing text, thereby provide a feasible and effective solution.</p><h2 id="Timeline"><a href="#Timeline" class="headerlink" title="Timeline"></a>Timeline</h2><p>The schedule of this project from the beginning of this project until the completion was listed below:</p><p><span id="_Toc383415736" class="anchor"></span>Table Project Timeline</p><table><thead><tr><th>Time</th><th>Task</th></tr></thead><tbody><tr><td>2013.4</td><td>Project topic settled down and first meeting with the supervisor.</td></tr><tr><td>2013.5-2013.9</td><td>Read relevant paper and get familiar with existing Chinese segmentation tools, try to understand the mechanism behind, and choose the one to improve.</td></tr><tr><td>2013.9-2013.10</td><td>Decide the tool to use, and set up the development environment; Prepare some data to test the tool and position the area/function to make improvement.</td></tr><tr><td>2013.11</td><td>Temporarily pause the project for the final examination of semester 1.</td></tr><tr><td>2013.12</td><td>Construct the general improvement idea and try to implement step by step.</td></tr><tr><td>2014.1</td><td>Meet with supervisor to report the progress, ask for consultation onto the current idea, and confirm the direction ahead for the project.</td></tr><tr><td></td><td>Complete the interim report.</td></tr><tr><td></td><td>Continue the implementation.</td></tr><tr><td>2014.2</td><td>Confirm the dataset selection, and Extract the weibo post texts from the dataset.</td></tr><tr><td></td><td>Re-organize and Clean the project code</td></tr><tr><td>2014.3</td><td>Finish the coding, analyze the result and write the final report.</td></tr><tr><td>2014.3-2014.4</td><td>Complete and submit the amended report, and check any modification needed to be made on the project for the demo and oral defense.</td></tr></tbody></table><h2 id="Report-Organisation"><a href="#Report-Organisation" class="headerlink" title="Report Organisation"></a>Report Organisation</h2><p>There are totally 7 chapters in this report, includes introduction, literature review, design of this project, measurement for the project evaluation, results and analysis, limitation and future suggestion, and conclusion.</p><p><em>Chapter 2</em> provides information about the background, bring out the problem to be solved and then clarify the objectives. Besides, it also includes the schedule of this project shows a rough timeline contains all the procedures in this project during two semesters.</p><p><em>Chapter 3</em> is the literature review, which provides a brief introduction and analysis to some existing theories and knowledge related to this project. At the same time, this chapter also reflects what the student has learnt from doing this project.</p><p><em>Chapter 4</em> provides more detailed discussions on the design and development process. Four different methods are introduced to improve the Chinese segmentation in user generate content. In addition, this chapter also introduces how these algorithms are implemented under a specific assumption.</p><p><em>Chapter 5 and 6</em> give the methodology to evaluate the project, shows the result data of the evaluation, and provides an analysis and evaluation of the algorithms implemented to meet the requirements of this project.</p><p><em>Chapter 7 and 8</em> give limitation and future recommendation of this project, and then an overall conclusion of the entire project is provided. </p><h1 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h1><h2 id="Natural-Language-Processing-and-Chinese-Language"><a href="#Natural-Language-Processing-and-Chinese-Language" class="headerlink" title="Natural Language Processing and Chinese Language "></a>Natural Language Processing and Chinese Language </h2><p>Natural language is defined as language that is used for human’s daily communication like English, Portuguese or Chinese, rather than the artificial languages designed for machine’s understanding such as mathematical notations or programming languages. However, the computer couldn’t parse and understand natural language directly. There must be a technical routine called Natural Language Processing, or NLP for short, guiding and operating machines to manipulate the natural language. The NLP is the set of all methodologies and operations related to natural language, it covers the tasks that simply counting the frequency of words in comparison of different writing styles, or extremely understand human’s voice command and response correctly [1].</p><p>The basic steps of the NLP in English or other kinds of language in Latin letters’ form contain text tokenization, categorizing and tagging words, syntactic analysis and further analysis and application. For instance, the tokenization in English is simply splitting sentences with white spaces and applying recognition filter for some special cases like Word-internal punctuation (e.g. some abbreviations such as “etc.”, “Vol.”, “Prof.”), clitics (e.g. “I’m”, “doesn’t”) and multi-token words (e.g. “New York”, “Ang Mo Kio”, “Johnson &amp; Johnson”) [2]. But the NLP in many East Asian languages such as Chinese has a little bit difference since the Chinese is written in consecutive Chinese characters and there is no natural boundaries separating each word, the text tokenization process could be much more complex when compared to the NLP in English [3]. That means the Chinese text must be processed by segmentation program for tokenization before further processing such as syntactic analysis.</p><h2 id="Language-Model-and-N-Gram"><a href="#Language-Model-and-N-Gram" class="headerlink" title="Language Model and N-Gram "></a>Language Model and N-Gram </h2><p>Suppose an incomplete sentence with one unknown word in specific position, the intuitive way to identify that missing word from plenty of candidates is choosing the one could achieve in highest likelihood probability as a sentence in its natural language. The Chain Rule enable us to compute probabilities of entire sequences like P(w1 , w2 , …, wn). The Chain Rule suggests that we could estimate the joint probability of an entire sequence of words by multiplying a number of conditional probabilities together. However, the human natural language is creative and we cannot ensure any words or sentences have occurred in the corpus. Therefore it seems impossible to estimate the joint probability by counting the number of occurring times in corpus of every word following every long string [4].</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-1.png"><br></div></div><p>The N-gram model is designed to solve the predicament that the joint probability seems not able to be calculated efficiently. What N-gram represents is a slice with N-character in length among a longer string. The most widely used N-gram models are uni-gram, bi-gram and tri-gram. According to the Markov Assumption, the probability of a word occurring after a string only related to the joint probability of N words sequence previous to its position (a prefix of N). It could be expressed in the mathematical notations as that:</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-2.png"><br></div></div><p>And thereby the joint probability of a whole sentence or paragraph could be calculated in much easier way as well. The formal is supposed like (Suppose N=2) [5]:</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-3.png"><br></div></div><p>Since the usage of N-gram is a part of Markov Assumption, the joint probability of a sentence or a sequence of text is just an approximated value. Theoretically speaking, the higher N is would lead to higher precision of the over probability. However, some N-grams would return zero or almost zero in probability that makes the true candidate difficult to be recognized among others. Besides that, the higher N is means higher calculation workload exerted to the machine and less performance efficiency in the prediction, and the Chain Rule formal could be regarded as an extreme situation of N-gram estimation where N is equal to the length of the text sequence [6].</p><p>Moreover, in order to prevent the existing of zero probability that might invalidate calculation of the joint probability, a statistics technique named additive smoothing (also known as Laplace smoothing) was widely adopted by different NLP tools. That smoothing technique measures all probabilities by adding a smoothing parameter α to force all estimation on resulting probability fall in the range of its original empirical estimate (xi/N) and uniform probability (1/d) [7].</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-4.png"><br></div></div><h2 id="Unsupervised-Chinese-Word-Segmentation"><a href="#Unsupervised-Chinese-Word-Segmentation" class="headerlink" title="Unsupervised Chinese Word Segmentation"></a>Unsupervised Chinese Word Segmentation</h2><p>The unsupervised Chinese word segmentation became a popular research topic and well developed in the past two decades. Nowadays there are many Chinese segmentation technologies and tools available and some of them could achieve as high as 99% precision rate [8]. But there are still some limitations in this field that make Chinese word segmentation imperfect, such as there is no clear definition of “word” itself in Chinese language – same pair or group of Chinese characters was recognized as a word by some people but may not be regarded as a word by some other people (e.g. “一只/det” or “一/num只/n” in “一只老虎” ); Besides that, the ambiguity (e.g. “才/ad 能/v” and “才能/n” when segmenting the sentence“只有这样才能成功”) and new words which are not recorded in dictionary also restrict the performance of Chinese word segmentation [8].</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/2-chinese-seg.png" title="An Example of Chinese Segmentation"><br></div></div><p>The existing unsupervised Chinese word segmentation could be classified into three classes in general.</p><p>(i) The first kind of segmentation is based on Maximum Matching method (short as MM method), the final output is the alternative with maximized tokenization result, in another word, is the alternative has maximum number of words be segmented. This method could be further divided into Forward Maximum Matching (FMM), Backward Maximum Matching (BMM) and combinatory of FMM and BMM (choose the optimal result from both FMM and BMM). The advantage of this method is simple, fast and fair quality of the output result [9].</p><p>However, this segmentation is not good enough for some cases that some sequences of Chinese characters wrongly recognized as words with high probability. E.g. “结婚的和尚未结婚的” will always be segmented as “结婚/的/和尚/未/结婚/的” rather than the correct one “结婚/的/和/尚未/结婚/的” because “和尚/n – monk” has more occurring frequency and higher probability than the word “尚未/ad – not yet” in some dictionaries, in another sentence, P(结婚)×P(的)×P(和)×P(尚未)×P(结婚)×P(的) &lt; P(结婚)×P(的)×P(和尚)×P(未)×P(结婚)×P(的).</p><p>(ii) There is another method named Statistical Language Models (SLM). The SLM segments Chinese text by considering the probability of specific combination of Chinese characters instead of just referring to the dictionary, and this model was also called N-gram, which is introduced in previous section. The N-gram is an important statistical language model and the most widely used is bi-gram. Most of well-known Chinese sentence segmentation tools are developed based on this bi-gram model, such as the free distribution of NLPIR 2013. In these models, the frequency of each word is determined by words ahead. The SLM method needs large enough corpus support and requires higher performance of the machine. The benefit of this method is high precision especially for higher level of N-gram model [9].</p><p>(iii) The last one method is also the latest one, is called Automatic Chinese Word Segmentation based on Artificial Intelligence. The keywords of this approach are Neural Networks algorithm and Expert System algorithm. By adoption of artificial intelligence theory, the segmentation program is able to learn and discover new words or potential combination of Chinese characters automatically after accepting enough training. This method could achieve highest precision ever but complex in logic and algorithm design. But under the rapid development of high performance computing and artificial intelligence, it will be the mainstream of Chinese segmentation research in the future [9].</p><h2 id="Active-Learning-for-NLP"><a href="#Active-Learning-for-NLP" class="headerlink" title="Active Learning for NLP "></a>Active Learning for NLP </h2><p>Machine Learning is a collection of artificial intelligence methods requires supervised data to learn a concept, and then provides the execution result with higher efficiency and accuracy. However, labelling data by hand-annotation is expensive, time consuming, tedious, mistake prone and hard to adapt for new purposes. Therefore, people has looked at a more compromise alternative that using semi-supervised and unsupervised learning techniques for the purpose of obtaining experimental result with high quality and with acceptable cost as well.</p><p>An active learning problem contains a classifier, a small set of labelled samples and a large set of unlabelled data. Firstly, the classifier is trained on the labelled samples under the supervision to learn the concept and form a rule to guide itself for further classifying. The active learning methods are widely used in two types of problems in natural language processing field, which are classification task like text classification, and structured prediction task such as parsing and name entity recognition. Based on Arora and Agarwal’s research, the application of active learning technique in protein names recognition could achieve the same F-score as unsupervised method with 39% less data samples [10].</p><p>In this project, the active learning technique was supposed to play a significant role in new word discovery with higher accuracy and efficiency.</p><h2 id="The-NLPIR"><a href="#The-NLPIR" class="headerlink" title="The NLPIR"></a>The NLPIR</h2><div align="center"><br><div style="width:35%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/3-hhmm-chinese-lex.png"><br></div></div><p>The NLPIR is short writing of the Natural Language Processing &amp; Information Retrieval Sharing Platform developed by Big Data Search and Mining Lab at Beijing Institute of Technology. Its Chinese word segmentation function was formerly known as ICTCLAS (Institute of Computing Technology, Chinese Lexical Analysis System), a powerful segmentation tools based on Hierarchical Hidden Markov model (HHMM) and it used to win the first place in SIGHAN competition in 2003. The latest stable version of NLPIR till the beginning of this project was NLPIR 2013 and the API package is freely distributed to the public.</p><p>The Hidden Markov Models (HMM) is a statistical Markov model belongs to the second class of segmentation method and it is widely used in for modelling stochastic processes and sequences in applications like text analysis or speech recognition [11]. To simplify, the HMM was probability notation in considering both transition probabilities and observation likelihoods. As one of HHMM-based Chinese lexical analysis tools, the NLPIR Chinese word segmentation work is done in five levels: atom segmentation, simple &amp; recursive unknown words recognition, class-based segmentation and POS tagging [12]. The first level HMM will be introduced with more details of the algorithm and mechanism of the NLPIR segmentation.</p><p>The formula to calculate the word segmentation is listed below:</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-5.png"><br></div></div><p>Some examples and figures in later sub-sections are citied from an academic blog:\ <em><a href="http://blog.csdn.net/sinboy" target="_blank" rel="noopener">http://blog.csdn.net/sinboy</a></em></p><h3 id="Atom-Segmentation"><a href="#Atom-Segmentation" class="headerlink" title="Atom Segmentation"></a>Atom Segmentation</h3><p>The atom segmentation is the first step during the Chinese sentence segmentation. A sentence will be tokenized at the beginning, and then be segmented into the minimum un-dividable unit as single Chinese character. This unit to present the minimum unit is called atom. Each atom has three attributes: the atom word; the POS tag annotation (“nPOS”, 1 as beginning mark, 4 as ending mark, and all other atom are labelled 0 since there are just an atom unit rather than a word) and the atom length. After atom segmentation, a sentence is supposed to be made up with a sequence of atoms that being filled with single Chinese character or a sequence of the longest consecutive non-Chinese characters like punctuation, digit or English words [13].</p><p>The figure below shows an example how a simple Chinese sentence being segmented and stored during atom segmentation.</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/4-atom-seg.png" title="An Example Showing Atom Segmentation"><br></div></div><p>According to the official description of the NLPIR, it is the one of the Chinese word segmentation tools with the highest performance and best segmentation result even it is still not perfect. Moreover, the NLPIR provides an API to the public freely for non-commercial purpose. That is why the project will use NLPIR 2013 as the research object and do further improvement if possible.</p><h3 id="Rough-Segmentation"><a href="#Rough-Segmentation" class="headerlink" title="Rough Segmentation"></a>Rough Segmentation</h3><p>After obtaining a sequence of atom units, the first round rough segmentation will be conducted next. During this round of segmentation, all the possible segmentation alternatives for the given sentence will be listed out and there are two iterations to realize the algorithm. In the first iteration, each atom is being traversed and searching all the possible words formed by it and other atoms connected in the dictionary. This search could be done in the second iteration by traverse all the atoms after the base atom [14].</p><p> Suppose a sentence A0A1…AnAn+1…Am is given, the pseudo code algorithm to do the rough segmentation could be like:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;m; i++)&#123;</span><br><span class="line">String s=A[i];</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>; j&lt;m; j++)&#123;</span><br><span class="line">s=s+A[j];</span><br><span class="line"><span class="keyword">if</span>(s is a word)&#123;</span><br><span class="line">add s into the list of the words;</span><br><span class="line">record the POS deteail;</span><br><span class="line">record the position and length of s;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>And a sentence “他说的确实在理” after this rough segmentation would become a words’ array chain with POS information, position and length saved into it.</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/5-example-rough-seg.png" title="An Example showing the Result of Rough Segmentation"><br></div></div><h3 id="N-Shortest-Path"><a href="#N-Shortest-Path" class="headerlink" title="N-Shortest Path"></a>N-Shortest Path</h3><p>After the rough segmentation, an array chain of all the possible words within the given sentence is acquired and next job is to form up one or more optimal paths linked all the candidate words from the beginning of the sentence to the end without any duplicate of single atom. The criteria of possibility of a specific words’ sequence, in another word the weight value, is determined by the weight of each atom and cohesion between each other. Here the idea of N-gram in HMM language model mentioned before is applied to calculate the weight value in this round of segmentation, the NLPIR call this algorithm N-Shortest Path [15]. The optimal path is the path with the minimum over weight by multiple all weights of candidate transitions together. The equation below is the simulation of the N-Shortest Path where N=2:</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-6.png"><br></div></div><p>After calculating all the weight of candidate word pairs, the sentence in current stage could be expressed as a table drawn in the next page.</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/table-2.png" title="An example of N-Shortest Path in 2-D matrix"><br></div></div><div align="center"><br><div style="width:50%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/6-n-short.png"><br></div></div><p>With the visual representation of the array chain of all the candidate words’ transition list above, the weight of each candidate path could be easily calculated and in NLPIR, the optimal path is obtained by executing Dijkstra algorithm for the case N=2. Back to the sample sentence, the shortest path could be obtained by applying the method stated before. The figure at right hand side shows the procedure of how the path acquired. The final path is “始##始@他@说@的@确实@在@理@末##末” with the minimum overall weight 68798.864.</p><p>After the first round of rough segmentation, the following several rounds of segmentation are focused on more specific cases such as geographic name entities, name entities and further optimization for final result. These following steps are realized by applying some additionally NLP related techniques. However, we won’t be discussed the details of other steps here because essentially they share the same idea in the segmentation. </p><h1 id="Project"><a href="#Project" class="headerlink" title="Project "></a>Project </h1><p>This Project Chapter introduces the details of this project which is implemented on the basis of NLPIR 2013 and could be separated into three main parts, the resources or materials to be used in this project; the logic and algorithm to realize the goal and the related documentation for this project.</p><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><h3 id="Hardware-Resources"><a href="#Hardware-Resources" class="headerlink" title="Hardware Resources"></a>Hardware Resources</h3><p>In consideration of this project needs lot of file I/O and text searching works, this project was implemented and tested on two computers with difference hardware configurations simultaneously to ensure the compatibility and performance of the project. </p><p><em>Main Implementation &amp; Testing Environment:</em></p><table><thead><tr><th><strong>Configuration</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td>Machine Model</td><td>Apple® MacBook Pro 15 (late 2011)</td></tr><tr><td>Operating System</td><td>Mac OS X Mavericks *(10.9.1 Build 13B42)</td></tr><tr><td>Processor</td><td>Intel® Core™ i7-2720QM Processor, 2.2GHz (up to 3.30 GHz)</td></tr><tr><td>Storage</td><td>250GB SSD</td></tr><tr><td>Memory</td><td>8GB DDR3</td></tr><tr><td>Software</td><td>eclipse (Standard 4.3.1 for Mac OS X)</td></tr><tr><td>Java Version</td><td>Java™ 7 pre-installed in Mac OS X</td></tr></tbody></table><p><em>Alternative Testing Environment:</em></p><table><thead><tr><th><strong>Configuration</strong></th><th><strong>Details</strong></th></tr></thead><tbody><tr><td>Machine Model</td><td>Lenovo® ThinkPad X61t (late 2007)</td></tr><tr><td>Operating System</td><td>Microsoft® Windows 7 Ultimate (Version 6.1, Build 7600)</td></tr><tr><td>Processor</td><td>Intel® Core™ 2 Duo L7500 Processor, 1.60GHz</td></tr><tr><td>Memory</td><td>2GB DDR2</td></tr><tr><td>Storage</td><td>500GB HDD, 7200 rpm</td></tr><tr><td>Software</td><td>eclipse (Kepler Release, Build id 20130614-0229)</td></tr><tr><td>Java Version</td><td>Java™ Standard Edition (Version 7 Update 45 build 1.7.0_45-b18)</td></tr></tbody></table><h3 id="Software-and-Project"><a href="#Software-and-Project" class="headerlink" title="Software and Project"></a>Software and Project</h3><p>The Software used in this project were two integrated development environment (IDE) software. The first one is the eclipse and JDK. The whole project was a Java project and it was fully set up on this IDE. The other software used in this project is JetBrains PyCharm, which is an IDE of Python, to run the Python codes written by student in purpose of extracting content of weibo post from JSON files.</p><p>This project was further implemented onto the latest NLPIR API downloaded from its official website: <a href="http://ictclas.nlpir.org" target="_blank" rel="noopener">http://ictclas.nlpir.org</a>. The API provided two library files “NLPIR.dll” and “NLPIR_JNI.dll”; some data files like dictionaries and a Java class listing all callable methods for NLPIR 2013.</p><p>The NLPIR project read string in UTF-8 format as input, and output the segmentation result in UTF-8 string, too. The output has two types, segmentation without POS tags and segmentation with POS tags. The NLPIR enables user to choose if there are POS tags in output string by changing a parameter’s value in a method calling like that: </p><p>Demo Code:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NLPIR testNLPIR = <span class="keyword">new</span> NLPIR(); </span><br><span class="line">String input = “第一次和家人一起装饰圣诞树很开心。”;</span><br><span class="line"><span class="keyword">byte</span> nativeBytesWithoutTag[] = testNLPIR.NLPIR_ParagraphProcess(</span><br><span class="line">input.getBytes(<span class="string">"UTF-8"</span>), <span class="number">0</span>);</span><br><span class="line">String segmentOutput = <span class="keyword">new</span> String(nativeBytesWithoutTag, <span class="number">0</span>, </span><br><span class="line">nativeBytesWithoutTag.length, <span class="string">"UTF-8"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">byte</span> nativeBytesWithTag[] = testNLPIR.NLPIR_ParagraphProcess(</span><br><span class="line">input.getBytes(<span class="string">"UTF-8"</span>), <span class="number">1</span>);</span><br><span class="line">String segmentOutputWithTags = <span class="keyword">new</span> String(nativeBytesWithTag, <span class="number">0</span>, </span><br><span class="line">nativeBytesWithTag.length, <span class="string">"UTF-8"</span>);</span><br></pre></td></tr></table></figure><p>Result:<br>  segmentOutput:<br>    第一 次 和 家人 一起 装饰 圣诞树 很 开心 。<br>  segmentOutputWithTags<br>    第一/m 次/qv 和/cc 家人/n 一起/s 装饰/vn 圣诞树/n  很/d 开心/a 。/wd</p><p>Besides the segmentation, the NLPIR also enables user to add or delete customized vocabulary into the dictionary, and this is the reason that NLPIR was chosen in this project to compare the segmentation result between old dictionary and new dictionary with new words, and then calculate how much improvement could be made.</p><p>The operation to add/delete vocabulary into the dictionary could be achieved by executing the code below: </p><p>For a New Word: 微博/pro</p><p>Add Operation:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">NLPIR testNLPIR = <span class="keyword">new</span> NLPIR(); </span><br><span class="line">newWordWithTag =  <span class="string">"微博"</span> + <span class="string">" "</span> + <span class="string">"pro"</span>;</span><br><span class="line"><span class="keyword">byte</span>[] newWord = newWordTag.getBytes();</span><br><span class="line">testNLPIR.NLPIR_AddUserWord(newWord);</span><br><span class="line"></span><br><span class="line">Delete Operation:</span><br><span class="line">testNLPIR.NLPIR_DelUsrWord(newWordWithTag);</span><br></pre></td></tr></table></figure><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The project was to implement an efficient solution for Chinese sentence segmentation onto informal text that was widely used in social networks. Therefore the text extracted from weibo, the best-known Chinese social networks nowadays and it was described as Chinese version twitter, was regarded as the perfect data to be used into this project. Besides that, in consideration of the real-time feature of the invention of new vocabulary on Internet, the mass weibo text sorted in chronological order in a given time period was required to simulate this real-time procedure and test the efficiency of the project.</p><p>The dataset used in this project was the historical weibo text of 2695 randomly chosen users until the 25^th^ December 2013. There are approximately 0.7 million weibo posts contained into 1.95GB uncompressed JSON files.</p><p>Here is the detailed information of the dataset:</p><div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/table-3.png" title="A Sample of Dataset"><br></div></div><h3 id="Test-Data"><a href="#Test-Data" class="headerlink" title="Test Data"></a>Test Data</h3><p>In order to test the performance of the implementation completely, three categories including both informal user generated text extracted from weibo and other kinds of more formal text found from Internet were adopted as the text data set. These three categories include:</p><p>(i) Formal Category</p><p>Since the topic of this project is Chinese Segmentation in user generated content, the informal user generated data is essential in testing the performance of this project. A text file contains 1500 weibo posts that had not been used in new word discovery were selected as the test data from informal category.</p><p>(ii) Semi-Formal Category</p><p>Nowadays, most of articles on the Internet were written in popular Internet slang even they were news articles or review articles. To ensure the algorithms implemented in this project could be used widely on Internet, the semi-formal test data was added. In this category, one review article and one news article were randomly selected.</p><p>(iii) Informal Category</p><p>Even through the existing Chinese segmentation product could segment formal text in pretty high efficiency and precision, the formal test data was still essential in the testing. The objective of selecting formal data in testing this project, which is Chinese segmentation in informal content, was to ensure the performance of algorithms implemented in this project could still maintain a high level in segmenting formal text, and improve the efficiency in informal text segmentation simultaneously. Here the Annual Government Report of China in 2014, which could be the most formal Chinese language in the world, was selected in this category.</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology "></a>Methodology </h2><p>To realize the goal of this project, which is more accurate Chinese segmentation in user generated content such as weibo, the student has to come up with a method to make the segmentation tool recognized as much words in given sentences as possible. The essential of the solution to this problem is the new words discovery and recognition.</p><p>The “new words” is a broad concept that includes latest vocabulary invented and other un-recognized words like short writing, informal writing with homophone or homograph. Most of these new words are invented and spread widely in the informal language on the Internet, for example in some forum, or social networks like weibo.</p><p>There is very common scenario that the sentences chosen from news article could be segmented by existing Chinese segmentation tool very well and sometimes the precision of the segmentation could reach almost 100%. But when these segmentation tools being tested by some posts extracted from weibo, the result is unsatisfying. This is because the dictionary is always limited and incomplete when facing to the user generated content on the Internet even it updates frequently. To make sure the “new words” in user generated content could be recognized automatically, a new words discovery program is needed to be implemented based on the existing Chinese segmentation tool, analysing and processing segmentation result from those segmentation tool, then telling the new words to the user.</p><p>Here is a sample sentence showing the difference between segmentation result output by NLPIR 2013 and the expected segmentation result after processing by this project.</p><div align="center"><br><div style="width:55%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/7-nlpir-seg.png"><br></div></div><h3 id="New-Words-Extraction-from-NLPIR-Segmentation-Result"><a href="#New-Words-Extraction-from-NLPIR-Segmentation-Result" class="headerlink" title="New Words Extraction from NLPIR Segmentation Result"></a>New Words Extraction from NLPIR Segmentation Result</h3><p>To find the hidden “new word” from the NLPIR segmentation result, a definition about what kind of word can be assumed as “new word” should be given before the algorithm design phrase. After reading and analysing plenty of the segmentation output, the student predicated that the invention of new words could be tracked by the increasing usage and frequency occurred on the Internet in a specific period. For example, after the case of a two-year-old baby’s death after a motorist throwing on the ground, a special word “摔婴”, which literally means “throwing baby”, emerged on the Internet and widely known among Internet user.</p><p>Therefore an experiment was carried out to test whether new words could be found by observing its frequency, in another word, whether we can differentiate “new words” or non-word atom sequence according to their occurring frequency.</p><p>Assumption: all new words are constructed by <em>two</em> atoms.</p><p><strong>Step 1</strong>: Randomly choose five consecutive text files contains weibo posts.</p><p>(e.g. 10.txt, 11.txt, 12.txt, 13.txt, 14.txt five files were chosen)</p><p><strong>Step 2</strong>: Find all two-atom-combinations which occurred in all these five files</p><p><strong>Step 3</strong>: Count their occurrence in all 5 files and analysis the relevance between the tendency of occurrence frequency and its possibility of being a “new word”.</p><p>w1, w2, w3, w4, w5 are the list of atoms<br>    segmented from file1, file2, file3, file4 and file5</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; w1.length – <span class="number">1</span>; i++) &#123;</span><br><span class="line">word = w1[i]+w1[i+<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> (w2.contains(word) &amp;&amp; w3.contains(word) </span><br><span class="line">&amp;&amp; w4.contains(word) &amp;&amp; w5.contains(word)) &#123;</span><br><span class="line">print i and its occurrence frequency in all <span class="number">5</span> files</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here is the figure showing the count of the occurrence in all 5 files for some of candidate “new words”:</p><div align="center"><br><div style="width:85%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/8-occurrence-freq.png"><br></div></div><p>It is not difficult to see the distribution of the occurrence frequency of some chosen candidate new words in consecutive five files. If these 10 candidate new words were separated into two groups: one group contains those candidates used to have great fluctuation on the figure, the other group contains all the other candidates:</p><p>Table 4 an Example of the<br>candidates found from 5 consecutive files</p><table><thead><tr><th><strong>Group</strong></th><th><strong>Candidate</strong></th><th><strong>f1</strong></th><th><strong>f2</strong></th><th><strong>f3</strong></th><th><strong>f4</strong></th><th><strong>f5</strong></th><th><strong>True Vocabulary</strong></th></tr></thead><tbody><tr><td><strong>A</strong></td><td><strong>短/a-信/n</strong></td><td>6</td><td>10</td><td>22</td><td>4</td><td>11</td><td>Yes</td></tr><tr><td></td><td><strong>吐/v-槽/ng</strong></td><td>2</td><td>2</td><td>2</td><td>4</td><td>8</td><td>Yes</td></tr><tr><td></td><td><strong>更/d-好/a</strong></td><td>1</td><td>1</td><td>1</td><td>16</td><td>2</td><td>Semi</td></tr><tr><td></td><td><strong>推/v-送/v</strong></td><td>4</td><td>2</td><td>2</td><td>2</td><td>14</td><td>Yes</td></tr><tr><td></td><td><strong>截/v-图/n</strong></td><td>2</td><td>1</td><td>9</td><td>1</td><td>1</td><td>Yes</td></tr><tr><td></td><td><strong>一/m-天/qt</strong></td><td>4</td><td>7</td><td>17</td><td>9</td><td>12</td><td>No</td></tr><tr><td><strong>B</strong></td><td><strong>是/vshi-吧/y</strong></td><td>2</td><td>1</td><td>1</td><td>3</td><td>2</td><td>No</td></tr><tr><td></td><td><strong>妹/n-纸/n</strong></td><td>3</td><td>1</td><td>1</td><td>1</td><td>5</td><td>Yes</td></tr><tr><td></td><td><strong>去/vf-的/ude1</strong></td><td>3</td><td>1</td><td>1</td><td>1</td><td>1</td><td>No</td></tr><tr><td></td><td><strong>一/m-种/q</strong></td><td>1</td><td>1</td><td>2</td><td>1</td><td>3</td><td>No</td></tr></tbody></table><p>If we were looking for new words from these two groups, we can find that the accuracy in group A, those candidates has obvious fluctuation, is 66.67% or 83.33% if we count “更好” as one word; whereas, the group B could only 25.00%, lower to the group A obviously. And the most important thing is, when more candidates were compared, or larger size of data was involved into this experiment, the outcome is stronger to prove that those candidates with greater fluctuations in frequency have higher probabilities to be a true vocabulary, or a “true new word”.</p><p>The explanation from the student for this phenomenon is that for the invention of every true new word, it must experience a process that being used by more and more user, or occurring on the Internet more and more frequently. And this process became the great slope leading to higher value in the graph during some periods in history. That means, the fluctuation in the graph is the necessary condition of the behaviour of a legal new word.</p><p>This experiment provided a guideline to find the candidate of new words. Therefore all the candidate words in the project were extracted from mass weibo text by this method. Since according to relevant study in Chinese language, 99% Chinese vocabulary has a length less than five characters – that means the new word discovery only considering those candidates with length between 2 to 4 characters is fair enough.</p><p>here is only one possibility to find a candidate new word in length of 2 characters, which is the candidate formed by 2 consecutive atoms with length of 1 Chinese character. For those candidates in length of 3 and 4, there are multiple possibilities of the form of those candidates and the new word discovery need to consider all of these possibilities in theory. </p><p><span id="_Toc383415740" class="anchor"></span>Table 5 The Possible<br>Combination of New Words in Different Length</p><table><thead><tr><th><strong>Length</strong></th><th><strong>Form</strong></th><th><strong>Example （Chinese Char）</strong></th></tr></thead><tbody><tr><td>2</td><td>A+B</td><td>微+博</td></tr><tr><td>3</td><td>A+B+C, AA+B, A+BB</td><td>喵+星+人，东北+银</td></tr><tr><td>4</td><td>A+B+C+D, A+BB+C, A+B+CC, A+BBB, AA+B+C, AA+BB, AAA+B</td><td>喜+大+普+奔，累+觉+不+爱，顶+礼+膜拜</td></tr></tbody></table><p>Not all the combination of atoms in the forms listed in the table above could be assumed as candidates. The criteria to judge one combination of atoms is a candidate is designed based on the rule found during the experiment. When the program looking for the combination of atoms, the program counting its occurrence frequency, too. Once there is evidence showing one sequence of atoms that not only following the construction listed in the table above, but also occurred in a number of consecutive files with increasing in frequency (the up slope in the graph), this sequence of atoms is a candidate new word.<br>The pseudo-code of this process could be expressed as:</p><p>Assumption:<br>number of consecutive files = 3,<br>only the sequence of atoms has count of occurrence shows 2x increasing when compared to the previous file is considered: the multiplier k = 2,<br>All new words has length of 2.</p><p>given w1, w2 and w3 as the list of atoms from file1, file2 and file3<br>given k = 2</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; w1.length – <span class="number">1</span>; i++) &#123;</span><br><span class="line">word = w1[i]+w1[i+<span class="number">1</span>]</span><br><span class="line"><span class="keyword">if</span> (w2.contains(word) &amp;&amp; w3.contains(word)) &#123;</span><br><span class="line"><span class="keyword">if</span>(count(word, w2)&gt;=k*count(word, w1) </span><br><span class="line">&amp;&amp; count(word, w3)&gt;=k*count(word, w2)) &#123;</span><br><span class="line">add word into the candidateNewWord list</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Qualification-of-New-Words-from-Candidates"><a href="#Qualification-of-New-Words-from-Candidates" class="headerlink" title="Qualification of New Words from Candidates"></a>Qualification of New Words from Candidates</h3><p>In this subsection, four different methods/algorithms are introduced to pick up and certify the candidates we got from last step. Each of these four methods/algorithms will be introduced in detail and the analysis on the outcome will be expressed into the data and charts obtained from the experiment.</p><h4 id="1-Unsupervised-Unconditional-New-Word-Discovery"><a href="#1-Unsupervised-Unconditional-New-Word-Discovery" class="headerlink" title="(1) Unsupervised Unconditional New Word Discovery"></a>(1) Unsupervised Unconditional New Word Discovery</h4><p>As the name of this method suggests, this method doesn’t do any selection or screening work onto the candidate new words. Every candidate new words extracted from the given numbers of weibo text will be added into the user customized dictionary and used into further segmentation. This method seems feasible especially when both the number of consecutive files and the multiplier k are set to big enough in value. However, the actual experiment result denied this hypothesis.</p><p>Firstly, an experience was conducted to find the n and k, which are the number of the files in each new word discovery and the multiplier of frequency to identify the new word in n consecutive files. The controlling variables method was applied in this experiment on both n and k to provide a suggestion in choosing the optimum value of these two variables in following part of project.</p><div align="center"><br><div style="width:55%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/9-num-candidate.png" title="The Number of Candidate with Different Combination of k and n"><br></div></div><p>In figure 9, we found the number of candidate new words was varied with different multiplier of frequency in n consecutive files. And the number of candidate new word when n=3 is the best instance which reflected the continuity of the process of new words’ invention, and at the same time kept the number of candidate new words founds in range from 1 to 150 with different <em>k</em> value, those are some values neither too big to do the manually statistics nor too small to satisfy the fairness of the final conclusion in this project.</p><div align="center"><br><div style="width:55%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/10-num-candidate.png" title="The Number of Candidate New Words with Different Value k when n=3"><br></div></div><p>After the confirmation of assigning 3 to the variable n, now let’s focus on the proper k value to use in following experiments. There are curves of 8 combinations of 3 consecutive files filled with weibo posts in timely order on the plot. It is not difficult to find that when there are too many candidate new words found when given k=1, however, when k is assigned to some other value greater than 2, the number of candidate new words seems too small because of too strict condition. For example, k=3 means a combination of segmented atoms could be assumed as a candidate new word only its occurrence frequency triples each time. In consideration of both reasonability and proper number of candidate new word found, k=2 is chosen in the end.</p><p>The unsupervised unconditional new word discovery was implemented after the confirmation of assignment of all key variables. The term “unsupervised” means no more interference from user will be imposed on the new word discovery, the project was executed totally by itself And the term “unconditional” means all the candidate new words was supposed to be true new word, or new invented vocabulary without being filtered with any condition.</p><p>The pseudo code of unsupervised unconditional new words discovery was given below:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">1</span>;</span><br><span class="line">ArrayList&lt;String&gt; newWordList = extractNewWordsFrom3Files(</span><br><span class="line">candidateNewWords(s1),</span><br><span class="line">candidateNewWords(s2),</span><br><span class="line">candidateNewWords(s3));</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">candidateNewWords</span><span class="params">(String s)</span>:</span></span><br><span class="line"><span class="function">String[] atoms </span>= s.split(“ ”);</span><br><span class="line">ArrayList&lt;String&gt; candidates = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> count = <span class="number">0</span>; count&lt;atoms.length-<span class="number">1</span>; count++)&#123;</span><br><span class="line"><span class="keyword">if</span>((atoms[count].length()==<span class="number">1</span>) &amp;&amp; (atoms[count+<span class="number">1</span>].length()==<span class="number">1</span>))</span><br><span class="line">candidates.add(atoms[count]+     atoms[count+<span class="number">1</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> candidates;</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">extractNewWordsFrom3Files</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">ArrayList&lt;String&gt; al1, ArrayList&lt;String&gt; al2, ArrayList&lt;String&gt; al3)</span></span></span><br><span class="line"><span class="function">ArrayList&lt;String&gt; newWords </span>= <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">String candidate = “”;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;atoms.length-<span class="number">1</span>; i++)&#123;</span><br><span class="line">candidate = al1.get(i);</span><br><span class="line"><span class="keyword">if</span>((al2.contains(candidate) &amp;&amp; (al2.contains(candidate)) &amp;&amp;</span><br><span class="line">(al2.count(candidate)/al1.count(candidate)&gt;=k) &amp;&amp; (al2.count(candidate)/al1.count(candidate)&gt;=k))</span><br><span class="line">newWords.add(al1.get(i));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> newWords;</span><br></pre></td></tr></table></figure><blockquote><p>Input:<br>s1: “过去 一 年 ， 我 每 天 都 会 使用 微 博 。”<br>s2: “微 博 现在 非常 流行 ， 我 每 天 都 用 微 博 ， 我 很 喜欢 。”<br>s3: “我 每 天 都 去 学校 ，我 常常 在 路 上 刷 微 博 ， 发 微 博 。”</p></blockquote><p>Output (only the words typed in underline bold style are true new words):<br>newWordList: “，我” “我每” “每天” “天都” “微博”</p><p>As results showed above, this unsupervised unconditional could found all potential new vocabulary from Internet in theory, but the problem it brought to us was there were too many useless candidate new words like the combination of atoms containing punctuations also found and mixed with those real new words, which imposed too much workload to choose real new words from the result manually. Obviously, this is not a good solution to discover new words.</p><h4 id="2-Unsupervised-Conditional-New-Words-Discovery"><a href="#2-Unsupervised-Conditional-New-Words-Discovery" class="headerlink" title="(2) Unsupervised Conditional New Words Discovery"></a>(2) Unsupervised Conditional New Words Discovery</h4><p>Since the result of new words discovery was not satisfying in unsupervised unconditional method, some improvement was urgent to be made to realize the goal of this project, which is a more efficient way to find the new words in user generated content.</p><p>By observing the result of the first method carefully, it was easy to find there are some of these candidate new words are obviously impossible to be one vocabulary or a new word. In another way, we could make a hypothesis that there is a rule to judge whether a combination of segmented atoms is a new words or not. And this rule was based on the POS tags of each segmented atoms, which is the properties of those atoms in the usage of Chinese.</p><p>After analysing the result from last method, some specific combinations of segmented atoms could be determined that they were mostly impossible to be qualified as a new word, especially a word made up with 2 characters. They included:<br>(a) Candidates new words contains punctuation(s) e.g. “，我”</p><p>(b) Candidates new words contains digits(s) e.g. “1个”</p><p>Besides that, based on the statistical work conducted for the known new words and analysis on Chinese words’ morphology, a rule can be summarized that most of new words were invented to describe a new phenomenon or new action. That means most of the new words created on Internet were nouns, adjectives or verbs. For those kinds of words made up with two Chinese characters, their combinations of POS tags mainly belong to one of those listed below:</p><p><span id="_Toc383415741" class="anchor"></span>Table 6 Some Common Combination of New Word</p><table><thead><tr><th><strong>Adjective/Noun</strong></th><th><strong>Verb</strong></th></tr></thead><tbody><tr><td>Noun + Noun, e.g. 砖家/n</td><td>Verb + Verb, e.g. 签售/v</td></tr><tr><td>Adjective + Adjective, e.g. 高大/a</td><td>Verb + Adverb, e.g. 走起/v</td></tr><tr><td>Adjective + Noun, e.g. 微信/n</td><td>Verb + Noun, e.g. 翻墙/v</td></tr><tr><td></td><td>Adverb + Verb, e.g. 猛戳/v</td></tr></tbody></table><p>Therefore, two filters were implemented onto the code of the first method to improve the precision of the new words discovery. The first filter (filter A) was designed to filter out all combinations of atoms containing punctuations and digits before adding them into the list of candidate new words. Then the second filter (filter B) restricted the condition to determine whether a candidate new word is new word or not. In this filter, only the candidates satisfying the seven combinations of POS tags listed above would be considered and added into the new words’ list in the end.</p><p>The reason of implementing 2 filters rather than using the 2nd one only was the POS tags in NLPIR were varies in different sentence or different words. But the POS tagging operation still has some rules to track with:</p><p>All the POS tags of punctuation symbols are starting with a “w”. For example the period in Chinese, “。”, is “wd” and the comma “，” is “wj”.</p><p>All verbs’ tags must have a letter “v”, all adverbs’ tags must have “d” , all noun/pronouns’ tags must have a letter “n” and all adjectives must have “a” in their tags.</p><p>To prevent the period was recognized as an adverb, double filter method was applied to provide a safe way to proceed with the conditional new words discovery. This implementation was summarized as unsupervised conditional new words discovery, it removed out all two consecutive atoms contains punctuation and digits before picking up those belong to specific POS tags combination.</p><div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/11-procedures.png" title="The Procedures of Unsupervised Conditional Algorithm"><br></div></div><p>The figure above showed the procedures and specific result of the unsupervised conditional new words discovery. It proved that the unsupervised conditional new words discovery could greatly decrease the number of qualified new words in the end. But the usage of filter was not perfect, sometimes even the true new words could not be qualified by the filter because of too strict and limited fileting condition in POS tags to recognize all new words. For instance, the atom combination “给/prep” and “力/noun” (给力/adj - awesome), and some un-recognized words marked as “nr” like the atom combination “尼/b” and “玛/nr” (尼玛/int - interjection), they are true new words and widely used on Internet but failed to be found in current new word discovery algorithm because their POS tags are too rare to be added into the filter.</p><h4 id="3-Active-Learning-New-Words-Discovery"><a href="#3-Active-Learning-New-Words-Discovery" class="headerlink" title="(3) Active-Learning New Words Discovery"></a>(3) Active-Learning New Words Discovery</h4><p>We can image that most of regular new words could be found by unsupervised new words discovery when as complete as possible conditions are given. But for some new words composed by the character with rare or even un-recognized POS tags, they seem impossible to pass the filter and then founded by unsupervised new word discovery. Now we need to move back to the essential feature of our definition of “new words”. Before designing the algorithm, several experiments was carried out to find proper value of the variable n and k, and then we made an assumption in this project that any consecutive characters had k times exponential growth in frequency among minimum n consecutive time period could be defined as a new word invented on Internet. That means all candidate new words generated actually hold same possibility to be a valid new word, or a true vocabulary. Therefore, an active-learning algorithm was designed to release the power that determines whether a candidate is a truly new word or not to the user, and the program only focused on learning from user’s input and summarizing them into a collection of rules to use into further new words discovery.</p><p>Firstly, instead of only recording the details of two consecutive atoms, the active learning new word discovery saved both two atoms themselves and two adjacent atoms. And each time a pair of atoms and their POS tags showed up, the user could also check the POS tag of their previous atom and their next atom. These previous tag and next tag was called transition record upon a candidate new word.</p><div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/12-difference.png" title="Difference between Unsupervised & Active-Learning Algorithm in New Words Discovery"><br></div></div><p>Besides that, not only those candidate new words that satisfying k times exponential increasing in frequency would be saved, another list of atoms’ pair was set up to save those candidates failed to satisfying k times exponential rule but existing in consecutive n time periods. The first list with regular candidate new words was renamed to candidates with higher frequency, or “HF”, at this time, and the second list was named candidates with lower frequency, shortly recorded as “LF”.</p><p>During the execution, the program would keep querying those candidate new words from “HF”. The content of query included whether it was a word or not and the POS tag of this new word if yes. Once user gave positive response and provided the POS tag, the program would search all others candidates from “HF” and added those candidate new words, who shared same POS tags for both transition record and two atoms’ themselves, together with the candidate queried to the user, into the qualified new words list. Because the candidates who shared same transition states and POS tags with a known true new word, were believed to be true new words also. And each time user response to query to a candidate new word, this candidate new word and all candidates automatically qualified and added into the new words list would be removed from the list “HF”, those to maintain the number of query within a lower level.</p><p>This algorithm is an active-learning process to concede the authority to the user and machines itself would focus on the learning and further automation. And the pseudo-code of this algorithm is given:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">HashMap&lt;String, ArrayList&lt;String&gt;&gt; tagTransitionMap;</span><br><span class="line"></span><br><span class="line">ArrayList&lt;String&gt; newWordList = extractNewWordsFrom3Files(</span><br><span class="line">candidateNewWords(s1),</span><br><span class="line">candidateNewWords(s2),</span><br><span class="line">candidateNewWords(s3));</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">candidateNewWords</span><span class="params">(String s)</span>:</span></span><br><span class="line"><span class="function">    String[] atoms </span>= s.split(“ ”);</span><br><span class="line">    ArrayList&lt;String&gt; candidates = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> count = <span class="number">0</span>; count&lt;atoms.length-<span class="number">1</span>; count++)&#123;</span><br><span class="line">        <span class="keyword">if</span>((atoms[count].length()==<span class="number">1</span>) &amp;&amp; (atoms[count+<span class="number">1</span>].length()==<span class="number">1</span>))</span><br><span class="line">            String prev = count &gt; <span class="number">0</span> ? atoms[count-<span class="number">1</span>].split(<span class="string">"/"</span>)[<span class="number">1</span>] : <span class="string">"*"</span>;</span><br><span class="line">            String next = count &lt; atoms.length-<span class="number">2</span> ? atoms[count+<span class="number">2</span>].split(<span class="string">"/"</span>)[<span class="number">1</span>] : <span class="string">"*"</span>;</span><br><span class="line"><span class="comment">//The “*” means it can be any tag to match</span></span><br><span class="line">            candidates.add(atoms[count] + atoms[count+<span class="number">1</span>]);</span><br><span class="line">            String transition = prev + atoms[count].split(<span class="string">"/"</span>)[<span class="number">1</span>] +</span><br><span class="line">                atoms[count+<span class="number">1</span>].split(<span class="string">"/"</span>)[<span class="number">1</span>] + next;</span><br><span class="line">            tagTransitionMap.put(transition, </span><br><span class="line">                tagTransitionMap.get(transition).add(atoms[count] + atoms[count+<span class="number">1</span>]));</span><br><span class="line">&#125;</span><br><span class="line">    <span class="keyword">return</span> candidates;</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">extractNewWordsFrom3Files</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    ArrayList&lt;String&gt; al1, ArrayList&lt;String&gt; al2, ArrayList&lt;String&gt; al3)</span></span></span><br><span class="line"><span class="function">    ArrayList&lt;String&gt; newWords </span>= <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    String candidate = “”;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;atoms.length-<span class="number">1</span>; i++)&#123;</span><br><span class="line">        candidate = al1.get(i);</span><br><span class="line">        <span class="keyword">if</span>((al2.contains(candidate) &amp;&amp; (al2.contains(candidate)) &amp;&amp;</span><br><span class="line">                (al2.count(candidate)/al1.count(candidate)&gt;=k) &amp;&amp; (al2.count(candidate)/al1.count(candidate)&gt;=k)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (AskUser(candidate) != <span class="keyword">null</span>)&#123;</span><br><span class="line">                newWords.add(candidate);</span><br><span class="line">                String transition = RetrieveTransition(candidate);</span><br><span class="line">                newWords.addAll(tagTransitionMap.get(transition));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newWords;</span><br></pre></td></tr></table></figure><p>The algorithm recently introduced was temporarily marked as Active-Learning new Word Discovery (Type A), and there is another algorithm being named Active-Learning new Word Discovery (Type B) with a few difference with the Type A one. In algorithm Type A, only candidates in “HF” were involved and only the match of full transition (including both <em>prev</em> and <em>next</em>) candidates could be added into the new words list automatically. But in Type B, these two restrictions were eased to find more related new words:</p><ul><li><p>In Type A, only candidate new words from list “HF” would be eligible as a new word, and only full transition match will be added by machine without user’s permit.</p></li><li><p>However, in Type B, only candidates from list “HF” will be queried to user but all candidates from both “HF” and “LF” were eligible to be a truly new word. Once receiving user’s positive response onto one candidate, all candidates from “LF” than matched their full transition and all candidates from “HF” than matched any one side of transition (<em>prev</em> or <em>next</em>) would be automatically added into the true new words list.</p></li></ul><div align="center"><br><div style="width:40%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/13-algo-type-a.png" title="The Algorithm of Active-Learning (Type A)"><br><img src="/blog/2014/05/20/chinese-segementation/14-algo-type-b.png" title="The Algorithm of Active-Learning (Type B)"><br></div></div><h1 id="Measurement"><a href="#Measurement" class="headerlink" title="Measurement"></a>Measurement</h1><p>The measurement of this project contains two main parts. The first one it to measure how good the new words discovery algorithms designed in this project could find the new words; the other one is how good the segmentation program could behaviour with new words found in this project added into the dictionary. The results of these two measurements could directly reflect the project was successful or not, and provide the proof to the evaluation of this project.</p><h2 id="Measurement-for-New-Words-Discovery"><a href="#Measurement-for-New-Words-Discovery" class="headerlink" title="Measurement for New Words Discovery "></a>Measurement for New Words Discovery </h2><p>The evaluation of new words discovery algorithms could be done by calculating their efficiency in finding the truly new words. For example, if there was a new word discovery algorithm A finding 100 new words but only 10 of them were true and valid new words (with efficiency 10%), and algorithm B finding only 10 new words but 8 of these 10 words were true and valid (with efficiency 80%), we cannot assert that the algorithm A had better performance than B because algorithm A contains more useless “words” and thus probably resulted in higher noises in the actual segmentation. </p><p>The procedure of the result statistics could be summarized as:</p><p>(1) Choose test data from different categories: Formal Article, Semi-formal Article and weibo Post (Informal User Generated Content), to test how much improvement could be made on different usage of Chinese segmentation.</p><p>(2) Run the four new words discovery algorithms separately and saved the new words found into different place.</p><p>(3) Manually Count the number of true words from the new word discovery result generated by each algorithm.</p><p>(4) Calculate the efficiency by dividing the number of true new words over the total number of words found for each of four algorithms.</p><h2 id="Measurement-for-Chinese-Segmentation"><a href="#Measurement-for-Chinese-Segmentation" class="headerlink" title="Measurement for Chinese Segmentation"></a>Measurement for Chinese Segmentation</h2><p>As what had mentioned in the Literature Review, the results of Chinese segmentation in this project were measured in terms of Precision, Recall and F-1 score.</p><div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/15-test-matrix.png" title="A 2×2 Contingency Table Showing 4 Outcomes in Classification Tasks"><br></div></div><p>The procedure of the result statistics could be summarized as:</p><p>(1) Run the four new words discovery algorithms separately and saved the new words found into different place.</p><p>(2) Choose test data from different categories: Formal Article, Semi-formal Article and weibo Post (Informal User Generated Content), to test how much improvement could be made on different usage of Chinese segmentation.</p><p>(3) Run the NLPIR 2013 program with original dictionary and with new words generated by four algorithms added into the customized dictionary one by one, save the segmentation result for further analysis.</p><p>(4) Manually Segment these test data, Record the result as the condition outcome, or the “Gold Standard” mentioned in figure 16.</p><p>(5) Manually Count the number of words correctly segmented, mark as “True Positive”; the number of words wrongly segmented, mark as “False Positive”; and the number of atoms failed to be segmented as a word, mark as “False Negative”.</p><pre><code>E.g. Original Sentence: “结婚的和尚未结婚的”Condition / “Gold Standard”: “结婚/的/和/尚未/结婚/的”Test Outcome: “结婚/的/和尚/未/结婚/的”True Positive: 4 (“结婚”, “的”, “结婚”, “的”)False Positive: 1 (“和尚”)False Negative: 1 (“尚未”)</code></pre><p>(6) Calculate the Precision and Recall for the segmentation result of each algorithm. The equation to calculate precision and recall were given below:</p><div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-7.png"><br></div></div><p>(7) Calculate the F-1 Score to evaluate the comprehensive performance in segmentation for each algorithm. The equation to compute the F-1 score was shown here:</p><div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/equation-8.png"><br></div></div><h1 id="Result-and-Analysis"><a href="#Result-and-Analysis" class="headerlink" title="Result and Analysis"></a>Result and Analysis</h1><h2 id="Efficiency-of-New-Word-Discovery"><a href="#Efficiency-of-New-Word-Discovery" class="headerlink" title="Efficiency of New Word Discovery"></a>Efficiency of New Word Discovery</h2><p>The results reflected the efficiency of four new words discovery algorithms were listed into the table in next page. These four algorithms were Unsupervised Unconditional New Words Discovery (shorted as UU), Unsupervised Conditional New Words Discovery (shorted as UC), Active-Learning New Words Discovery Type A (shorted as Active Type A) and Active-Learning New Words Discovery Type B (shorted as Active Type B).</p><p>As for some new words found from these algorithms such as “一个”, “读书”, “小狗”, “不错”, “较好”, “最大”, which could be recognized as either a vocabulary entity or a combination of two words. In NLPIR 2013, this kind of atoms’ combinations would not be recognized as single words according to NLPIR dictionary standard. However, segmenting this kind of atoms’ combinations as distinct “words” will not leave any negative influence to the existing segmentation tool but it improved the efficiency of new words discovery algorithms a lot since these words totally satisfy the rule of a “new word” in the algorithms implemented in this project. Therefore, this kind of words were marked as “reasonable words” and the efficiency of new words discovery was calculated twice, the first one strictly followed the NLPIR standard without counting reasonable words in; and the other version of efficiency considered those reasonable words as valid new words. </p><table><thead><tr><th><strong>What this Project Recognized</strong></th><th><strong>What NLPIR Recognized</strong></th></tr></thead><tbody><tr><td>一个/q</td><td>一/m 个/q</td></tr><tr><td>读书/v</td><td>读/v 书/n</td></tr><tr><td>小狗/n</td><td>小/ad 狗/n</td></tr><tr><td>不错/ad</td><td>不/no 错/ad</td></tr><tr><td>较好/ad_comp</td><td>较/d 好/ad</td></tr><tr><td>最大/ad_sup</td><td>最/d 大/ad</td></tr></tbody></table><p>Table 7 Samples of Reasonable Words</p><table><thead><tr><th></th><th><strong>New Words Found</strong></th><th><strong>Valid New Words</strong></th><th><strong>Efficiency</strong></th><th><strong>Reasonable Words Found</strong></th><th><strong>Efficiency (with Reasonable Words Counted In)</strong></th></tr></thead><tbody><tr><td>UU</td><td>3945</td><td>189</td><td>4.7909%</td><td>114</td><td>7.6806%</td></tr><tr><td>UC</td><td>920</td><td>161</td><td>17.5000%</td><td>97</td><td>28.0435%</td></tr><tr><td>Active Type A</td><td>487</td><td>173</td><td>35.5236%</td><td>51</td><td>45.9959%</td></tr><tr><td>Active Type B</td><td>698</td><td>224</td><td>32.0917%</td><td>130</td><td>50.7163%</td></tr></tbody></table><p>Table Efficiency of New Words Discovery</p><p>From the result, it is easy to conclude that the Active-Learning algorithm Type B is the best in both the total number of valid new words found and efficiency. On the contrary, the efficiency of Unsupervised Unconditional new words discovery algorithm (UU) is only 7.6806% even counting in all reasonable words. That means each time a hundred “new words” found by UU algorithm, there are more than 92 of them are not truly new words and these fake new words might severely affect the segmentation outcome in the end.</p><h2 id="Evaluation-on-Chinese-Sentence-Segmentation"><a href="#Evaluation-on-Chinese-Sentence-Segmentation" class="headerlink" title="Evaluation on Chinese Sentence Segmentation"></a>Evaluation on Chinese Sentence Segmentation</h2><p>In this section, all 4 algorithms were tested by adding the new words found into the dictionary and then analysing the segmentation results on 3 text file. The result was represented in the form of Precision, Recall and F1-Score. </p><div align="center"><br><div style="width:65%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/table-9.png" title="Comparison of Precision, Recall and F-1 Score between Original NLPIR (The Maximum Value was Underlined)"><br></div></div><p>By observing the data shown in the Table 8, it is not difficult to find that the Unsupervised Unconditional New Words Discovery algorithm could always lead a higher precision, and all the new words discovery algorithms seem lead to worse recall when compared to the original NLPIR output. These phoneme could be explained that all these four algorithms had more or less added some useless words into the dictionary during the new word discovery. More valid new words found means potentially higher precision of the segmentation, but more useless new words found would bring more noise into the segmentation and make lots of sentences, which should have been successfully segmented, wrongly segmented in the end.</p><p>Therefore, a good solution should not only focus on either precision or recall, but the comprehensive result concerning both these two. A measurement called F-1 Score was adopted here to show us which alternative could lead to a better behaviour accounting for both precision and recall.</p><p>And in order to represent the F-1 score in easier way to understand, a chart was drawn to show the testing result in visual way.</p><div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/blog/2014/05/20/chinese-segementation/16-f1-scores.png" title="Comparison of F-1 Scores among Different Segmentation Algorithms"><br></div></div><p>After calculating the F-1 scores, we could find that only the Active-Learning New Words Discovery algorithms could make improvement on segmentation of informal Chinese language only. That was because the F-1 score was affected by both precision and recall, that means any improvement made on either one but led another parameter in very low level couldn’t be reflected on the F-1 score, on the contrary, the F-1 score might be lower than the original result without any improvement implemented. Besides that, the existing Chinese segmentation tools such as NLPIR 2013 were good enough. This product itself was believed as a best adjustment on precision and recall to realize the maximum F-1 score, and that made any improvement designed for it could be very difficult.</p><p>Even though the significant improvement made on segmentation was extremely hard, the Active-Learning algorithm Type A stilled made over 0.16% enhancement when segmenting Chinese text in informal category, in another word, on user generate content like weibo posts.</p><p>You may ask why the performance of the algorithms implemented by student not enhanced in linear against the formalization of the testing data. The possible explanation might be that the semi-formal data was mostly formed by words in dictionary, but unlike the formal text, there were still some contents made up with new words. And when these algorithms implemented by the student did the regular new words discovery and applied new words into the segmentation, relatively too many useless new words created great noise in segmenting the sentence and that led the original segmentation routine not able to work correctly. Nevertheless, in the testing of formal Chinese language category, all algorithms didn’t affect the original segmentation too much because there were seldom opportunities containing a new words in a very formal article. This made the all these four algorithms resulted in very similar F-1 score in the end.</p><h2 id="Evaluation-on-Chinese-Sentence-Segmentation-1"><a href="#Evaluation-on-Chinese-Sentence-Segmentation-1" class="headerlink" title="Evaluation on Chinese Sentence Segmentation"></a>Evaluation on Chinese Sentence Segmentation</h2><h1 id="Limitation-and-Recommendations"><a href="#Limitation-and-Recommendations" class="headerlink" title="Limitation and Recommendations"></a>Limitation and Recommendations</h1><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>Some limitations was found during this project and considered could vary the evaluation result from the ideal outcome:</p><p>(1) The dataset used in this project was limited. This project needed larger dataset that unlimitedly close to the universe data to obtain the good enough simulative result. Even 1.2 GB was a very big data sample; it still seems not enough for this project if further improvement or research would be carried out.</p><p>(2) The new words discovery algorithms highly relied on the real-time feature of the dataset. But the dataset used in this project was extracted based on specific group of users, rather than extracted from the real-time weibo posts. That would affect the trueness and reliability of the simulation.</p><p>(3) The difference between Chinese and Western language and the ambiguity of Chinese grammar restricted the development of existing Chinese segmentation because nearly all related theory and hypotheses were built based on English. For example, the comparative adjectives and superlative adjectives, these were recognized as distinct words in English but may not in Chinese like “较好”, “最大”. The confusion on the ambiguity, or the lack of a standard, would greatly vary the evaluation result of segmentation.</p><h2 id="Future-Recommendations"><a href="#Future-Recommendations" class="headerlink" title="Future Recommendations"></a>Future Recommendations</h2><p>If this project would be further carried out for some improvement or<br>referenced by other research, here some future recommendations were<br>provided and please take account them into further research:</p><p>(1) Try to use more data in simulating the invention of the new words on Internet.</p><p>(2) Try to implement a real-time data extraction program and used it into the new words discovery for better simulation quality.</p><p>(3) Pay more attention on the complexity and efficiency of the algorithms designed for this project because lots of IO operations are needed to discover new words. Clumsy algorithms and redundant variables are memory costly and probably affect the result of the simulation. (For example, the robust algorithm discovers new words from <em>k</em> files will cost <em>O</em>(n^k^) time, that makes the new words discovery impossible when <em>k</em> is required to be large)</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>The objective of this project is to study on the existing Chinese segmentation techniques and discover any improvement for the segmentation on informal user generated text. In this project, a long period of study on related topics such as Chinese language theory and natural language processing was done by the student. In the end, four algorithms designed for the improvement upon one of the existing Chinese segmentation product were implemented and tested by the data in different categories. The final result showed one of these algorithms named Active Learning New Word Discovery (Type A) had successfully improved the performance of the Chinese segmentation with informal user generated content like weibo posts.</p><p>This project offered a great opportunity that allowed student to participate into the academic research, find the problem and solve it by learning and experiments independently. During the whole year period of this project, the student has obtained not only the knowledge about the natural language processing and Chinese segmentation, but plenty of practices of time management, project management and programming. In the end, the student was succeeded in implementing some improvement and satisfied the requirement by achieving a higher F-1 score. </p><h1 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h1><p>[1] E. K. Steven Bird, Edward Loper, Natural Language Processing with Python, 1st Edition ed. Sebastopol, CA: O’Reilly, 2009.<br>[2] P. T. Gregory Grefenstette, “What is a word, What is a sentence? Problems of Tokenization,” 1994.<br>[3] G. T. Zimin Wu, “￼Chinese Text Segmentation for Text Retrieval: Achievements and Problems,” 1993.<br>[4] P. V. d. Peter F. Brown, Robert L. Mercer, Vincent J. Della Pietra, Jenifer C. Lar, “Class-Based n-gram Models of Natural Language,” 1992.<br>[5] J. M. T. William B. Cavnar “N-Gram-Based Text Categorization,” In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, 1994.<br>[6] D. J. a. J. H. Martin, SPEECH and LANGUAGE PROCESSING: An Introduction to Natural Language Processing,<br>Computational Linguistics, and Speech Recognition, 2nd Edition ed.: Prentice Hall, 2008.<br>[7] J. G. Stanley F. Chen, “An Empirical Study of Smo othing Techniques for Language Modeling,” 1998.<br>[8] H. J. Qian Liu, “A View of Chinese Word Automatic Segmentation Research in the Chinese Information Disposal,” 2006.<br>[9] J. G. Mu Li, Changning Huang, Jianfeng Li, “Unsupervised Training for Overlapping Ambiguity Resolution in Chinese Word Segmentation,” 2003.<br>[10] S. A. Shilpa Arora, “Active Learning for Natural Language Processing,” 2007.<br>[11] Y. S. Shai Fine, Naftali Tishby, “The Hierarchical Hidden Markov Model: Analysis and Applications,” 1998.<br>[12] H.-K. Y. Hua-ping Zhang, De-Yi Xiong, Qun Liu, “HHMM-based Chinese Lexical Analyzer ICTCLAS,” 2003.<br>[13] Xinbo Zhang. (2006). Research on ICTCLAS Segmentation (3) - Atom Segementation. Available: <a href="http://blog.csdn.net/sinboy/article/details/624929" target="_blank" rel="noopener">http://blog.csdn.net/sinboy/article/details/624929</a><br>[14] Xinbo Zhang. (2006). Research on ICTCLAS Segmentation (4) - Rough Segementation. Available: <a href="http://blog.csdn.net/sinboy/article/details/663123" target="_blank" rel="noopener">http://blog.csdn.net/sinboy/article/details/663123</a><br>[15] Xinbo Zhang. (2006). Research on ICTCLAS Segmentation (5) - N-shortest Paths. Available: <a href="http://blog.csdn.net/sinboy/article/details/745498" target="_blank" rel="noopener">http://blog.csdn.net/sinboy/article/details/745498</a></p><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The project has been a challenging but wonderful experience that offered me an opportunity of self-learning and research in related fields to solve a specific problem. This is my first time doing research related work under the supervision of a professor, and what I have learned were not only the knowledge but also the methodology to analyse a problem and then solve it.<br>I would like to express my deepest gratitude to my supervisor, <em>Associate Professor Sun Aixin</em>, for his constantly encouragement and guidance during this one-year-period project, helping me overcome numberless difficulties and challenges that once made me not able to move forward.<br>Finally, a special thanks to my friends and my family had given me suggestions, technical support, and sincere encouragement throughout the project.</p><h1 id="Abbreviations"><a href="#Abbreviations" class="headerlink" title="Abbreviations"></a>Abbreviations</h1><p><strong>NLP</strong> – Natural Language Processing<br><strong>HMM</strong> – Hidden Markov Model<br><strong>POS</strong> – Part of Speech<br><strong>NLPIR</strong> – Natural Language Processing &amp; Information Retrieval Sharing Platform<br><strong>ICTCLAS</strong> – Institute of Computing Technology, Chinese Lexical Analysis System</p>]]></content>
    
    <summary type="html">
    
      &lt;div align=&quot;center&quot; style=&quot;padding-bottom:15px;&quot;&gt;&lt;img src=&quot;/blog/2014/05/20/chinese-segementation/cvr-chinese.png&quot;&gt;&lt;/div&gt;

&lt;p&gt;This project is my final year project to fulfil the requirement of the degree of bachelor of engineering (hon.) at Nanyang Technological University (Singapore), School of Computer Engineering. This project talks about the Natural Language Processing in Chinese Language.&lt;br&gt;
    
    </summary>
    
    
      <category term="academic, thesis" scheme="http://mmhhss1991.github.io/tags/academic-thesis/"/>
    
  </entry>
  
</feed>
