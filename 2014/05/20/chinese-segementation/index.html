<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|PT Serif:300,300italic,400,400italic,700,700italic|PT Serif:300,300italic,400,400italic,700,700italic|Cinzel:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="academic, thesis," />





  <link rel="alternate" href="/atom.xml" title="Mark's Tech Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="This project is my final year project to fulfil the requirement of the degree of bachelor of engineering (hon.) at Nanyang Technological University (Singapore), School of Computer Engineering. This">
<meta name="keywords" content="academic, thesis">
<meta property="og:type" content="article">
<meta property="og:title" content="Chinese Segmentation in User-Generated Content">
<meta property="og:url" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/index.html">
<meta property="og:site_name" content="Mark&#39;s Tech Blog">
<meta property="og:description" content="This project is my final year project to fulfil the requirement of the degree of bachelor of engineering (hon.) at Nanyang Technological University (Singapore), School of Computer Engineering. This">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/cvr-chinese.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/1-new-words.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-1.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-2.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-3.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-4.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/2-chinese-seg.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/3-hhmm-chinese-lex.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-5.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/4-atom-seg.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/5-example-rough-seg.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-6.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/table-2.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/6-n-short.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/table-3.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/7-nlpir-seg.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/8-occurrence-freq.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/9-num-candidate.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/10-num-candidate.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/11-procedures.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/12-difference.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/13-algo-type-a.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/14-algo-type-b.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/15-test-matrix.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-7.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/equation-8.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/table-9.png">
<meta property="og:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/16-f1-scores.png">
<meta property="og:updated_time" content="2018-09-01T10:18:42.724Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chinese Segmentation in User-Generated Content">
<meta name="twitter:description" content="This project is my final year project to fulfil the requirement of the degree of bachelor of engineering (hon.) at Nanyang Technological University (Singapore), School of Computer Engineering. This">
<meta name="twitter:image" content="http://mark-h-meng.github.io/2014/05/20/chinese-segementation/cvr-chinese.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> Chinese Segmentation in User-Generated Content | Mark's Tech Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Mark's Tech Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">技术理想国</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Chinese Segmentation in User-Generated Content
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2014-05-20T21:13:14+08:00" content="2014-05-20">
              2014-05-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <div align="center" style="padding-bottom:15px;"><img src="/2014/05/20/chinese-segementation/cvr-chinese.png"></div>

<p>This project is my final year project to fulfil the requirement of the degree of bachelor of engineering (hon.) at Nanyang Technological University (Singapore), School of Computer Engineering. This project talks about the Natural Language Processing in Chinese Language.<br><a id="more"></a></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>With the rapid development of social media, website like weibo has become a very important channel of getting information in Chinese speaking regions. In past few years, more and more people chose to use weibo to record what happened around them and express their experience and emotion in daily life due to its highly socialized and real-time features. Human society was greatly speeded up by Internet. And many new vocabularies or new informal writing were invented and widely used in the Internet.<br>Thanks to the natural language processing technical, the new vocabulary discovery could be done by computer programs rather than manually search. Some statistical concepts, such as Hidden Markov Model, provided a guideline to design the algorithm that could segment one sentence into a sequence of words with the maximum probability and best rationality. However, the realization of sentence segmentation would be varied for different languages. For instance, the words are naturally separately by spaces in Western languages but for most of East Asian languages such as Chinese, Japanese, Korean or Thai, on the contrary, the words are presented into characters and connected with each other without any separator. And that makes new words discovery in East Asian languages more difficult, challenging and flexible.<br>This project aimed to find an efficient way to discover new Chinese vocabularies invented from the Internet and used them into Chinese article segmentation, and consequently provided suggestion and methods to improve the current Chinese natural language processing technology. In this project, several concepts and techniques related to natural language processing and information extraction were adopted and several times of experiments were carried out to obtain the most desirable result.<br>The experimental results showed that the algorithms implemented for this project could find most of new vocabularies successfully and make some improvement onto the existing Chinese segmentation tools. The detailed result analysis showing how much improvement could be made and further discussion about the limitation and future development work were conducted and reflected in this report.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background "></a>Background </h2><h3 id="Internet-Slang"><a href="#Internet-Slang" class="headerlink" title="Internet Slang"></a>Internet Slang</h3><p>21st Century is the century of Internet. With the explosion in population of Internet users, a new kind of language, the Internet Slang, was introduced into people’s lives and became extremely popular during last few years.</p>
<p>As the language with most people used around the world, Chinese language also been exposed to the language revolution brought by Internet. More and more new vocabularies were invented on the Internet and adopted by people into real daily lives. Within these new vocabularies, some of them were simpler or shorter writing of the formal words (e.g. 女主 - heroine); some of them were invented to express new things or new phenomena in the society (e.g. 微博 - weibo, a Chinese social network like twitter), and what the most interesting is, some of the new words were informal or mistake writing to the existing words intentionally or unintentionally, but more popular than the original ones (e.g. 程序猿 – a homophony of programmer in Chinese).</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/1-new-words.png" title="New words"><br></div></div>

<h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h3><p>The natural language processing (shorted as NLP) became a hot topic in computer science field in last few years. With statistical knowledge and many cutting-edge information technologies like text extraction, machine learning and artificial intelligence involved in, the NLP enables machine to understand human’s natural languages and response with intelligence. These natural languages could not only be English but also Arabic, Germany or even Chinese.</p>
<h3 id="When-Natural-Language-Processing-Meets-Internet-Slang"><a href="#When-Natural-Language-Processing-Meets-Internet-Slang" class="headerlink" title="When Natural Language Processing Meets Internet Slang"></a>When Natural Language Processing Meets Internet Slang</h3><p>The mechanism that enables machine to understand human’s natural languages is completely realized by being given a dictionary with all vocabularies listed in it. And it works well in formal language such as documentations, reports or journals. But in past few years, the Internet slang started being used by people frequently and widely, the traditional natural language processing could not satisfy people’s need just by simply reading language and checking in the dictionary. Therefore, the latest natural language processing technics were expected to discovery new words in natural languages and add them into the dictionary automatically. In fact, the efficiency of the new words discovery nowadays has already became one of the most significant factors for determining the quality of a natural language processing tool.</p>
<h2 id="Project-Objectives"><a href="#Project-Objectives" class="headerlink" title="Project Objectives"></a>Project Objectives</h2><p>The natural language processing in English and other Western languages were well developed in last several decades. Today there are more than one mature products or toolkits for the NLP in English, for example, the well-known personal voice assistant application called “Siri”, or a famous free NLP development toolkit named “Natural Language Toolkit (NLTK)”. However, unlike those language formed by Latin letters, some East Asian languages, such as Chinese, it is presented as consecutive characters rather than writing in letters and segmented by white spaces. In another word, before understanding and analysis the detailed meaning of Chinese language, the machine must be able to segment an entire coherent sentence constructed by a series of Chinese characters in right ways.</p>
<p>There are two main objectives of this project:</p>
<p>Firstly, this project was to learn the existing Chinese sentence segmentation tools and research on the working mechanism behind.</p>
<p>Then, the student was expected to come up with a solution than could make improvement onto one of them for usage onto the texts used in social networks like weibo, and then implement it. The final result will be measured by the precision/recall rate in both the new words discovery and the segmentation result on the given testing text, thereby provide a feasible and effective solution.</p>
<h2 id="Timeline"><a href="#Timeline" class="headerlink" title="Timeline"></a>Timeline</h2><p>The schedule of this project from the beginning of this project until the completion was listed below:</p>
<p><span id="_Toc383415736" class="anchor"></span>Table Project Timeline</p>
<table>
<thead>
<tr>
<th>Time</th>
<th>Task</th>
</tr>
</thead>
<tbody>
<tr>
<td>2013.4</td>
<td>Project topic settled down and first meeting with the supervisor.</td>
</tr>
<tr>
<td>2013.5-2013.9</td>
<td>Read relevant paper and get familiar with existing Chinese segmentation tools, try to understand the mechanism behind, and choose the one to improve.</td>
</tr>
<tr>
<td>2013.9-2013.10</td>
<td>Decide the tool to use, and set up the development environment; Prepare some data to test the tool and position the area/function to make improvement.</td>
</tr>
<tr>
<td>2013.11</td>
<td>Temporarily pause the project for the final examination of semester 1.</td>
</tr>
<tr>
<td>2013.12</td>
<td>Construct the general improvement idea and try to implement step by step.</td>
</tr>
<tr>
<td>2014.1</td>
<td>Meet with supervisor to report the progress, ask for consultation onto the current idea, and confirm the direction ahead for the project.</td>
</tr>
<tr>
<td></td>
<td>Complete the interim report.</td>
</tr>
<tr>
<td></td>
<td>Continue the implementation.</td>
</tr>
<tr>
<td>2014.2</td>
<td>Confirm the dataset selection, and Extract the weibo post texts from the dataset.</td>
</tr>
<tr>
<td></td>
<td>Re-organize and Clean the project code</td>
</tr>
<tr>
<td>2014.3</td>
<td>Finish the coding, analyze the result and write the final report.</td>
</tr>
<tr>
<td>2014.3-2014.4</td>
<td>Complete and submit the amended report, and check any modification needed to be made on the project for the demo and oral defense.</td>
</tr>
</tbody>
</table>
<h2 id="Report-Organisation"><a href="#Report-Organisation" class="headerlink" title="Report Organisation"></a>Report Organisation</h2><p>There are totally 7 chapters in this report, includes introduction, literature review, design of this project, measurement for the project evaluation, results and analysis, limitation and future suggestion, and conclusion.</p>
<p><em>Chapter 2</em> provides information about the background, bring out the problem to be solved and then clarify the objectives. Besides, it also includes the schedule of this project shows a rough timeline contains all the procedures in this project during two semesters.</p>
<p><em>Chapter 3</em> is the literature review, which provides a brief introduction and analysis to some existing theories and knowledge related to this project. At the same time, this chapter also reflects what the student has learnt from doing this project.</p>
<p><em>Chapter 4</em> provides more detailed discussions on the design and development process. Four different methods are introduced to improve the Chinese segmentation in user generate content. In addition, this chapter also introduces how these algorithms are implemented under a specific assumption.</p>
<p><em>Chapter 5 and 6</em> give the methodology to evaluate the project, shows the result data of the evaluation, and provides an analysis and evaluation of the algorithms implemented to meet the requirements of this project.</p>
<p><em>Chapter 7 and 8</em> give limitation and future recommendation of this project, and then an overall conclusion of the entire project is provided. </p>
<h1 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h1><h2 id="Natural-Language-Processing-and-Chinese-Language"><a href="#Natural-Language-Processing-and-Chinese-Language" class="headerlink" title="Natural Language Processing and Chinese Language "></a>Natural Language Processing and Chinese Language </h2><p>Natural language is defined as language that is used for human’s daily communication like English, Portuguese or Chinese, rather than the artificial languages designed for machine’s understanding such as mathematical notations or programming languages. However, the computer couldn’t parse and understand natural language directly. There must be a technical routine called Natural Language Processing, or NLP for short, guiding and operating machines to manipulate the natural language. The NLP is the set of all methodologies and operations related to natural language, it covers the tasks that simply counting the frequency of words in comparison of different writing styles, or extremely understand human’s voice command and response correctly [1].</p>
<p>The basic steps of the NLP in English or other kinds of language in Latin letters’ form contain text tokenization, categorizing and tagging words, syntactic analysis and further analysis and application. For instance, the tokenization in English is simply splitting sentences with white spaces and applying recognition filter for some special cases like Word-internal punctuation (e.g. some abbreviations such as “etc.”, “Vol.”, “Prof.”), clitics (e.g. “I’m”, “doesn’t”) and multi-token words (e.g. “New York”, “Ang Mo Kio”, “Johnson &amp; Johnson”) [2]. But the NLP in many East Asian languages such as Chinese has a little bit difference since the Chinese is written in consecutive Chinese characters and there is no natural boundaries separating each word, the text tokenization process could be much more complex when compared to the NLP in English [3]. That means the Chinese text must be processed by segmentation program for tokenization before further processing such as syntactic analysis.</p>
<h2 id="Language-Model-and-N-Gram"><a href="#Language-Model-and-N-Gram" class="headerlink" title="Language Model and N-Gram "></a>Language Model and N-Gram </h2><p>Suppose an incomplete sentence with one unknown word in specific position, the intuitive way to identify that missing word from plenty of candidates is choosing the one could achieve in highest likelihood probability as a sentence in its natural language. The Chain Rule enable us to compute probabilities of entire sequences like P(w1 , w2 , …, wn). The Chain Rule suggests that we could estimate the joint probability of an entire sequence of words by multiplying a number of conditional probabilities together. However, the human natural language is creative and we cannot ensure any words or sentences have occurred in the corpus. Therefore it seems impossible to estimate the joint probability by counting the number of occurring times in corpus of every word following every long string [4].</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-1.png"><br></div></div>

<p>The N-gram model is designed to solve the predicament that the joint probability seems not able to be calculated efficiently. What N-gram represents is a slice with N-character in length among a longer string. The most widely used N-gram models are uni-gram, bi-gram and tri-gram. According to the Markov Assumption, the probability of a word occurring after a string only related to the joint probability of N words sequence previous to its position (a prefix of N). It could be expressed in the mathematical notations as that:</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-2.png"><br></div></div>

<p>And thereby the joint probability of a whole sentence or paragraph could be calculated in much easier way as well. The formal is supposed like (Suppose N=2) [5]:</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-3.png"><br></div></div>

<p>Since the usage of N-gram is a part of Markov Assumption, the joint probability of a sentence or a sequence of text is just an approximated value. Theoretically speaking, the higher N is would lead to higher precision of the over probability. However, some N-grams would return zero or almost zero in probability that makes the true candidate difficult to be recognized among others. Besides that, the higher N is means higher calculation workload exerted to the machine and less performance efficiency in the prediction, and the Chain Rule formal could be regarded as an extreme situation of N-gram estimation where N is equal to the length of the text sequence [6].</p>
<p>Moreover, in order to prevent the existing of zero probability that might invalidate calculation of the joint probability, a statistics technique named additive smoothing (also known as Laplace smoothing) was widely adopted by different NLP tools. That smoothing technique measures all probabilities by adding a smoothing parameter α to force all estimation on resulting probability fall in the range of its original empirical estimate (xi/N) and uniform probability (1/d) [7].</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-4.png"><br></div></div>

<h2 id="Unsupervised-Chinese-Word-Segmentation"><a href="#Unsupervised-Chinese-Word-Segmentation" class="headerlink" title="Unsupervised Chinese Word Segmentation"></a>Unsupervised Chinese Word Segmentation</h2><p>The unsupervised Chinese word segmentation became a popular research topic and well developed in the past two decades. Nowadays there are many Chinese segmentation technologies and tools available and some of them could achieve as high as 99% precision rate [8]. But there are still some limitations in this field that make Chinese word segmentation imperfect, such as there is no clear definition of “word” itself in Chinese language – same pair or group of Chinese characters was recognized as a word by some people but may not be regarded as a word by some other people (e.g. “一只/det” or “一/num只/n” in “一只老虎” ); Besides that, the ambiguity (e.g. “才/ad 能/v” and “才能/n” when segmenting the sentence“只有这样才能成功”) and new words which are not recorded in dictionary also restrict the performance of Chinese word segmentation [8].</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/2-chinese-seg.png" title="An Example of Chinese Segmentation"><br></div></div>

<p>The existing unsupervised Chinese word segmentation could be classified into three classes in general.</p>
<p>(i) The first kind of segmentation is based on Maximum Matching method (short as MM method), the final output is the alternative with maximized tokenization result, in another word, is the alternative has maximum number of words be segmented. This method could be further divided into Forward Maximum Matching (FMM), Backward Maximum Matching (BMM) and combinatory of FMM and BMM (choose the optimal result from both FMM and BMM). The advantage of this method is simple, fast and fair quality of the output result [9].</p>
<p>However, this segmentation is not good enough for some cases that some sequences of Chinese characters wrongly recognized as words with high probability. E.g. “结婚的和尚未结婚的” will always be segmented as “结婚/的/和尚/未/结婚/的” rather than the correct one “结婚/的/和/尚未/结婚/的” because “和尚/n – monk” has more occurring frequency and higher probability than the word “尚未/ad – not yet” in some dictionaries, in another sentence, P(结婚)×P(的)×P(和)×P(尚未)×P(结婚)×P(的) &lt; P(结婚)×P(的)×P(和尚)×P(未)×P(结婚)×P(的).</p>
<p>(ii) There is another method named Statistical Language Models (SLM). The SLM segments Chinese text by considering the probability of specific combination of Chinese characters instead of just referring to the dictionary, and this model was also called N-gram, which is introduced in previous section. The N-gram is an important statistical language model and the most widely used is bi-gram. Most of well-known Chinese sentence segmentation tools are developed based on this bi-gram model, such as the free distribution of NLPIR 2013. In these models, the frequency of each word is determined by words ahead. The SLM method needs large enough corpus support and requires higher performance of the machine. The benefit of this method is high precision especially for higher level of N-gram model [9].</p>
<p>(iii) The last one method is also the latest one, is called Automatic Chinese Word Segmentation based on Artificial Intelligence. The keywords of this approach are Neural Networks algorithm and Expert System algorithm. By adoption of artificial intelligence theory, the segmentation program is able to learn and discover new words or potential combination of Chinese characters automatically after accepting enough training. This method could achieve highest precision ever but complex in logic and algorithm design. But under the rapid development of high performance computing and artificial intelligence, it will be the mainstream of Chinese segmentation research in the future [9].</p>
<h2 id="Active-Learning-for-NLP"><a href="#Active-Learning-for-NLP" class="headerlink" title="Active Learning for NLP "></a>Active Learning for NLP </h2><p>Machine Learning is a collection of artificial intelligence methods requires supervised data to learn a concept, and then provides the execution result with higher efficiency and accuracy. However, labelling data by hand-annotation is expensive, time consuming, tedious, mistake prone and hard to adapt for new purposes. Therefore, people has looked at a more compromise alternative that using semi-supervised and unsupervised learning techniques for the purpose of obtaining experimental result with high quality and with acceptable cost as well.</p>
<p>An active learning problem contains a classifier, a small set of labelled samples and a large set of unlabelled data. Firstly, the classifier is trained on the labelled samples under the supervision to learn the concept and form a rule to guide itself for further classifying. The active learning methods are widely used in two types of problems in natural language processing field, which are classification task like text classification, and structured prediction task such as parsing and name entity recognition. Based on Arora and Agarwal’s research, the application of active learning technique in protein names recognition could achieve the same F-score as unsupervised method with 39% less data samples [10].</p>
<p>In this project, the active learning technique was supposed to play a significant role in new word discovery with higher accuracy and efficiency.</p>
<h2 id="The-NLPIR"><a href="#The-NLPIR" class="headerlink" title="The NLPIR"></a>The NLPIR</h2><div align="center"><br><div style="width:35%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/3-hhmm-chinese-lex.png"><br></div></div>

<p>The NLPIR is short writing of the Natural Language Processing &amp; Information Retrieval Sharing Platform developed by Big Data Search and Mining Lab at Beijing Institute of Technology. Its Chinese word segmentation function was formerly known as ICTCLAS (Institute of Computing Technology, Chinese Lexical Analysis System), a powerful segmentation tools based on Hierarchical Hidden Markov model (HHMM) and it used to win the first place in SIGHAN competition in 2003. The latest stable version of NLPIR till the beginning of this project was NLPIR 2013 and the API package is freely distributed to the public.</p>
<p>The Hidden Markov Models (HMM) is a statistical Markov model belongs to the second class of segmentation method and it is widely used in for modelling stochastic processes and sequences in applications like text analysis or speech recognition [11]. To simplify, the HMM was probability notation in considering both transition probabilities and observation likelihoods. As one of HHMM-based Chinese lexical analysis tools, the NLPIR Chinese word segmentation work is done in five levels: atom segmentation, simple &amp; recursive unknown words recognition, class-based segmentation and POS tagging [12]. The first level HMM will be introduced with more details of the algorithm and mechanism of the NLPIR segmentation.</p>
<p>The formula to calculate the word segmentation is listed below:</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-5.png"><br></div></div>

<p>Some examples and figures in later sub-sections are citied from an academic blog:\ <em><a href="http://blog.csdn.net/sinboy" target="_blank" rel="noopener">http://blog.csdn.net/sinboy</a></em></p>
<h3 id="Atom-Segmentation"><a href="#Atom-Segmentation" class="headerlink" title="Atom Segmentation"></a>Atom Segmentation</h3><p>The atom segmentation is the first step during the Chinese sentence segmentation. A sentence will be tokenized at the beginning, and then be segmented into the minimum un-dividable unit as single Chinese character. This unit to present the minimum unit is called atom. Each atom has three attributes: the atom word; the POS tag annotation (“nPOS”, 1 as beginning mark, 4 as ending mark, and all other atom are labelled 0 since there are just an atom unit rather than a word) and the atom length. After atom segmentation, a sentence is supposed to be made up with a sequence of atoms that being filled with single Chinese character or a sequence of the longest consecutive non-Chinese characters like punctuation, digit or English words [13].</p>
<p>The figure below shows an example how a simple Chinese sentence being segmented and stored during atom segmentation.</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/4-atom-seg.png" title="An Example Showing Atom Segmentation"><br></div></div>

<p>According to the official description of the NLPIR, it is the one of the Chinese word segmentation tools with the highest performance and best segmentation result even it is still not perfect. Moreover, the NLPIR provides an API to the public freely for non-commercial purpose. That is why the project will use NLPIR 2013 as the research object and do further improvement if possible.</p>
<h3 id="Rough-Segmentation"><a href="#Rough-Segmentation" class="headerlink" title="Rough Segmentation"></a>Rough Segmentation</h3><p>After obtaining a sequence of atom units, the first round rough segmentation will be conducted next. During this round of segmentation, all the possible segmentation alternatives for the given sentence will be listed out and there are two iterations to realize the algorithm. In the first iteration, each atom is being traversed and searching all the possible words formed by it and other atoms connected in the dictionary. This search could be done in the second iteration by traverse all the atoms after the base atom [14].</p>
<p> Suppose a sentence A0A1…AnAn+1…Am is given, the pseudo code algorithm to do the rough segmentation could be like:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;m; i++)&#123;</span><br><span class="line">	String s=A[i];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>; j&lt;m; j++)&#123;</span><br><span class="line">		s=s+A[j];</span><br><span class="line">		<span class="keyword">if</span>(s is a word)&#123;</span><br><span class="line">			add s into the list of the words;</span><br><span class="line">			record the POS deteail;</span><br><span class="line">			record the position and length of s;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>And a sentence “他说的确实在理” after this rough segmentation would become a words’ array chain with POS information, position and length saved into it.</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/5-example-rough-seg.png" title="An Example showing the Result of Rough Segmentation"><br></div></div>

<h3 id="N-Shortest-Path"><a href="#N-Shortest-Path" class="headerlink" title="N-Shortest Path"></a>N-Shortest Path</h3><p>After the rough segmentation, an array chain of all the possible words within the given sentence is acquired and next job is to form up one or more optimal paths linked all the candidate words from the beginning of the sentence to the end without any duplicate of single atom. The criteria of possibility of a specific words’ sequence, in another word the weight value, is determined by the weight of each atom and cohesion between each other. Here the idea of N-gram in HMM language model mentioned before is applied to calculate the weight value in this round of segmentation, the NLPIR call this algorithm N-Shortest Path [15]. The optimal path is the path with the minimum over weight by multiple all weights of candidate transitions together. The equation below is the simulation of the N-Shortest Path where N=2:</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-6.png"><br></div></div>

<p>After calculating all the weight of candidate word pairs, the sentence in current stage could be expressed as a table drawn in the next page.</p>
<div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/table-2.png" title="An example of N-Shortest Path in 2-D matrix"><br></div></div>

<div align="center"><br><div style="width:50%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/6-n-short.png"><br></div></div>

<p>With the visual representation of the array chain of all the candidate words’ transition list above, the weight of each candidate path could be easily calculated and in NLPIR, the optimal path is obtained by executing Dijkstra algorithm for the case N=2. Back to the sample sentence, the shortest path could be obtained by applying the method stated before. The figure at right hand side shows the procedure of how the path acquired. The final path is “始##始@他@说@的@确实@在@理@末##末” with the minimum overall weight 68798.864.</p>
<p>After the first round of rough segmentation, the following several rounds of segmentation are focused on more specific cases such as geographic name entities, name entities and further optimization for final result. These following steps are realized by applying some additionally NLP related techniques. However, we won’t be discussed the details of other steps here because essentially they share the same idea in the segmentation. </p>
<h1 id="Project"><a href="#Project" class="headerlink" title="Project "></a>Project </h1><p>This Project Chapter introduces the details of this project which is implemented on the basis of NLPIR 2013 and could be separated into three main parts, the resources or materials to be used in this project; the logic and algorithm to realize the goal and the related documentation for this project.</p>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><h3 id="Hardware-Resources"><a href="#Hardware-Resources" class="headerlink" title="Hardware Resources"></a>Hardware Resources</h3><p>In consideration of this project needs lot of file I/O and text searching works, this project was implemented and tested on two computers with difference hardware configurations simultaneously to ensure the compatibility and performance of the project. </p>
<p><em>Main Implementation &amp; Testing Environment:</em></p>
<table>
<thead>
<tr>
<th><strong>Configuration</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Machine Model</td>
<td>Apple® MacBook Pro 15 (late 2011)</td>
</tr>
<tr>
<td>Operating System</td>
<td>Mac OS X Mavericks *(10.9.1 Build 13B42)</td>
</tr>
<tr>
<td>Processor</td>
<td>Intel® Core™ i7-2720QM Processor, 2.2GHz (up to 3.30 GHz)</td>
</tr>
<tr>
<td>Storage</td>
<td>250GB SSD</td>
</tr>
<tr>
<td>Memory</td>
<td>8GB DDR3</td>
</tr>
<tr>
<td>Software</td>
<td>eclipse (Standard 4.3.1 for Mac OS X)</td>
</tr>
<tr>
<td>Java Version</td>
<td>Java™ 7 pre-installed in Mac OS X</td>
</tr>
</tbody>
</table>
<p><em>Alternative Testing Environment:</em></p>
<table>
<thead>
<tr>
<th><strong>Configuration</strong></th>
<th><strong>Details</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Machine Model</td>
<td>Lenovo® ThinkPad X61t (late 2007)</td>
</tr>
<tr>
<td>Operating System</td>
<td>Microsoft® Windows 7 Ultimate (Version 6.1, Build 7600)</td>
</tr>
<tr>
<td>Processor</td>
<td>Intel® Core™ 2 Duo L7500 Processor, 1.60GHz</td>
</tr>
<tr>
<td>Memory</td>
<td>2GB DDR2</td>
</tr>
<tr>
<td>Storage</td>
<td>500GB HDD, 7200 rpm</td>
</tr>
<tr>
<td>Software</td>
<td>eclipse (Kepler Release, Build id 20130614-0229)</td>
</tr>
<tr>
<td>Java Version</td>
<td>Java™ Standard Edition (Version 7 Update 45 build 1.7.0_45-b18)</td>
</tr>
</tbody>
</table>
<h3 id="Software-and-Project"><a href="#Software-and-Project" class="headerlink" title="Software and Project"></a>Software and Project</h3><p>The Software used in this project were two integrated development environment (IDE) software. The first one is the eclipse and JDK. The whole project was a Java project and it was fully set up on this IDE. The other software used in this project is JetBrains PyCharm, which is an IDE of Python, to run the Python codes written by student in purpose of extracting content of weibo post from JSON files.</p>
<p>This project was further implemented onto the latest NLPIR API downloaded from its official website: <a href="http://ictclas.nlpir.org" target="_blank" rel="noopener">http://ictclas.nlpir.org</a>. The API provided two library files “NLPIR.dll” and “NLPIR_JNI.dll”; some data files like dictionaries and a Java class listing all callable methods for NLPIR 2013.</p>
<p>The NLPIR project read string in UTF-8 format as input, and output the segmentation result in UTF-8 string, too. The output has two types, segmentation without POS tags and segmentation with POS tags. The NLPIR enables user to choose if there are POS tags in output string by changing a parameter’s value in a method calling like that: </p>
<p>Demo Code:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">NLPIR testNLPIR = <span class="keyword">new</span> NLPIR(); </span><br><span class="line">String input = “第一次和家人一起装饰圣诞树很开心。”;</span><br><span class="line"><span class="keyword">byte</span> nativeBytesWithoutTag[] = testNLPIR.NLPIR_ParagraphProcess(</span><br><span class="line">input.getBytes(<span class="string">"UTF-8"</span>), <span class="number">0</span>);</span><br><span class="line">String segmentOutput = <span class="keyword">new</span> String(nativeBytesWithoutTag, <span class="number">0</span>, </span><br><span class="line">nativeBytesWithoutTag.length, <span class="string">"UTF-8"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">byte</span> nativeBytesWithTag[] = testNLPIR.NLPIR_ParagraphProcess(</span><br><span class="line">input.getBytes(<span class="string">"UTF-8"</span>), <span class="number">1</span>);</span><br><span class="line">String segmentOutputWithTags = <span class="keyword">new</span> String(nativeBytesWithTag, <span class="number">0</span>, </span><br><span class="line">nativeBytesWithTag.length, <span class="string">"UTF-8"</span>);</span><br></pre></td></tr></table></figure>
<p>Result:<br>  segmentOutput:<br>    第一 次 和 家人 一起 装饰 圣诞树 很 开心 。<br>  segmentOutputWithTags<br>    第一/m 次/qv 和/cc 家人/n 一起/s 装饰/vn 圣诞树/n  很/d 开心/a 。/wd</p>
<p>Besides the segmentation, the NLPIR also enables user to add or delete customized vocabulary into the dictionary, and this is the reason that NLPIR was chosen in this project to compare the segmentation result between old dictionary and new dictionary with new words, and then calculate how much improvement could be made.</p>
<p>The operation to add/delete vocabulary into the dictionary could be achieved by executing the code below: </p>
<p>For a New Word: 微博/pro</p>
<p>Add Operation:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">NLPIR testNLPIR = <span class="keyword">new</span> NLPIR(); </span><br><span class="line">newWordWithTag =  <span class="string">"微博"</span> + <span class="string">" "</span> + <span class="string">"pro"</span>;</span><br><span class="line"><span class="keyword">byte</span>[] newWord = newWordTag.getBytes();</span><br><span class="line">testNLPIR.NLPIR_AddUserWord(newWord);</span><br><span class="line"></span><br><span class="line">Delete Operation:</span><br><span class="line">testNLPIR.NLPIR_DelUsrWord(newWordWithTag);</span><br></pre></td></tr></table></figure>
<h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The project was to implement an efficient solution for Chinese sentence segmentation onto informal text that was widely used in social networks. Therefore the text extracted from weibo, the best-known Chinese social networks nowadays and it was described as Chinese version twitter, was regarded as the perfect data to be used into this project. Besides that, in consideration of the real-time feature of the invention of new vocabulary on Internet, the mass weibo text sorted in chronological order in a given time period was required to simulate this real-time procedure and test the efficiency of the project.</p>
<p>The dataset used in this project was the historical weibo text of 2695 randomly chosen users until the 25^th^ December 2013. There are approximately 0.7 million weibo posts contained into 1.95GB uncompressed JSON files.</p>
<p>Here is the detailed information of the dataset:</p>
<div align="center"><br><div style="width:90%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/table-3.png" title="A Sample of Dataset"><br></div></div>

<h3 id="Test-Data"><a href="#Test-Data" class="headerlink" title="Test Data"></a>Test Data</h3><p>In order to test the performance of the implementation completely, three categories including both informal user generated text extracted from weibo and other kinds of more formal text found from Internet were adopted as the text data set. These three categories include:</p>
<p>(i) Formal Category</p>
<p>Since the topic of this project is Chinese Segmentation in user generated content, the informal user generated data is essential in testing the performance of this project. A text file contains 1500 weibo posts that had not been used in new word discovery were selected as the test data from informal category.</p>
<p>(ii) Semi-Formal Category</p>
<p>Nowadays, most of articles on the Internet were written in popular Internet slang even they were news articles or review articles. To ensure the algorithms implemented in this project could be used widely on Internet, the semi-formal test data was added. In this category, one review article and one news article were randomly selected.</p>
<p>(iii) Informal Category</p>
<p>Even through the existing Chinese segmentation product could segment formal text in pretty high efficiency and precision, the formal test data was still essential in the testing. The objective of selecting formal data in testing this project, which is Chinese segmentation in informal content, was to ensure the performance of algorithms implemented in this project could still maintain a high level in segmenting formal text, and improve the efficiency in informal text segmentation simultaneously. Here the Annual Government Report of China in 2014, which could be the most formal Chinese language in the world, was selected in this category.</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology "></a>Methodology </h2><p>To realize the goal of this project, which is more accurate Chinese segmentation in user generated content such as weibo, the student has to come up with a method to make the segmentation tool recognized as much words in given sentences as possible. The essential of the solution to this problem is the new words discovery and recognition.</p>
<p>The “new words” is a broad concept that includes latest vocabulary invented and other un-recognized words like short writing, informal writing with homophone or homograph. Most of these new words are invented and spread widely in the informal language on the Internet, for example in some forum, or social networks like weibo.</p>
<p>There is very common scenario that the sentences chosen from news article could be segmented by existing Chinese segmentation tool very well and sometimes the precision of the segmentation could reach almost 100%. But when these segmentation tools being tested by some posts extracted from weibo, the result is unsatisfying. This is because the dictionary is always limited and incomplete when facing to the user generated content on the Internet even it updates frequently. To make sure the “new words” in user generated content could be recognized automatically, a new words discovery program is needed to be implemented based on the existing Chinese segmentation tool, analysing and processing segmentation result from those segmentation tool, then telling the new words to the user.</p>
<p>Here is a sample sentence showing the difference between segmentation result output by NLPIR 2013 and the expected segmentation result after processing by this project.</p>
<div align="center"><br><div style="width:55%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/7-nlpir-seg.png"><br></div></div>

<h3 id="New-Words-Extraction-from-NLPIR-Segmentation-Result"><a href="#New-Words-Extraction-from-NLPIR-Segmentation-Result" class="headerlink" title="New Words Extraction from NLPIR Segmentation Result"></a>New Words Extraction from NLPIR Segmentation Result</h3><p>To find the hidden “new word” from the NLPIR segmentation result, a definition about what kind of word can be assumed as “new word” should be given before the algorithm design phrase. After reading and analysing plenty of the segmentation output, the student predicated that the invention of new words could be tracked by the increasing usage and frequency occurred on the Internet in a specific period. For example, after the case of a two-year-old baby’s death after a motorist throwing on the ground, a special word “摔婴”, which literally means “throwing baby”, emerged on the Internet and widely known among Internet user.</p>
<p>Therefore an experiment was carried out to test whether new words could be found by observing its frequency, in another word, whether we can differentiate “new words” or non-word atom sequence according to their occurring frequency.</p>
<p>Assumption: all new words are constructed by <em>two</em> atoms.</p>
<p><strong>Step 1</strong>: Randomly choose five consecutive text files contains weibo posts.</p>
<p>(e.g. 10.txt, 11.txt, 12.txt, 13.txt, 14.txt five files were chosen)</p>
<p><strong>Step 2</strong>: Find all two-atom-combinations which occurred in all these five files</p>
<p><strong>Step 3</strong>: Count their occurrence in all 5 files and analysis the relevance between the tendency of occurrence frequency and its possibility of being a “new word”.</p>
<p>w1, w2, w3, w4, w5 are the list of atoms<br>    segmented from file1, file2, file3, file4 and file5</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; w1.length – <span class="number">1</span>; i++) &#123;</span><br><span class="line">word = w1[i]+w1[i+<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">if</span> (w2.contains(word) &amp;&amp; w3.contains(word) </span><br><span class="line">			&amp;&amp; w4.contains(word) &amp;&amp; w5.contains(word)) &#123;</span><br><span class="line">		print i and its occurrence frequency in all <span class="number">5</span> files</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Here is the figure showing the count of the occurrence in all 5 files for some of candidate “new words”:</p>
<div align="center"><br><div style="width:85%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/8-occurrence-freq.png"><br></div></div>

<p>It is not difficult to see the distribution of the occurrence frequency of some chosen candidate new words in consecutive five files. If these 10 candidate new words were separated into two groups: one group contains those candidates used to have great fluctuation on the figure, the other group contains all the other candidates:</p>
<p>Table 4 an Example of the<br>candidates found from 5 consecutive files</p>
<table>
<thead>
<tr>
<th><strong>Group</strong></th>
<th><strong>Candidate</strong></th>
<th><strong>f1</strong></th>
<th><strong>f2</strong></th>
<th><strong>f3</strong></th>
<th><strong>f4</strong></th>
<th><strong>f5</strong></th>
<th><strong>True Vocabulary</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>A</strong></td>
<td><strong>短/a-信/n</strong></td>
<td>6</td>
<td>10</td>
<td>22</td>
<td>4</td>
<td>11</td>
<td>Yes</td>
</tr>
<tr>
<td></td>
<td><strong>吐/v-槽/ng</strong></td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>4</td>
<td>8</td>
<td>Yes</td>
</tr>
<tr>
<td></td>
<td><strong>更/d-好/a</strong></td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>16</td>
<td>2</td>
<td>Semi</td>
</tr>
<tr>
<td></td>
<td><strong>推/v-送/v</strong></td>
<td>4</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>14</td>
<td>Yes</td>
</tr>
<tr>
<td></td>
<td><strong>截/v-图/n</strong></td>
<td>2</td>
<td>1</td>
<td>9</td>
<td>1</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td></td>
<td><strong>一/m-天/qt</strong></td>
<td>4</td>
<td>7</td>
<td>17</td>
<td>9</td>
<td>12</td>
<td>No</td>
</tr>
<tr>
<td><strong>B</strong></td>
<td><strong>是/vshi-吧/y</strong></td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>No</td>
</tr>
<tr>
<td></td>
<td><strong>妹/n-纸/n</strong></td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
<td>Yes</td>
</tr>
<tr>
<td></td>
<td><strong>去/vf-的/ude1</strong></td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>No</td>
</tr>
<tr>
<td></td>
<td><strong>一/m-种/q</strong></td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>3</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>If we were looking for new words from these two groups, we can find that the accuracy in group A, those candidates has obvious fluctuation, is 66.67% or 83.33% if we count “更好” as one word; whereas, the group B could only 25.00%, lower to the group A obviously. And the most important thing is, when more candidates were compared, or larger size of data was involved into this experiment, the outcome is stronger to prove that those candidates with greater fluctuations in frequency have higher probabilities to be a true vocabulary, or a “true new word”.</p>
<p>The explanation from the student for this phenomenon is that for the invention of every true new word, it must experience a process that being used by more and more user, or occurring on the Internet more and more frequently. And this process became the great slope leading to higher value in the graph during some periods in history. That means, the fluctuation in the graph is the necessary condition of the behaviour of a legal new word.</p>
<p>This experiment provided a guideline to find the candidate of new words. Therefore all the candidate words in the project were extracted from mass weibo text by this method. Since according to relevant study in Chinese language, 99% Chinese vocabulary has a length less than five characters – that means the new word discovery only considering those candidates with length between 2 to 4 characters is fair enough.</p>
<p>here is only one possibility to find a candidate new word in length of 2 characters, which is the candidate formed by 2 consecutive atoms with length of 1 Chinese character. For those candidates in length of 3 and 4, there are multiple possibilities of the form of those candidates and the new word discovery need to consider all of these possibilities in theory. </p>
<p><span id="_Toc383415740" class="anchor"></span>Table 5 The Possible<br>Combination of New Words in Different Length</p>
<table>
<thead>
<tr>
<th><strong>Length</strong></th>
<th><strong>Form</strong></th>
<th><strong>Example （Chinese Char）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>A+B</td>
<td>微+博</td>
</tr>
<tr>
<td>3</td>
<td>A+B+C, AA+B, A+BB</td>
<td>喵+星+人，东北+银</td>
</tr>
<tr>
<td>4</td>
<td>A+B+C+D, A+BB+C, A+B+CC, A+BBB, AA+B+C, AA+BB, AAA+B</td>
<td>喜+大+普+奔，累+觉+不+爱，顶+礼+膜拜</td>
</tr>
</tbody>
</table>
<p>Not all the combination of atoms in the forms listed in the table above could be assumed as candidates. The criteria to judge one combination of atoms is a candidate is designed based on the rule found during the experiment. When the program looking for the combination of atoms, the program counting its occurrence frequency, too. Once there is evidence showing one sequence of atoms that not only following the construction listed in the table above, but also occurred in a number of consecutive files with increasing in frequency (the up slope in the graph), this sequence of atoms is a candidate new word.<br>The pseudo-code of this process could be expressed as:</p>
<p>Assumption:<br>number of consecutive files = 3,<br>only the sequence of atoms has count of occurrence shows 2x increasing when compared to the previous file is considered: the multiplier k = 2,<br>All new words has length of 2.</p>
<p>given w1, w2 and w3 as the list of atoms from file1, file2 and file3<br>given k = 2</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; w1.length – <span class="number">1</span>; i++) &#123;</span><br><span class="line">	word = w1[i]+w1[i+<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">if</span> (w2.contains(word) &amp;&amp; w3.contains(word)) &#123;</span><br><span class="line">		<span class="keyword">if</span>(count(word, w2)&gt;=k*count(word, w1) </span><br><span class="line">				&amp;&amp; count(word, w3)&gt;=k*count(word, w2)) &#123;</span><br><span class="line">			add word into the candidateNewWord list</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Qualification-of-New-Words-from-Candidates"><a href="#Qualification-of-New-Words-from-Candidates" class="headerlink" title="Qualification of New Words from Candidates"></a>Qualification of New Words from Candidates</h3><p>In this subsection, four different methods/algorithms are introduced to pick up and certify the candidates we got from last step. Each of these four methods/algorithms will be introduced in detail and the analysis on the outcome will be expressed into the data and charts obtained from the experiment.</p>
<h4 id="1-Unsupervised-Unconditional-New-Word-Discovery"><a href="#1-Unsupervised-Unconditional-New-Word-Discovery" class="headerlink" title="(1) Unsupervised Unconditional New Word Discovery"></a>(1) Unsupervised Unconditional New Word Discovery</h4><p>As the name of this method suggests, this method doesn’t do any selection or screening work onto the candidate new words. Every candidate new words extracted from the given numbers of weibo text will be added into the user customized dictionary and used into further segmentation. This method seems feasible especially when both the number of consecutive files and the multiplier k are set to big enough in value. However, the actual experiment result denied this hypothesis.</p>
<p>Firstly, an experience was conducted to find the n and k, which are the number of the files in each new word discovery and the multiplier of frequency to identify the new word in n consecutive files. The controlling variables method was applied in this experiment on both n and k to provide a suggestion in choosing the optimum value of these two variables in following part of project.</p>
<div align="center"><br><div style="width:55%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/9-num-candidate.png" title="The Number of Candidate with Different Combination of k and n"><br></div></div>

<p>In figure 9, we found the number of candidate new words was varied with different multiplier of frequency in n consecutive files. And the number of candidate new word when n=3 is the best instance which reflected the continuity of the process of new words’ invention, and at the same time kept the number of candidate new words founds in range from 1 to 150 with different <em>k</em> value, those are some values neither too big to do the manually statistics nor too small to satisfy the fairness of the final conclusion in this project.</p>
<div align="center"><br><div style="width:55%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/10-num-candidate.png" title="The Number of Candidate New Words with Different Value k when n=3"><br></div></div>

<p>After the confirmation of assigning 3 to the variable n, now let’s focus on the proper k value to use in following experiments. There are curves of 8 combinations of 3 consecutive files filled with weibo posts in timely order on the plot. It is not difficult to find that when there are too many candidate new words found when given k=1, however, when k is assigned to some other value greater than 2, the number of candidate new words seems too small because of too strict condition. For example, k=3 means a combination of segmented atoms could be assumed as a candidate new word only its occurrence frequency triples each time. In consideration of both reasonability and proper number of candidate new word found, k=2 is chosen in the end.</p>
<p>The unsupervised unconditional new word discovery was implemented after the confirmation of assignment of all key variables. The term “unsupervised” means no more interference from user will be imposed on the new word discovery, the project was executed totally by itself And the term “unconditional” means all the candidate new words was supposed to be true new word, or new invented vocabulary without being filtered with any condition.</p>
<p>The pseudo code of unsupervised unconditional new words discovery was given below:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">1</span>;</span><br><span class="line">ArrayList&lt;String&gt; newWordList = extractNewWordsFrom3Files(</span><br><span class="line">					candidateNewWords(s1),</span><br><span class="line">					candidateNewWords(s2),</span><br><span class="line">					candidateNewWords(s3));</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">candidateNewWords</span><span class="params">(String s)</span>:</span></span><br><span class="line"><span class="function">	String[] atoms </span>= s.split(“ ”);</span><br><span class="line">	ArrayList&lt;String&gt; candidates = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> count = <span class="number">0</span>; count&lt;atoms.length-<span class="number">1</span>; count++)&#123;</span><br><span class="line"><span class="keyword">if</span>((atoms[count].length()==<span class="number">1</span>) &amp;&amp; (atoms[count+<span class="number">1</span>].length()==<span class="number">1</span>))</span><br><span class="line">candidates.add(atoms[count]+     atoms[count+<span class="number">1</span>]);</span><br><span class="line">	&#125;</span><br><span class="line"><span class="keyword">return</span> candidates;</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">extractNewWordsFrom3Files</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		ArrayList&lt;String&gt; al1, ArrayList&lt;String&gt; al2, ArrayList&lt;String&gt; al3)</span></span></span><br><span class="line"><span class="function">	ArrayList&lt;String&gt; newWords </span>= <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">String candidate = “”;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;atoms.length-<span class="number">1</span>; i++)&#123;</span><br><span class="line">candidate = al1.get(i);</span><br><span class="line"><span class="keyword">if</span>((al2.contains(candidate) &amp;&amp; (al2.contains(candidate)) &amp;&amp;</span><br><span class="line">	(al2.count(candidate)/al1.count(candidate)&gt;=k) &amp;&amp; (al2.count(candidate)/al1.count(candidate)&gt;=k))</span><br><span class="line">			newWords.add(al1.get(i));</span><br><span class="line">	&#125;</span><br><span class="line"><span class="keyword">return</span> newWords;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Input:<br>s1: “过去 一 年 ， 我 每 天 都 会 使用 微 博 。”<br>s2: “微 博 现在 非常 流行 ， 我 每 天 都 用 微 博 ， 我 很 喜欢 。”<br>s3: “我 每 天 都 去 学校 ，我 常常 在 路 上 刷 微 博 ， 发 微 博 。”</p>
</blockquote>
<p>Output (only the words typed in underline bold style are true new words):<br>newWordList: “，我” “我每” “每天” “天都” “微博”</p>
<p>As results showed above, this unsupervised unconditional could found all potential new vocabulary from Internet in theory, but the problem it brought to us was there were too many useless candidate new words like the combination of atoms containing punctuations also found and mixed with those real new words, which imposed too much workload to choose real new words from the result manually. Obviously, this is not a good solution to discover new words.</p>
<h4 id="2-Unsupervised-Conditional-New-Words-Discovery"><a href="#2-Unsupervised-Conditional-New-Words-Discovery" class="headerlink" title="(2) Unsupervised Conditional New Words Discovery"></a>(2) Unsupervised Conditional New Words Discovery</h4><p>Since the result of new words discovery was not satisfying in unsupervised unconditional method, some improvement was urgent to be made to realize the goal of this project, which is a more efficient way to find the new words in user generated content.</p>
<p>By observing the result of the first method carefully, it was easy to find there are some of these candidate new words are obviously impossible to be one vocabulary or a new word. In another way, we could make a hypothesis that there is a rule to judge whether a combination of segmented atoms is a new words or not. And this rule was based on the POS tags of each segmented atoms, which is the properties of those atoms in the usage of Chinese.</p>
<p>After analysing the result from last method, some specific combinations of segmented atoms could be determined that they were mostly impossible to be qualified as a new word, especially a word made up with 2 characters. They included:<br>(a) Candidates new words contains punctuation(s) e.g. “，我”</p>
<p>(b) Candidates new words contains digits(s) e.g. “1个”</p>
<p>Besides that, based on the statistical work conducted for the known new words and analysis on Chinese words’ morphology, a rule can be summarized that most of new words were invented to describe a new phenomenon or new action. That means most of the new words created on Internet were nouns, adjectives or verbs. For those kinds of words made up with two Chinese characters, their combinations of POS tags mainly belong to one of those listed below:</p>
<p><span id="_Toc383415741" class="anchor"></span>Table 6 Some Common Combination of New Word</p>
<table>
<thead>
<tr>
<th><strong>Adjective/Noun</strong></th>
<th><strong>Verb</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Noun + Noun, e.g. 砖家/n</td>
<td>Verb + Verb, e.g. 签售/v</td>
</tr>
<tr>
<td>Adjective + Adjective, e.g. 高大/a</td>
<td>Verb + Adverb, e.g. 走起/v</td>
</tr>
<tr>
<td>Adjective + Noun, e.g. 微信/n</td>
<td>Verb + Noun, e.g. 翻墙/v</td>
</tr>
<tr>
<td></td>
<td>Adverb + Verb, e.g. 猛戳/v</td>
</tr>
</tbody>
</table>
<p>Therefore, two filters were implemented onto the code of the first method to improve the precision of the new words discovery. The first filter (filter A) was designed to filter out all combinations of atoms containing punctuations and digits before adding them into the list of candidate new words. Then the second filter (filter B) restricted the condition to determine whether a candidate new word is new word or not. In this filter, only the candidates satisfying the seven combinations of POS tags listed above would be considered and added into the new words’ list in the end.</p>
<p>The reason of implementing 2 filters rather than using the 2nd one only was the POS tags in NLPIR were varies in different sentence or different words. But the POS tagging operation still has some rules to track with:</p>
<p>All the POS tags of punctuation symbols are starting with a “w”. For example the period in Chinese, “。”, is “wd” and the comma “，” is “wj”.</p>
<p>All verbs’ tags must have a letter “v”, all adverbs’ tags must have “d” , all noun/pronouns’ tags must have a letter “n” and all adjectives must have “a” in their tags.</p>
<p>To prevent the period was recognized as an adverb, double filter method was applied to provide a safe way to proceed with the conditional new words discovery. This implementation was summarized as unsupervised conditional new words discovery, it removed out all two consecutive atoms contains punctuation and digits before picking up those belong to specific POS tags combination.</p>
<div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/11-procedures.png" title="The Procedures of Unsupervised Conditional Algorithm"><br></div></div>

<p>The figure above showed the procedures and specific result of the unsupervised conditional new words discovery. It proved that the unsupervised conditional new words discovery could greatly decrease the number of qualified new words in the end. But the usage of filter was not perfect, sometimes even the true new words could not be qualified by the filter because of too strict and limited fileting condition in POS tags to recognize all new words. For instance, the atom combination “给/prep” and “力/noun” (给力/adj - awesome), and some un-recognized words marked as “nr” like the atom combination “尼/b” and “玛/nr” (尼玛/int - interjection), they are true new words and widely used on Internet but failed to be found in current new word discovery algorithm because their POS tags are too rare to be added into the filter.</p>
<h4 id="3-Active-Learning-New-Words-Discovery"><a href="#3-Active-Learning-New-Words-Discovery" class="headerlink" title="(3) Active-Learning New Words Discovery"></a>(3) Active-Learning New Words Discovery</h4><p>We can image that most of regular new words could be found by unsupervised new words discovery when as complete as possible conditions are given. But for some new words composed by the character with rare or even un-recognized POS tags, they seem impossible to pass the filter and then founded by unsupervised new word discovery. Now we need to move back to the essential feature of our definition of “new words”. Before designing the algorithm, several experiments was carried out to find proper value of the variable n and k, and then we made an assumption in this project that any consecutive characters had k times exponential growth in frequency among minimum n consecutive time period could be defined as a new word invented on Internet. That means all candidate new words generated actually hold same possibility to be a valid new word, or a true vocabulary. Therefore, an active-learning algorithm was designed to release the power that determines whether a candidate is a truly new word or not to the user, and the program only focused on learning from user’s input and summarizing them into a collection of rules to use into further new words discovery.</p>
<p>Firstly, instead of only recording the details of two consecutive atoms, the active learning new word discovery saved both two atoms themselves and two adjacent atoms. And each time a pair of atoms and their POS tags showed up, the user could also check the POS tag of their previous atom and their next atom. These previous tag and next tag was called transition record upon a candidate new word.</p>
<div align="center"><br><div style="width:60%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/12-difference.png" title="Difference between Unsupervised & Active-Learning Algorithm in New Words Discovery"><br></div></div>

<p>Besides that, not only those candidate new words that satisfying k times exponential increasing in frequency would be saved, another list of atoms’ pair was set up to save those candidates failed to satisfying k times exponential rule but existing in consecutive n time periods. The first list with regular candidate new words was renamed to candidates with higher frequency, or “HF”, at this time, and the second list was named candidates with lower frequency, shortly recorded as “LF”.</p>
<p>During the execution, the program would keep querying those candidate new words from “HF”. The content of query included whether it was a word or not and the POS tag of this new word if yes. Once user gave positive response and provided the POS tag, the program would search all others candidates from “HF” and added those candidate new words, who shared same POS tags for both transition record and two atoms’ themselves, together with the candidate queried to the user, into the qualified new words list. Because the candidates who shared same transition states and POS tags with a known true new word, were believed to be true new words also. And each time user response to query to a candidate new word, this candidate new word and all candidates automatically qualified and added into the new words list would be removed from the list “HF”, those to maintain the number of query within a lower level.</p>
<p>This algorithm is an active-learning process to concede the authority to the user and machines itself would focus on the learning and further automation. And the pseudo-code of this algorithm is given:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">HashMap&lt;String, ArrayList&lt;String&gt;&gt; tagTransitionMap;</span><br><span class="line"></span><br><span class="line">ArrayList&lt;String&gt; newWordList = extractNewWordsFrom3Files(</span><br><span class="line">	candidateNewWords(s1),</span><br><span class="line">	candidateNewWords(s2),</span><br><span class="line">	candidateNewWords(s3));</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">candidateNewWords</span><span class="params">(String s)</span>:</span></span><br><span class="line"><span class="function">    String[] atoms </span>= s.split(“ ”);</span><br><span class="line">    ArrayList&lt;String&gt; candidates = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> count = <span class="number">0</span>; count&lt;atoms.length-<span class="number">1</span>; count++)&#123;</span><br><span class="line">        <span class="keyword">if</span>((atoms[count].length()==<span class="number">1</span>) &amp;&amp; (atoms[count+<span class="number">1</span>].length()==<span class="number">1</span>))</span><br><span class="line">            String prev = count &gt; <span class="number">0</span> ? atoms[count-<span class="number">1</span>].split(<span class="string">"/"</span>)[<span class="number">1</span>] : <span class="string">"*"</span>;</span><br><span class="line">            String next = count &lt; atoms.length-<span class="number">2</span> ? atoms[count+<span class="number">2</span>].split(<span class="string">"/"</span>)[<span class="number">1</span>] : <span class="string">"*"</span>;</span><br><span class="line">		<span class="comment">//The “*” means it can be any tag to match</span></span><br><span class="line">            candidates.add(atoms[count] + atoms[count+<span class="number">1</span>]);</span><br><span class="line">            String transition = prev + atoms[count].split(<span class="string">"/"</span>)[<span class="number">1</span>] +</span><br><span class="line">                atoms[count+<span class="number">1</span>].split(<span class="string">"/"</span>)[<span class="number">1</span>] + next;</span><br><span class="line">            tagTransitionMap.put(transition, </span><br><span class="line">                tagTransitionMap.get(transition).add(atoms[count] + atoms[count+<span class="number">1</span>]));</span><br><span class="line">	&#125;</span><br><span class="line">    <span class="keyword">return</span> candidates;</span><br><span class="line"></span><br><span class="line"><span class="function">function <span class="title">extractNewWordsFrom3Files</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    ArrayList&lt;String&gt; al1, ArrayList&lt;String&gt; al2, ArrayList&lt;String&gt; al3)</span></span></span><br><span class="line"><span class="function">    ArrayList&lt;String&gt; newWords </span>= <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    String candidate = “”;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;atoms.length-<span class="number">1</span>; i++)&#123;</span><br><span class="line">        candidate = al1.get(i);</span><br><span class="line">        <span class="keyword">if</span>((al2.contains(candidate) &amp;&amp; (al2.contains(candidate)) &amp;&amp;</span><br><span class="line">                (al2.count(candidate)/al1.count(candidate)&gt;=k) &amp;&amp; (al2.count(candidate)/al1.count(candidate)&gt;=k)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (AskUser(candidate) != <span class="keyword">null</span>)&#123;</span><br><span class="line">                newWords.add(candidate);</span><br><span class="line">                String transition = RetrieveTransition(candidate);</span><br><span class="line">                newWords.addAll(tagTransitionMap.get(transition));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newWords;</span><br></pre></td></tr></table></figure>
<p>The algorithm recently introduced was temporarily marked as Active-Learning new Word Discovery (Type A), and there is another algorithm being named Active-Learning new Word Discovery (Type B) with a few difference with the Type A one. In algorithm Type A, only candidates in “HF” were involved and only the match of full transition (including both <em>prev</em> and <em>next</em>) candidates could be added into the new words list automatically. But in Type B, these two restrictions were eased to find more related new words:</p>
<ul>
<li><p>In Type A, only candidate new words from list “HF” would be eligible as a new word, and only full transition match will be added by machine without user’s permit.</p>
</li>
<li><p>However, in Type B, only candidates from list “HF” will be queried to user but all candidates from both “HF” and “LF” were eligible to be a truly new word. Once receiving user’s positive response onto one candidate, all candidates from “LF” than matched their full transition and all candidates from “HF” than matched any one side of transition (<em>prev</em> or <em>next</em>) would be automatically added into the true new words list.</p>
</li>
</ul>
<div align="center"><br><div style="width:40%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/13-algo-type-a.png" title="The Algorithm of Active-Learning (Type A)"><br><img src="/2014/05/20/chinese-segementation/14-algo-type-b.png" title="The Algorithm of Active-Learning (Type B)"><br></div></div>

<h1 id="Measurement"><a href="#Measurement" class="headerlink" title="Measurement"></a>Measurement</h1><p>The measurement of this project contains two main parts. The first one it to measure how good the new words discovery algorithms designed in this project could find the new words; the other one is how good the segmentation program could behaviour with new words found in this project added into the dictionary. The results of these two measurements could directly reflect the project was successful or not, and provide the proof to the evaluation of this project.</p>
<h2 id="Measurement-for-New-Words-Discovery"><a href="#Measurement-for-New-Words-Discovery" class="headerlink" title="Measurement for New Words Discovery "></a>Measurement for New Words Discovery </h2><p>The evaluation of new words discovery algorithms could be done by calculating their efficiency in finding the truly new words. For example, if there was a new word discovery algorithm A finding 100 new words but only 10 of them were true and valid new words (with efficiency 10%), and algorithm B finding only 10 new words but 8 of these 10 words were true and valid (with efficiency 80%), we cannot assert that the algorithm A had better performance than B because algorithm A contains more useless “words” and thus probably resulted in higher noises in the actual segmentation. </p>
<p>The procedure of the result statistics could be summarized as:</p>
<p>(1) Choose test data from different categories: Formal Article, Semi-formal Article and weibo Post (Informal User Generated Content), to test how much improvement could be made on different usage of Chinese segmentation.</p>
<p>(2) Run the four new words discovery algorithms separately and saved the new words found into different place.</p>
<p>(3) Manually Count the number of true words from the new word discovery result generated by each algorithm.</p>
<p>(4) Calculate the efficiency by dividing the number of true new words over the total number of words found for each of four algorithms.</p>
<h2 id="Measurement-for-Chinese-Segmentation"><a href="#Measurement-for-Chinese-Segmentation" class="headerlink" title="Measurement for Chinese Segmentation"></a>Measurement for Chinese Segmentation</h2><p>As what had mentioned in the Literature Review, the results of Chinese segmentation in this project were measured in terms of Precision, Recall and F-1 score.</p>
<div align="center"><br><div style="width:80%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/15-test-matrix.png" title="A 2×2 Contingency Table Showing 4 Outcomes in Classification Tasks"><br></div></div>

<p>The procedure of the result statistics could be summarized as:</p>
<p>(1) Run the four new words discovery algorithms separately and saved the new words found into different place.</p>
<p>(2) Choose test data from different categories: Formal Article, Semi-formal Article and weibo Post (Informal User Generated Content), to test how much improvement could be made on different usage of Chinese segmentation.</p>
<p>(3) Run the NLPIR 2013 program with original dictionary and with new words generated by four algorithms added into the customized dictionary one by one, save the segmentation result for further analysis.</p>
<p>(4) Manually Segment these test data, Record the result as the condition outcome, or the “Gold Standard” mentioned in figure 16.</p>
<p>(5) Manually Count the number of words correctly segmented, mark as “True Positive”; the number of words wrongly segmented, mark as “False Positive”; and the number of atoms failed to be segmented as a word, mark as “False Negative”.</p>
<pre><code>E.g. Original Sentence: “结婚的和尚未结婚的”
Condition / “Gold Standard”: “结婚/的/和/尚未/结婚/的”
Test Outcome: “结婚/的/和尚/未/结婚/的”
True Positive: 4 (“结婚”, “的”, “结婚”, “的”)
False Positive: 1 (“和尚”)
False Negative: 1 (“尚未”)
</code></pre><p>(6) Calculate the Precision and Recall for the segmentation result of each algorithm. The equation to calculate precision and recall were given below:</p>
<div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-7.png"><br></div></div>

<p>(7) Calculate the F-1 Score to evaluate the comprehensive performance in segmentation for each algorithm. The equation to compute the F-1 score was shown here:</p>
<div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/equation-8.png"><br></div></div>

<h1 id="Result-and-Analysis"><a href="#Result-and-Analysis" class="headerlink" title="Result and Analysis"></a>Result and Analysis</h1><h2 id="Efficiency-of-New-Word-Discovery"><a href="#Efficiency-of-New-Word-Discovery" class="headerlink" title="Efficiency of New Word Discovery"></a>Efficiency of New Word Discovery</h2><p>The results reflected the efficiency of four new words discovery algorithms were listed into the table in next page. These four algorithms were Unsupervised Unconditional New Words Discovery (shorted as UU), Unsupervised Conditional New Words Discovery (shorted as UC), Active-Learning New Words Discovery Type A (shorted as Active Type A) and Active-Learning New Words Discovery Type B (shorted as Active Type B).</p>
<p>As for some new words found from these algorithms such as “一个”, “读书”, “小狗”, “不错”, “较好”, “最大”, which could be recognized as either a vocabulary entity or a combination of two words. In NLPIR 2013, this kind of atoms’ combinations would not be recognized as single words according to NLPIR dictionary standard. However, segmenting this kind of atoms’ combinations as distinct “words” will not leave any negative influence to the existing segmentation tool but it improved the efficiency of new words discovery algorithms a lot since these words totally satisfy the rule of a “new word” in the algorithms implemented in this project. Therefore, this kind of words were marked as “reasonable words” and the efficiency of new words discovery was calculated twice, the first one strictly followed the NLPIR standard without counting reasonable words in; and the other version of efficiency considered those reasonable words as valid new words. </p>
<table>
<thead>
<tr>
<th><strong>What this Project Recognized</strong></th>
<th><strong>What NLPIR Recognized</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>一个/q</td>
<td>一/m 个/q</td>
</tr>
<tr>
<td>读书/v</td>
<td>读/v 书/n</td>
</tr>
<tr>
<td>小狗/n</td>
<td>小/ad 狗/n</td>
</tr>
<tr>
<td>不错/ad</td>
<td>不/no 错/ad</td>
</tr>
<tr>
<td>较好/ad_comp</td>
<td>较/d 好/ad</td>
</tr>
<tr>
<td>最大/ad_sup</td>
<td>最/d 大/ad</td>
</tr>
</tbody>
</table>
<p>Table 7 Samples of Reasonable Words</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>New Words Found</strong></th>
<th><strong>Valid New Words</strong></th>
<th><strong>Efficiency</strong></th>
<th><strong>Reasonable Words Found</strong></th>
<th><strong>Efficiency (with Reasonable Words Counted In)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>UU</td>
<td>3945</td>
<td>189</td>
<td>4.7909%</td>
<td>114</td>
<td>7.6806%</td>
</tr>
<tr>
<td>UC</td>
<td>920</td>
<td>161</td>
<td>17.5000%</td>
<td>97</td>
<td>28.0435%</td>
</tr>
<tr>
<td>Active Type A</td>
<td>487</td>
<td>173</td>
<td>35.5236%</td>
<td>51</td>
<td>45.9959%</td>
</tr>
<tr>
<td>Active Type B</td>
<td>698</td>
<td>224</td>
<td>32.0917%</td>
<td>130</td>
<td>50.7163%</td>
</tr>
</tbody>
</table>
<p>Table Efficiency of New Words Discovery</p>
<p>From the result, it is easy to conclude that the Active-Learning algorithm Type B is the best in both the total number of valid new words found and efficiency. On the contrary, the efficiency of Unsupervised Unconditional new words discovery algorithm (UU) is only 7.6806% even counting in all reasonable words. That means each time a hundred “new words” found by UU algorithm, there are more than 92 of them are not truly new words and these fake new words might severely affect the segmentation outcome in the end.</p>
<h2 id="Evaluation-on-Chinese-Sentence-Segmentation"><a href="#Evaluation-on-Chinese-Sentence-Segmentation" class="headerlink" title="Evaluation on Chinese Sentence Segmentation"></a>Evaluation on Chinese Sentence Segmentation</h2><p>In this section, all 4 algorithms were tested by adding the new words found into the dictionary and then analysing the segmentation results on 3 text file. The result was represented in the form of Precision, Recall and F1-Score. </p>
<div align="center"><br><div style="width:65%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/table-9.png" title="Comparison of Precision, Recall and F-1 Score between Original NLPIR (The Maximum Value was Underlined)"><br></div></div>

<p>By observing the data shown in the Table 8, it is not difficult to find that the Unsupervised Unconditional New Words Discovery algorithm could always lead a higher precision, and all the new words discovery algorithms seem lead to worse recall when compared to the original NLPIR output. These phoneme could be explained that all these four algorithms had more or less added some useless words into the dictionary during the new word discovery. More valid new words found means potentially higher precision of the segmentation, but more useless new words found would bring more noise into the segmentation and make lots of sentences, which should have been successfully segmented, wrongly segmented in the end.</p>
<p>Therefore, a good solution should not only focus on either precision or recall, but the comprehensive result concerning both these two. A measurement called F-1 Score was adopted here to show us which alternative could lead to a better behaviour accounting for both precision and recall.</p>
<p>And in order to represent the F-1 score in easier way to understand, a chart was drawn to show the testing result in visual way.</p>
<div align="center"><br><div style="width:70%;margin-top:-20px;"><br><img src="/2014/05/20/chinese-segementation/16-f1-scores.png" title="Comparison of F-1 Scores among Different Segmentation Algorithms"><br></div></div>

<p>After calculating the F-1 scores, we could find that only the Active-Learning New Words Discovery algorithms could make improvement on segmentation of informal Chinese language only. That was because the F-1 score was affected by both precision and recall, that means any improvement made on either one but led another parameter in very low level couldn’t be reflected on the F-1 score, on the contrary, the F-1 score might be lower than the original result without any improvement implemented. Besides that, the existing Chinese segmentation tools such as NLPIR 2013 were good enough. This product itself was believed as a best adjustment on precision and recall to realize the maximum F-1 score, and that made any improvement designed for it could be very difficult.</p>
<p>Even though the significant improvement made on segmentation was extremely hard, the Active-Learning algorithm Type A stilled made over 0.16% enhancement when segmenting Chinese text in informal category, in another word, on user generate content like weibo posts.</p>
<p>You may ask why the performance of the algorithms implemented by student not enhanced in linear against the formalization of the testing data. The possible explanation might be that the semi-formal data was mostly formed by words in dictionary, but unlike the formal text, there were still some contents made up with new words. And when these algorithms implemented by the student did the regular new words discovery and applied new words into the segmentation, relatively too many useless new words created great noise in segmenting the sentence and that led the original segmentation routine not able to work correctly. Nevertheless, in the testing of formal Chinese language category, all algorithms didn’t affect the original segmentation too much because there were seldom opportunities containing a new words in a very formal article. This made the all these four algorithms resulted in very similar F-1 score in the end.</p>
<h2 id="Evaluation-on-Chinese-Sentence-Segmentation-1"><a href="#Evaluation-on-Chinese-Sentence-Segmentation-1" class="headerlink" title="Evaluation on Chinese Sentence Segmentation"></a>Evaluation on Chinese Sentence Segmentation</h2><h1 id="Limitation-and-Recommendations"><a href="#Limitation-and-Recommendations" class="headerlink" title="Limitation and Recommendations"></a>Limitation and Recommendations</h1><h2 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h2><p>Some limitations was found during this project and considered could vary the evaluation result from the ideal outcome:</p>
<p>(1) The dataset used in this project was limited. This project needed larger dataset that unlimitedly close to the universe data to obtain the good enough simulative result. Even 1.2 GB was a very big data sample; it still seems not enough for this project if further improvement or research would be carried out.</p>
<p>(2) The new words discovery algorithms highly relied on the real-time feature of the dataset. But the dataset used in this project was extracted based on specific group of users, rather than extracted from the real-time weibo posts. That would affect the trueness and reliability of the simulation.</p>
<p>(3) The difference between Chinese and Western language and the ambiguity of Chinese grammar restricted the development of existing Chinese segmentation because nearly all related theory and hypotheses were built based on English. For example, the comparative adjectives and superlative adjectives, these were recognized as distinct words in English but may not in Chinese like “较好”, “最大”. The confusion on the ambiguity, or the lack of a standard, would greatly vary the evaluation result of segmentation.</p>
<h2 id="Future-Recommendations"><a href="#Future-Recommendations" class="headerlink" title="Future Recommendations"></a>Future Recommendations</h2><p>If this project would be further carried out for some improvement or<br>referenced by other research, here some future recommendations were<br>provided and please take account them into further research:</p>
<p>(1) Try to use more data in simulating the invention of the new words on Internet.</p>
<p>(2) Try to implement a real-time data extraction program and used it into the new words discovery for better simulation quality.</p>
<p>(3) Pay more attention on the complexity and efficiency of the algorithms designed for this project because lots of IO operations are needed to discover new words. Clumsy algorithms and redundant variables are memory costly and probably affect the result of the simulation. (For example, the robust algorithm discovers new words from <em>k</em> files will cost <em>O</em>(n^k^) time, that makes the new words discovery impossible when <em>k</em> is required to be large)</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>The objective of this project is to study on the existing Chinese segmentation techniques and discover any improvement for the segmentation on informal user generated text. In this project, a long period of study on related topics such as Chinese language theory and natural language processing was done by the student. In the end, four algorithms designed for the improvement upon one of the existing Chinese segmentation product were implemented and tested by the data in different categories. The final result showed one of these algorithms named Active Learning New Word Discovery (Type A) had successfully improved the performance of the Chinese segmentation with informal user generated content like weibo posts.</p>
<p>This project offered a great opportunity that allowed student to participate into the academic research, find the problem and solve it by learning and experiments independently. During the whole year period of this project, the student has obtained not only the knowledge about the natural language processing and Chinese segmentation, but plenty of practices of time management, project management and programming. In the end, the student was succeeded in implementing some improvement and satisfied the requirement by achieving a higher F-1 score. </p>
<h1 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h1><p>[1] E. K. Steven Bird, Edward Loper, Natural Language Processing with Python, 1st Edition ed. Sebastopol, CA: O’Reilly, 2009.<br>[2] P. T. Gregory Grefenstette, “What is a word, What is a sentence? Problems of Tokenization,” 1994.<br>[3] G. T. Zimin Wu, “￼Chinese Text Segmentation for Text Retrieval: Achievements and Problems,” 1993.<br>[4] P. V. d. Peter F. Brown, Robert L. Mercer, Vincent J. Della Pietra, Jenifer C. Lar, “Class-Based n-gram Models of Natural Language,” 1992.<br>[5] J. M. T. William B. Cavnar “N-Gram-Based Text Categorization,” In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, 1994.<br>[6] D. J. a. J. H. Martin, SPEECH and LANGUAGE PROCESSING: An Introduction to Natural Language Processing,<br>Computational Linguistics, and Speech Recognition, 2nd Edition ed.: Prentice Hall, 2008.<br>[7] J. G. Stanley F. Chen, “An Empirical Study of Smo othing Techniques for Language Modeling,” 1998.<br>[8] H. J. Qian Liu, “A View of Chinese Word Automatic Segmentation Research in the Chinese Information Disposal,” 2006.<br>[9] J. G. Mu Li, Changning Huang, Jianfeng Li, “Unsupervised Training for Overlapping Ambiguity Resolution in Chinese Word Segmentation,” 2003.<br>[10] S. A. Shilpa Arora, “Active Learning for Natural Language Processing,” 2007.<br>[11] Y. S. Shai Fine, Naftali Tishby, “The Hierarchical Hidden Markov Model: Analysis and Applications,” 1998.<br>[12] H.-K. Y. Hua-ping Zhang, De-Yi Xiong, Qun Liu, “HHMM-based Chinese Lexical Analyzer ICTCLAS,” 2003.<br>[13] Xinbo Zhang. (2006). Research on ICTCLAS Segmentation (3) - Atom Segementation. Available: <a href="http://blog.csdn.net/sinboy/article/details/624929" target="_blank" rel="noopener">http://blog.csdn.net/sinboy/article/details/624929</a><br>[14] Xinbo Zhang. (2006). Research on ICTCLAS Segmentation (4) - Rough Segementation. Available: <a href="http://blog.csdn.net/sinboy/article/details/663123" target="_blank" rel="noopener">http://blog.csdn.net/sinboy/article/details/663123</a><br>[15] Xinbo Zhang. (2006). Research on ICTCLAS Segmentation (5) - N-shortest Paths. Available: <a href="http://blog.csdn.net/sinboy/article/details/745498" target="_blank" rel="noopener">http://blog.csdn.net/sinboy/article/details/745498</a></p>
<h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>The project has been a challenging but wonderful experience that offered me an opportunity of self-learning and research in related fields to solve a specific problem. This is my first time doing research related work under the supervision of a professor, and what I have learned were not only the knowledge but also the methodology to analyse a problem and then solve it.<br>I would like to express my deepest gratitude to my supervisor, <em>Associate Professor Sun Aixin</em>, for his constantly encouragement and guidance during this one-year-period project, helping me overcome numberless difficulties and challenges that once made me not able to move forward.<br>Finally, a special thanks to my friends and my family had given me suggestions, technical support, and sincere encouragement throughout the project.</p>
<h1 id="Abbreviations"><a href="#Abbreviations" class="headerlink" title="Abbreviations"></a>Abbreviations</h1><p><strong>NLP</strong> – Natural Language Processing<br><strong>HMM</strong> – Hidden Markov Model<br><strong>POS</strong> – Part of Speech<br><strong>NLPIR</strong> – Natural Language Processing &amp; Information Retrieval Sharing Platform<br><strong>ICTCLAS</strong> – Institute of Computing Technology, Chinese Lexical Analysis System</p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/academic-thesis/" rel="tag">#academic, thesis</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/03/24/biz-analytics/" rel="prev" title="Business Analytics - What Motivates You to Open a Restaurant There?">
                Business Analytics - What Motivates You to Open a Restaurant There? <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Mark Meng" />
          <p class="site-author-name" itemprop="name">Mark Meng</p>
          <p class="site-description motion-element" itemprop="description">Mark Meng's Tech Blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">4</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">2.1.</span> <span class="nav-text">Background </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Internet-Slang"><span class="nav-number">2.1.1.</span> <span class="nav-text">Internet Slang</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Natural-Language-Processing"><span class="nav-number">2.1.2.</span> <span class="nav-text">Natural Language Processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#When-Natural-Language-Processing-Meets-Internet-Slang"><span class="nav-number">2.1.3.</span> <span class="nav-text">When Natural Language Processing Meets Internet Slang</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Project-Objectives"><span class="nav-number">2.2.</span> <span class="nav-text">Project Objectives</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Timeline"><span class="nav-number">2.3.</span> <span class="nav-text">Timeline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Report-Organisation"><span class="nav-number">2.4.</span> <span class="nav-text">Report Organisation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Literature-Review"><span class="nav-number">3.</span> <span class="nav-text">Literature Review</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Natural-Language-Processing-and-Chinese-Language"><span class="nav-number">3.1.</span> <span class="nav-text">Natural Language Processing and Chinese Language </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-Model-and-N-Gram"><span class="nav-number">3.2.</span> <span class="nav-text">Language Model and N-Gram </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised-Chinese-Word-Segmentation"><span class="nav-number">3.3.</span> <span class="nav-text">Unsupervised Chinese Word Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Active-Learning-for-NLP"><span class="nav-number">3.4.</span> <span class="nav-text">Active Learning for NLP </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-NLPIR"><span class="nav-number">3.5.</span> <span class="nav-text">The NLPIR</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Atom-Segmentation"><span class="nav-number">3.5.1.</span> <span class="nav-text">Atom Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rough-Segmentation"><span class="nav-number">3.5.2.</span> <span class="nav-text">Rough Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-Shortest-Path"><span class="nav-number">3.5.3.</span> <span class="nav-text">N-Shortest Path</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Project"><span class="nav-number">4.</span> <span class="nav-text">Project </span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Resources"><span class="nav-number">4.1.</span> <span class="nav-text">Resources</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hardware-Resources"><span class="nav-number">4.1.1.</span> <span class="nav-text">Hardware Resources</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Software-and-Project"><span class="nav-number">4.1.2.</span> <span class="nav-text">Software and Project</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataset"><span class="nav-number">4.1.3.</span> <span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Test-Data"><span class="nav-number">4.1.4.</span> <span class="nav-text">Test Data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Methodology"><span class="nav-number">4.2.</span> <span class="nav-text">Methodology </span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#New-Words-Extraction-from-NLPIR-Segmentation-Result"><span class="nav-number">4.2.1.</span> <span class="nav-text">New Words Extraction from NLPIR Segmentation Result</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qualification-of-New-Words-from-Candidates"><span class="nav-number">4.2.2.</span> <span class="nav-text">Qualification of New Words from Candidates</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Unsupervised-Unconditional-New-Word-Discovery"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">(1) Unsupervised Unconditional New Word Discovery</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Unsupervised-Conditional-New-Words-Discovery"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">(2) Unsupervised Conditional New Words Discovery</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Active-Learning-New-Words-Discovery"><span class="nav-number">4.2.2.3.</span> <span class="nav-text">(3) Active-Learning New Words Discovery</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Measurement"><span class="nav-number">5.</span> <span class="nav-text">Measurement</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Measurement-for-New-Words-Discovery"><span class="nav-number">5.1.</span> <span class="nav-text">Measurement for New Words Discovery </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Measurement-for-Chinese-Segmentation"><span class="nav-number">5.2.</span> <span class="nav-text">Measurement for Chinese Segmentation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Result-and-Analysis"><span class="nav-number">6.</span> <span class="nav-text">Result and Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Efficiency-of-New-Word-Discovery"><span class="nav-number">6.1.</span> <span class="nav-text">Efficiency of New Word Discovery</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-on-Chinese-Sentence-Segmentation"><span class="nav-number">6.2.</span> <span class="nav-text">Evaluation on Chinese Sentence Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluation-on-Chinese-Sentence-Segmentation-1"><span class="nav-number">6.3.</span> <span class="nav-text">Evaluation on Chinese Sentence Segmentation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Limitation-and-Recommendations"><span class="nav-number">7.</span> <span class="nav-text">Limitation and Recommendations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Limitations"><span class="nav-number">7.1.</span> <span class="nav-text">Limitations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Future-Recommendations"><span class="nav-number">7.2.</span> <span class="nav-text">Future Recommendations</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">8.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bibliography"><span class="nav-number">9.</span> <span class="nav-text">Bibliography</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Acknowledgement"><span class="nav-number">10.</span> <span class="nav-text">Acknowledgement</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Abbreviations"><span class="nav-number">11.</span> <span class="nav-text">Abbreviations</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mark Meng</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  

  

  

</body>
</html>
